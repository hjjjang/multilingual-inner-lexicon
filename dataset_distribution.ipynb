{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multi-token words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Directory containing the files\n",
    "data_dir = \"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordNonword\"\n",
    "\n",
    "# Load all files that don't end with \"2token.csv\"\n",
    "files = [f for f in os.listdir(data_dir) if not f.endswith(\"2token.csv\")]\n",
    "# files = [f for f in os.listdir(data_dir) if f.endswith(\"2token.csv\")]\n",
    "files.sort()\n",
    "# Initialize a dictionary to store data for each model and language\n",
    "data_dict = {}\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract model and language from the filename\n",
    "    parts = file.split(\"_\")\n",
    "    model = parts[1]\n",
    "    language = parts[2].split(\".\")[0]\n",
    "    \n",
    "    # Load the CSV file\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure the required columns exist\n",
    "    if \"token_num\" not in df.columns or \"freq\" not in df.columns:\n",
    "        print(f\"Skipping file {file} due to missing columns.\")\n",
    "        continue\n",
    "    df = df[df[\"label\"]==\"realword\"]\n",
    "    # Store the processed data\n",
    "    key = f\"{model} ({language})\"\n",
    "    data_dict[key] = df\n",
    "\n",
    "# Define the desired order for languages and models\n",
    "lang_order = [\"English\", \"German\", \"Korean\"]\n",
    "model_order = [\"Babel-9B-Chat\", \"gemma-3-12b-it\", \"Llama-2-7b-chat-hf\"]\n",
    "\n",
    "# Sort the data_dict keys based on the specified order\n",
    "ordered_keys = sorted(\n",
    "    data_dict.keys(),\n",
    "    key=lambda k: (lang_order.index(k.split(\" (\")[1][:-1]), model_order.index(k.split(\" (\")[0]))\n",
    "    # key=lambda k: (lang_order.index(k.split(\" (\")[1][:-1].split(\"-\")[0]), model_order.index(k.split(\" (\")[0]))\n",
    ")\n",
    "\n",
    "# Reorder the data_dict based on the ordered keys\n",
    "ordered_data_dict = {key: data_dict[key] for key in ordered_keys}\n",
    "\n",
    "# Create a 3x3 grid of subplots\n",
    "num_subplots = len(ordered_data_dict)\n",
    "rows, cols = 3, 3  # Adjust rows and columns as needed\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, (key, df)) in enumerate(zip(axes, ordered_data_dict.items())):\n",
    "    # Filter data for token lengths 2, 3, and 4\n",
    "    filtered_df = df[df[\"token_num\"].isin([2, 3, 4])]\n",
    "\n",
    "    # Create a boxplot\n",
    "    sns.boxplot(data=filtered_df, x=\"token_num\", y=\"freq\", ax=ax, palette=\"Set2\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(f\"{key}\", fontsize=20)\n",
    "    ax.set_xlabel(\"Token Length\", labelpad=20, fontsize=16)\n",
    "    ax.set_xticklabels(\"\")\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Calculate dataset size for each token length\n",
    "    token_sizes = filtered_df.groupby(\"token_num\").size()\n",
    "\n",
    "    # Add dataset size below each boxplot\n",
    "    for token, size in token_sizes.items():\n",
    "        ax.text(\n",
    "            x=token - 2,  # Adjust x position based on token length (2, 3, 4)\n",
    "            y=ax.get_ylim()[0] - 1.5,  # Add more space below the boxplot\n",
    "            s=f\"{token} (n={size})\",\n",
    "            ha=\"center\",\n",
    "            fontsize=15\n",
    "        )\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(ordered_data_dict), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout for more space between subplots and labels\n",
    "# plt.tight_layout(pad=3.0)  # Increase padding between subplots\n",
    "fig.subplots_adjust(bottom=0.1, top=0.95, hspace=0.35, wspace=0.1)  # Add more space between rows and adjust margins\n",
    "# fig.text(0.5, 0.04, \"Sentence length\", ha=\"center\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Directory containing the files\n",
    "data_dir = \"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordNonword\"\n",
    "\n",
    "# Load all files that don't end with \"2token.csv\"\n",
    "# files = [f for f in os.listdir(data_dir) if not f.endswith(\"2token.csv\")]\n",
    "files = [f for f in os.listdir(data_dir) if f.endswith(\"2token.csv\")]\n",
    "files.sort()\n",
    "# Initialize a dictionary to store data for each model and language\n",
    "data_dict = {}\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract model and language from the filename\n",
    "    parts = file.split(\"_\")\n",
    "    model = parts[1]\n",
    "    language = parts[2].split(\".\")[0].split(\"-\")[0]\n",
    "    \n",
    "    # Load the CSV file\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure the required columns exist\n",
    "    if \"token_num\" not in df.columns or \"freq\" not in df.columns:\n",
    "        print(f\"Skipping file {file} due to missing columns.\")\n",
    "        continue\n",
    "    df = df[df[\"label\"]==\"realword\"]\n",
    "    # Store the processed data\n",
    "    key = f\"{model} ({language})\"\n",
    "    data_dict[key] = df\n",
    "\n",
    "# Define the desired order for languages and models\n",
    "lang_order = [\"English\", \"German\", \"Korean\"]\n",
    "model_order = [\"Babel-9B-Chat\", \"gemma-3-12b-it\", \"Llama-2-7b-chat-hf\"]\n",
    "\n",
    "# Sort the data_dict keys based on the specified order\n",
    "ordered_keys = sorted(\n",
    "    data_dict.keys(),\n",
    "    key=lambda k: (lang_order.index(k.split(\" (\")[1][:-1]), model_order.index(k.split(\" (\")[0]))\n",
    "    # key=lambda k: (lang_order.index(k.split(\" (\")[1][:-1].split(\"-\")[0]), model_order.index(k.split(\" (\")[0]))\n",
    ")\n",
    "\n",
    "# Reorder the data_dict based on the ordered keys\n",
    "ordered_data_dict = {key: data_dict[key] for key in ordered_keys}\n",
    "\n",
    "# Create a 3x3 grid of subplots\n",
    "num_subplots = len(ordered_data_dict)\n",
    "rows, cols = 3, 3  # Adjust rows and columns as needed\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, (key, df)) in enumerate(zip(axes, ordered_data_dict.items())):\n",
    "    # Filter data for token lengths 2, 3, and 4\n",
    "    filtered_df = df[df[\"token_num\"].isin([2])]\n",
    "\n",
    "    sns.boxplot(data=filtered_df, x=\"token_num\", y=\"freq\", ax=ax, palette=\"Set2\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(f\"{key}\", fontsize=20)\n",
    "    ax.set_xlabel(\"Token Length\", labelpad=20, fontsize=16)\n",
    "    ax.set_xticklabels(\"\")\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Calculate dataset size for each token length\n",
    "    token_sizes = filtered_df.groupby(\"token_num\").size()\n",
    "\n",
    "    # Add dataset size below each boxplot\n",
    "    for token, size in token_sizes.items():\n",
    "        ax.text(\n",
    "            x=token - 2,  # Adjust x position based on token length (2, 3, 4)\n",
    "            y=ax.get_ylim()[0] - 1.5,  # Add more space below the boxplot\n",
    "            s=f\"{token} (n={size})\",\n",
    "            ha=\"center\",\n",
    "            fontsize=15\n",
    "        )\n",
    "\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(ordered_data_dict), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout for more space between subplots and labels\n",
    "plt.tight_layout(pad=3.0)  # Increase padding between subplots\n",
    "fig.subplots_adjust(bottom=0.1, top=0.95, hspace=0.35, wspace=0.1)  # Add more space between rows and adjust margins\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Single-token words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "# Directory containing the files\n",
    "data_dir = \"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity\"\n",
    "\n",
    "# Load all files that don't end with \"2token.csv\"\n",
    "files = [f for f in os.listdir(data_dir) if \"single_token_splitted\" in f]\n",
    "\n",
    "# Initialize a dictionary to store data for each model and language\n",
    "data_dict = {}\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract model and language from the filename\n",
    "    parts = file.split(\"_\")\n",
    "    model = parts[-3]\n",
    "    language = parts[-2]\n",
    "    \n",
    "    # Load the CSV file\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"token_num\"] = df[\"splitted_tokens\"].apply(literal_eval).apply(len)\n",
    "    \n",
    "    # Ensure the required columns exist\n",
    "    if \"token_num\" not in df.columns or \"freq\" not in df.columns:\n",
    "        print(f\"Skipping file {file} due to missing columns.\")\n",
    "        continue\n",
    "    # df = df[df[\"label\"]==\"realword\"]\n",
    "    # Store the processed data\n",
    "    key = f\"{model} ({language})\"\n",
    "    data_dict[key] = df\n",
    "\n",
    "# Define the desired order for languages and models\n",
    "lang_order = [\"English\", \"German\"]\n",
    "model_order = [\"Babel-9B-Chat\", \"gemma-3-12b-it\", \"Llama-2-7b-chat-hf\"]\n",
    "\n",
    "# Sort the data_dict keys based on the specified order\n",
    "ordered_keys = sorted(\n",
    "    data_dict.keys(),\n",
    "    key=lambda k: (lang_order.index(k.split(\" (\")[1][:-1]), model_order.index(k.split(\" (\")[0]))\n",
    ")\n",
    "\n",
    "# Reorder the data_dict based on the ordered keys\n",
    "ordered_data_dict = {key: data_dict[key] for key in ordered_keys}\n",
    "\n",
    "# Create a grid of subplots with languages as rows and models as columns\n",
    "rows, cols = len(lang_order), len(model_order)\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, (key, df)) in enumerate(zip(axes, ordered_data_dict.items())):\n",
    "    # Filter data for token lengths 2, 3, and 4\n",
    "    # filtered_df = df[df[\"token_num\"].isin([2, 3, 4])]\n",
    "\n",
    "    # Create a boxplot\n",
    "    sns.boxplot(data=df, x=\"token_num\", y=\"freq\", ax=ax, palette=\"Set2\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(f\"{key}\", fontsize=20)\n",
    "    ax.set_xlabel(\"Token Length\", labelpad=20, fontsize=16)\n",
    "    ax.set_xticklabels(\"\")\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "    # Calculate dataset size for each token length\n",
    "    token_sizes = df.groupby(\"token_num\").size()\n",
    "\n",
    "    # Add dataset size below each boxplot\n",
    "    for token, size in token_sizes.items():\n",
    "        ax.text(\n",
    "            x=token - 2,  # Adjust x position based on token length (2, 3, 4)\n",
    "            y=ax.get_ylim()[0] - 1.5,  # Add more space below the boxplot\n",
    "            s=f\"{token} (n={size})\",\n",
    "            ha=\"center\",\n",
    "            fontsize=15\n",
    "        )\n",
    "\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(ordered_data_dict), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout for more space between subplots and labels\n",
    "plt.tight_layout(pad=3.0)  # Increase padding between subplots\n",
    "fig.subplots_adjust(bottom=0.3, top=0.95, hspace=0.3)  # Add more space between rows and adjust margins\n",
    "# fig.text(0.5, 0.04, \"Token Length\", ha=\"center\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "# Directory containing the files\n",
    "data_dir = \"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity\"\n",
    "\n",
    "# Load all files that don't end with \"2token.csv\"\n",
    "files = [f for f in os.listdir(data_dir) if \"single_token_typos\" in f]\n",
    "\n",
    "# Initialize a dictionary to store data for each model and language\n",
    "data_dict = {}\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract model and language from the filename\n",
    "    parts = file.split(\"_\")\n",
    "    model = parts[-3]\n",
    "    language = parts[-2]\n",
    "    \n",
    "    # Load the CSV file\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"token_num\"] = df[\"splitted_typo_tokens\"].apply(literal_eval).apply(len)\n",
    "    \n",
    "    # Ensure the required columns exist\n",
    "    if \"token_num\" not in df.columns or \"freq\" not in df.columns:\n",
    "        print(f\"Skipping file {file} due to missing columns.\")\n",
    "        continue\n",
    "    # df = df[df[\"label\"]==\"realword\"]\n",
    "    # Store the processed data\n",
    "    key = f\"{model} ({language})\"\n",
    "    data_dict[key] = df\n",
    "\n",
    "# Define the desired order for languages and models\n",
    "lang_order = [\"English\", \"German\"]\n",
    "model_order = [\"Babel-9B-Chat\", \"gemma-3-12b-it\", \"Llama-2-7b-chat-hf\"]\n",
    "\n",
    "# Sort the data_dict keys based on the specified order\n",
    "ordered_keys = sorted(\n",
    "    data_dict.keys(),\n",
    "    key=lambda k: (lang_order.index(k.split(\" (\")[1][:-1]), model_order.index(k.split(\" (\")[0]))\n",
    ")\n",
    "\n",
    "# Reorder the data_dict based on the ordered keys\n",
    "ordered_data_dict = {key: data_dict[key] for key in ordered_keys}\n",
    "\n",
    "# Create a grid of subplots with languages as rows and models as columns\n",
    "rows, cols = len(lang_order), len(model_order)\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 10), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, (key, df)) in enumerate(zip(axes, ordered_data_dict.items())):\n",
    "    # Filter data for token lengths 2, 3, and 4\n",
    "    # filtered_df = df[df[\"token_num\"].isin([2, 3, 4])]\n",
    "\n",
    "    # Create a boxplot\n",
    "    sns.boxplot(data=df, x=\"token_num\", y=\"freq\", ax=ax, palette=\"Set2\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(f\"{key}\", fontsize=20)\n",
    "    ax.set_xlabel(\"Token Length\", labelpad=20, fontsize=16)\n",
    "    ax.set_xticklabels(\"\")\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "    # Calculate dataset size for each token length\n",
    "    token_sizes = df.groupby(\"token_num\").size()\n",
    "\n",
    "    # Add dataset size below each boxplot\n",
    "    for token, size in token_sizes.items():\n",
    "        ax.text(\n",
    "            x=token - 2,  # Adjust x position based on token length (2, 3, 4)\n",
    "            y=ax.get_ylim()[0] - 1.5,  # Add more space below the boxplot\n",
    "            s=f\"{token} (n={size})\",\n",
    "            ha=\"center\",\n",
    "            fontsize=15\n",
    "        )\n",
    "\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(ordered_data_dict), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout for more space between subplots and labels\n",
    "plt.tight_layout(pad=3.0)  # Increase padding between subplots\n",
    "fig.subplots_adjust(bottom=0.3, top=0.95, hspace=0.3)  # Add more space between rows and adjust margins\n",
    "# fig.text(0.5, 0.04, \"Token Length\", ha=\"center\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity\"\n",
    "\n",
    "# Load all files that don't end with \"2token.csv\"\n",
    "files = [f for f in os.listdir(data_dir) if \"single_token_typos\" in f]\n",
    "\n",
    "# Initialize a dictionary to store data for each model and language\n",
    "data_dict = {}\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract model and language from the filename\n",
    "    parts = file.split(\"_\")\n",
    "    model = parts[-3]\n",
    "    language = parts[-2]\n",
    "    \n",
    "    # Load the CSV file\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    if \"typo_type\" in df.columns:\n",
    "        # Calculate the count of each typo type\n",
    "        typo_type_counts = df[\"typo_type\"].value_counts()\n",
    "\n",
    "        # Calculate the percentage of each typo type\n",
    "        typo_type_percentages = df[\"typo_type\"].value_counts(normalize=True) * 100\n",
    "\n",
    "        # Combine counts and percentages into a DataFrame\n",
    "        typo_stats = pd.DataFrame({\n",
    "            \"Count\": typo_type_counts,\n",
    "            \"Percentage\": typo_type_percentages\n",
    "        })\n",
    "\n",
    "        # Display the statistics\n",
    "        print(\"Typo Type Statistics:\")\n",
    "        print(typo_stats)\n",
    "    else:\n",
    "        print(\"The column 'typo_type' does not exist in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- config ----------\n",
    "data_dir = \"/home/hyujang/multilingual-inner-lexicon/output/RQ1/ComponentAnalysis/attention_weights2\"\n",
    "ref_dir  = \"/home/hyujang/multilingual-inner-lexicon/data/RQ1/ComponentAnalysis\"\n",
    "\n",
    "# Files look like: Babel-9B-Chat_English_1token.csv\n",
    "lang_order  = [\"English\", \"German\", \"Korean\"]\n",
    "model_order = [\"Babel-9B-Chat\", \"gemma-3-12b-it\", \"Llama-2-7b-chat-hf\"]\n",
    "\n",
    "# ---------- load & merge ----------\n",
    "files_1token = [f for f in os.listdir(data_dir) if \"1token\" in f and f.endswith(\".csv\")]\n",
    "files_2token = [f for f in os.listdir(data_dir) if \"2token\" in f and f.endswith(\".csv\")]\n",
    "\n",
    "data_dict_1token = {}  # key: \"Model (Language)\" -> merged df\n",
    "data_dict_2token = {}  # key: \"Model (Language)\" -> merged df\n",
    "\n",
    "def load_and_merge(files, data_dict):\n",
    "    for file in files:\n",
    "        parts = file.replace(\".csv\", \"\").split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        model, language = parts[0], parts[1]\n",
    "\n",
    "        # read primary\n",
    "        df = pd.read_csv(os.path.join(data_dir, file))\n",
    "\n",
    "        # read reference for merge (provides sentence_length)\n",
    "        ref_path = os.path.join(ref_dir, f\"{model}_{language}_wiki_noun_frequencies_context.csv\")\n",
    "        if not os.path.exists(ref_path):\n",
    "            print(f\"[warn] Missing ref CSV: {ref_path}\")\n",
    "            continue\n",
    "        ref_df = pd.read_csv(ref_path)\n",
    "\n",
    "        if \"word\" in df.columns and \"word\" in ref_df.columns:\n",
    "            merged = df.merge(\n",
    "                ref_df[[\"word\", \"sentence_length\", \"original_frequency\"]],\n",
    "                on=\"word\",\n",
    "                how=\"left\"\n",
    "            )\n",
    "            data_dict[f\"{model} ({language})\"] = merged\n",
    "        else:\n",
    "            print(f\"[warn] 'word' column missing in {file} or its ref; skipped.\")\n",
    "\n",
    "load_and_merge(files_1token, data_dict_1token)\n",
    "load_and_merge(files_2token, data_dict_2token)\n",
    "\n",
    "# ---------- figure layout ----------\n",
    "n_rows, n_cols = len(lang_order), len(model_order)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 10), sharex=False, sharey=True)\n",
    "if n_rows == 1 and n_cols == 1:\n",
    "    axes = [[axes]]\n",
    "elif n_rows == 1:\n",
    "    axes = [axes]\n",
    "elif n_cols == 1:\n",
    "    axes = [[ax] for ax in axes]\n",
    "\n",
    "# compute a global max sentence length to align bins\n",
    "all_lengths = []\n",
    "for df in data_dict_1token.values():\n",
    "    if \"sentence_length\" in df.columns:\n",
    "        all_lengths.extend(df[\"sentence_length\"].dropna().astype(int).tolist())\n",
    "for df in data_dict_2token.values():\n",
    "    if \"sentence_length\" in df.columns:\n",
    "        all_lengths.extend(df[\"sentence_length\"].dropna().astype(int).tolist())\n",
    "max_len = max(all_lengths) if all_lengths else 0\n",
    "bins = range(1, max_len + 2) if max_len > 0 else 10  # 1..max_len inclusive; fallback 10 bins\n",
    "\n",
    "# map for quick lookup\n",
    "def key_for(model, lang):\n",
    "    return f\"{model} ({lang})\"\n",
    "\n",
    "# ---------- plot ----------\n",
    "for r, lang in enumerate(lang_order):\n",
    "    for c, model in enumerate(model_order):\n",
    "        ax = axes[r][c]\n",
    "        key = key_for(model, lang)\n",
    "        if key in data_dict_1token:\n",
    "            df_1token = data_dict_1token[key]\n",
    "            sl_1token = df_1token[\"sentence_length\"].dropna().astype(int)\n",
    "            if len(sl_1token) > 0:\n",
    "                ax.hist(sl_1token, bins=bins, edgecolor=\"black\", alpha=0.5, label=\"single-token words\", color=\"r\")\n",
    "        if key in data_dict_2token:\n",
    "            df_2token = data_dict_2token[key]\n",
    "            sl_2token = df_2token[\"sentence_length\"].dropna().astype(int)\n",
    "            if len(sl_2token) > 0:\n",
    "                ax.hist(sl_2token, bins=bins, edgecolor=\"black\", alpha=0.5, label=\"two-token words\", color=\"b\")\n",
    "\n",
    "        # Add legend and tidy grid\n",
    "        ax.legend(loc=\"upper right\", fontsize=16)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for c, model in enumerate(model_order):\n",
    "    axes[0][c].set_title(model, fontsize=20, pad=10)\n",
    "\n",
    "# row labels (languages) on left; also set y-axis label on left column only\n",
    "for r, lang in enumerate(lang_order):\n",
    "    axes[r][0].set_ylabel(f\"{lang} Count\", fontsize=20)\n",
    "\n",
    "# shared labels\n",
    "plt.tight_layout(pad=0.6, w_pad=0.3, h_pad=0.3)  # smaller pads = less gap\n",
    "fig.subplots_adjust(bottom=0.1, top=0.95, hspace=0.1)  # Add more space between rows and adjust margins\n",
    "fig.text(0.5, 0.04, \"Sentence length\", ha=\"center\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# word translation pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "korean → german: 3408\n",
    "\n",
    "korean → english: 1690\n",
    "\n",
    "german → english: 2096\n",
    "\n",
    "german → korean: 2210\n",
    "\n",
    "english → german: 1868\n",
    "\n",
    "english → korean: 1341"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
