{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizers\n",
    "with open(\"../user_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "token_1 = config[\"huggingface_token\"]['token_1'] # from the main acoount\n",
    "token_2 = config[\"huggingface_token\"]['token_2'] # from the sub account\n",
    "\n",
    "tokenizer_configs = {\n",
    "    # \"google/gemma-2-2b-it\": token_1,\n",
    "    \"google/gemma-2-9b-it\": token_1,\n",
    "    \"google/gemma-3-12b-it\": token_1,\n",
    "    # \"meta-llama/Llama-3.2-1B-Instruct\": token_2,\n",
    "    # \"meta-llama/Llama-3.2-3B-Instruct\": token_2,\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\": token_2,\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\": token_1,\n",
    "    \"microsoft/phi-4\": None,\n",
    "    # \"microsoft/Phi-4-mini-instruct\": None,\n",
    "    \"microsoft/Phi-3-small-8k-instruct\": None,\n",
    "    # \"microsoft/Phi-3-mini-4k-instruct\": None,\n",
    "    # \"microsoft/Phi-3.5-mini-instruct\": None,\n",
    "    \"bigscience/bloom-7b1\": None,\n",
    "    \"Tower-Babel/Babel-9B-Chat\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Testsets\n",
    "seed = 2025\n",
    "n_sample = 100\n",
    "\n",
    "languages = {\"eng_Latn\": \"English\", \"kor_Hang\": \"Korean\", \"deu_Latn\": \"German\"}\n",
    "samples = {}\n",
    "\n",
    "for lang_code, lang_name in languages.items():\n",
    "    dataset = load_dataset(\"facebook/flores\", lang_code, trust_remote_code=True)\n",
    "    samples[lang_name] = dataset[\"dev\"].shuffle(seed=seed).select(range(n_sample))[\"sentence\"]\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "for lang_name, sample_data in samples.items():\n",
    "    df[lang_name] = [s for s in sample_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DownStream Performance Results - MMLU-Pro & FLORES\n",
    "\n",
    "html_file_path = '/home/hyujang/multilingual-inner-lexicon/data/results.html'\n",
    "tables = pd.read_html(html_file_path)\n",
    "df_results = tables[0]\n",
    "df_results = df_results.rename(columns={\"Unnamed: 0\": \"Benchmark\"})\n",
    "\n",
    "tokenizer_mmlupro = df_results.iloc[41,1:].to_dict() # https://huggingface.co/spaces/open-llm-leaderboard/comparator\n",
    "del(df_results)\n",
    "\n",
    "tokenizer_configs.keys() - tokenizer_mmlupro.keys()\n",
    "tokenizer_mmlupro['google/gemma-3-12b-it'] = 0.606 # https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro\n",
    "tokenizer_mmlupro['Tower-Babel/Babel-9B-Chat'] = 0 # No information available\n",
    "\n",
    "tokenizer_flores = { # https://huggingface.co/Tower-Babel/Babel-9B-Chat\n",
    "    \"google/gemma-2-2b-it\": 0,\n",
    "    \"google/gemma-2-9b-it\": 0.548,\n",
    "    \"google/gemma-3-12b-it\": 0, #0.460 #https://huggingface.co/google/gemma-3-12b-it#multilingual\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": 0,\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": 0,\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\": 0.473,\n",
    "    \"microsoft/phi-4\": 0,\n",
    "    \"microsoft/Phi-4-mini-instruct\": 0,\n",
    "    \"microsoft/Phi-3-small-8k-instruct\": 0,\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\": 0,\n",
    "    \"microsoft/Phi-3.5-mini-instruct\": 0,\n",
    "    \"bigscience/bloom-7b1\": 0,\n",
    "    \"Tower-Babel/Babel-9B-Chat\": 0.567\n",
    "}\n",
    "\n",
    "tokenizer_mmmlu = {\n",
    "    \"google/gemma-2-2b-it\": 0,\n",
    "    \"google/gemma-2-9b-it\": 0.596,\n",
    "    \"google/gemma-3-12b-it\": 0,\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": 0,\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": 0,\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\": 0.506,\n",
    "    \"microsoft/phi-4\": 0,\n",
    "    \"microsoft/Phi-4-mini-instruct\": 0,\n",
    "    \"microsoft/Phi-3-small-8k-instruct\": 0,\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\": 0,\n",
    "    \"microsoft/Phi-3.5-mini-instruct\": 0,\n",
    "    \"bigscience/bloom-7b1\": 0,\n",
    "    \"Tower-Babel/Babel-9B-Chat\": 0.598\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Fertility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE -> WORD\n",
    "\n",
    "### English\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt_tab', download_dir='/work/hyujang/miniconda3/envs/thesis/nltk_data')\n",
    "# nltk.download('punkt', download_dir='/work/hyujang/miniconda3/envs/thesis/nltk_data')\n",
    "# nltk.data.path.append('/work/hyujang/miniconda3/envs/thesis/nltk_data')\n",
    "df['English_nltk'] = df['English'].apply(lambda x: word_tokenize(x, language='english'))\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# df['English_spacy'] = df['English'].apply(lambda text: [token.text for token in nlp(text)])\n",
    "\n",
    "### Korean\n",
    "# from kiwipiepy import Kiwi\n",
    "# kiwi = Kiwi()\n",
    "# df['Korean_kiwi'] = df['Korean'].apply(lambda x: [token[0] for token in kiwi.tokenize(x)])\n",
    "from konlpy.tag import Okt # TODO: Mecab\n",
    "okt = Okt()\n",
    "df['Korean_okt'] = df['Korean'].apply(lambda x: okt.morphs(x))\n",
    "\n",
    "### German\n",
    "df['German_nltk'] = df['German'].apply(lambda x: word_tokenize(x, language='german'))\n",
    "\n",
    "lang_wordinzer = {\"English\": \"nltk\", \"Korean\": \"okt\", \"German\": \"nltk\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subword_fertility(words_list, tokenizer):\n",
    "    token_lengths = [len(tokenizer.tokenize(word)) for word in words_list]\n",
    "    # return np.mean(token_lengths)\n",
    "    return token_lengths\n",
    "\n",
    "for tokenizer_name, token in tokenizer_configs.items():\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True, token=token, trust_remote_code=True)\n",
    "        print(f\"Loaded {tokenizer_name}\")\n",
    "        for lang_name in languages.values():\n",
    "            df[f\"{tokenizer_name}_{lang_name}_tokens\"] = df[lang_name].apply(lambda x: tokenizer.tokenize(x)) \n",
    "            # df[f\"{tokenizer_name}_{lang_name}_token_length\"] = df[f\"{tokenizer_name}_{lang_name}_tokens\"].apply(len)\n",
    "            # df[f\"{tokenizer_name}_{lang_name}_fertility\"]= df[f\"{lang_name}_{lang_wordinzer[lang_name]}\"].apply(lambda x: calculate_subword_fertility(x, tokenizer))\n",
    "            df[f\"{tokenizer_name}_{lang_name}_token_len\"]= df[f\"{lang_name}_{lang_wordinzer[lang_name]}\"].apply(lambda x: calculate_subword_fertility(x, tokenizer))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load tokenizer {tokenizer_name}: {e}\")\n",
    "\n",
    "# df.to_csv(\"./output/tokenizers_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_name in lang_wordinzer.keys():\n",
    "    print(lang_name)\n",
    "    print(len(df[f\"{lang_name}_{lang_wordinzer[lang_name]}\"].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "save = False\n",
    "\n",
    "language_colors = {\n",
    "    \"English\": \"#1f77b4\",\n",
    "    \"German\": \"#2ca02c\",\n",
    "    \"Korean\": \"#ff7f0e\",\n",
    "}\n",
    "\n",
    "# Create subplots for each language\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True)\n",
    "\n",
    "for ax, (lang, color) in zip(axes, language_colors.items()):\n",
    "    # Extract average subword fertility for each tokenizer\n",
    "    avg_fertility = {\n",
    "        tokenizer_name: df[f\"{tokenizer_name}_{lang}_fertility\"].mean()\n",
    "        for tokenizer_name in tokenizer_configs.keys()\n",
    "    }\n",
    "\n",
    "    tokenizer_names = list(tokenizer_configs.keys())\n",
    "    tokenizer_names = [tokenizer_name.split(\"/\")[1] for tokenizer_name in tokenizer_names]\n",
    "    x = np.arange(len(tokenizer_names))  # Position for bars\n",
    "\n",
    "    # Plot Avg Subword Fertility\n",
    "    ax.bar(x, avg_fertility.values(), color=color, alpha=0.7)\n",
    "    ax.set_ylabel(\"Avg Subword Fertility\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(tokenizer_names, rotation=90)\n",
    "    ax.set_title(f\"Avg Subword Fertility for {lang}\")\n",
    "\n",
    "# Add common x-axis label\n",
    "fig.text(0.5, 0.04, \"Tokenizers\", ha=\"center\", fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "if save:\n",
    "    save_path = os.path.join(\"output/image\", \"avg_fertility_subplots.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "save = False\n",
    "\n",
    "# Keep a stable language order from your colors dict\n",
    "languages = list(language_colors.keys())  # [\"English\", \"German\", \"Korean\"]\n",
    "\n",
    "# Tokenizer names: full keys for data access, short names for labels\n",
    "tokenizer_names_full = list(tokenizer_configs.keys())\n",
    "tokenizer_names_disp = [name.split(\"/\")[-1] for name in tokenizer_names_full]\n",
    "\n",
    "# Build fertility matrix: rows = tokenizers, cols = languages\n",
    "fertility_matrix = np.array([\n",
    "    [\n",
    "        # df[f\"{tok}_{lang}_fertility\"].mean()\n",
    "        sum(df[f\"{tok}_{lang}_token_len\"].sum()) / len(df[f\"{tok}_{lang}_token_len\"].sum())\n",
    "        for lang in languages\n",
    "    ]\n",
    "    for tok in tokenizer_names_full\n",
    "])  # shape: (num_tokenizers, num_languages)\n",
    "\n",
    "# X positions for each tokenizer group\n",
    "x = np.arange(len(tokenizer_names_disp))\n",
    "num_langs = len(languages)\n",
    "bar_width = 0.8 / num_langs  # total group width ~0.8\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot grouped bars: one loop per language (offset per language)\n",
    "for j, lang in enumerate(languages):\n",
    "    vals = fertility_matrix[:, j]\n",
    "    ax.bar(\n",
    "        x + j * bar_width,\n",
    "        vals,\n",
    "        width=bar_width,\n",
    "        label=lang,\n",
    "        color=language_colors.get(lang, None),\n",
    "        alpha=0.9,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "# Labels, ticks, legend\n",
    "ax.set_ylabel(\"Avg Subword Fertility\")\n",
    "ax.set_xlabel(\"Tokenizers\")\n",
    "# ax.set_title(\"Average Subword Fertility by Language\")\n",
    "ax.set_xticks(x + (num_langs - 1) * bar_width / 2)\n",
    "ax.set_xticklabels(tokenizer_names_disp, rotation=45, ha=\"right\")\n",
    "ax.legend(title=\"Language\")\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save:\n",
    "    save_path = os.path.join(\"output/image\", \"avg_fertility_grouped.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Example data (replace these with your actual data)\n",
    "sorted_mmlupro = [tokenizer_mmlupro[tok] for tok in tokenizer_configs.keys()]\n",
    "sorted_flores = [tokenizer_flores[tok] for tok in tokenizer_configs.keys()]\n",
    "sorted_mmmlu = [tokenizer_mmmlu[tok] for tok in tokenizer_configs.keys()]\n",
    "\n",
    "tokenizer_names = list(tokenizer_configs.keys())\n",
    "x = np.arange(len(tokenizer_names))  # Position for groups\n",
    "\n",
    "# Bar width (ensures bars don't overlap)\n",
    "bar_width = 0.25  \n",
    "\n",
    "# Create the bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot MMLUPro\n",
    "ax.bar(x - bar_width, sorted_mmlupro, width=bar_width, color=\"green\", label=\"MMLUPro\", alpha=0.7)\n",
    "\n",
    "# Plot Flores\n",
    "ax.bar(x, sorted_flores, width=bar_width, color=\"blue\", label=\"Flores\", alpha=0.7)\n",
    "\n",
    "# Plot MMMLU\n",
    "ax.bar(x + bar_width, sorted_mmmlu, width=bar_width, color=\"orange\", label=\"MMMLU\", alpha=0.7)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_ylabel(\"Scores\")\n",
    "ax.set_xlabel(\"Tokenizers\")\n",
    "ax.set_title(\"Comparison of MMLUPro, Flores, and MMMLU Scores per Tokenizer\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tokenizer_names, rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "# Save the plot if needed\n",
    "save = True\n",
    "if save:\n",
    "    save_path = os.path.join(\"output/image\", \"mmlu_flores_mmmlu_comparison.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "- issue: no single source that compares many models for a dataset\n",
    "- final decision: gemma3-12-it, babel-9b-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Inspect Encoding Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/hyujang/multilingual-inner-lexicon/output/tokenizers_comparison.csv\")\n",
    "df.loc[:, df.columns.str.contains(\"Korean\", case=False)]\n",
    "# df['microsoft/Phi-4-mini-instruct_Korean_tokens'].tolist() # t\n",
    "# import unicodedata\n",
    "# text = unicodedata.normalize(\"NFKC\", df['microsoft/Phi-4-mini-instruct_Korean_tokens'][0][2])\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.columns.str.contains(\"German\", case=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", use_fast=True, token=token_2, trust_remote_code=True)\n",
    "\n",
    "print(tokenizer.tokenize(\"안녕하세요\"))\n",
    "a = tokenizer.encode(\"안녕하세요\")\n",
    "print(a)\n",
    "print(tokenizer.decode(a))\n",
    "tokens = tokenizer.convert_ids_to_tokens(a)\n",
    "print(tokens)\n",
    "decoded_tokens = \" \".join(tokens)\n",
    "print(decoded_tokens)\n",
    "print('ìķĪ'.encode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
