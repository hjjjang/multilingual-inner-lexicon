{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Dictionaries\n",
    "- no frequency check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /work/hyujang/miniconda3/envs/thesis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58690\n"
     ]
    }
   ],
   "source": [
    "## English\n",
    "# nltk.download('averaged_perceptron_tagger_eng', download_dir='/work/hyujang/miniconda3/envs/thesis/nltk_data') # for en-pos tagging\n",
    "nltk.download('wordnet', download_dir='/work/hyujang/miniconda3/envs/thesis/nltk_data') # en vocabulary\n",
    "# en_ds = load_dataset(\"manu/project_gutenberg\", split=\"en\", streaming=True)\n",
    "# en_ds = load_dataset(\"deepmind/pg19\", split=None, streaming=True, trust_remote_code=True)\n",
    "# en_ds = load_dataset(\"cambridge-climb/BabyLM\", split=None, streaming=True, trust_remote_code=True) # smaller size\n",
    "# en_ds = load_dataset(\"marksverdhei/wordnet-definitions-en-2021\")\n",
    "# en_df = en_ds['train'].to_pandas()\n",
    "# data_list = list(en_ds.take(100))\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "noun_synsets = list(wordnet.all_synsets(pos=wordnet.NOUN))\n",
    "en_nouns = set(lemma.name() for synset in noun_synsets for lemma in synset.lemmas() if \"_\" not in lemma.name())\n",
    "print(len(en_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Korean\n",
    "# ds = load_dataset(\"wicho/kor_3i4k\")\n",
    "# ds = load_dataset(\"binjang/NIKL-korean-english-dictionary\")\n",
    "import sys\n",
    "# sys.setdefaultencoding(\"utf-8\")\n",
    "ds = load_dataset(\"hyunwoongko/korean-word-dict\")\n",
    "ko_dict = ds['train'].to_pandas()\n",
    "# ds = load_dataset(\"hac541309/woori_spring_dict\")\n",
    "# df = ds['train'].to_pandas()\n",
    "# df_split = df['text'].str.split(\", \", expand=True) # takes too long\n",
    "\n",
    "from konlpy.tag import Okt # TODO: Mecab\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "okt = Okt()\n",
    "ko_dict['word'] = ko_dict['word'].progress_apply(lambda x: unicodedata.normalize(\"NFC\", x)) # ㄴㅐ -> 내\n",
    "ko_dict['pos_okt'] = ko_dict['word'].progress_apply(lambda x: okt.pos(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_dict_filtered = ko_dict[\n",
    "    (ko_dict['pos_okt'].apply(len) == 1) & \n",
    "    (ko_dict['pos_okt'].apply(lambda x: x[0][1] == 'Noun')) &\n",
    "    (ko_dict['word'].apply(lambda x: len(x) > 1)) &\n",
    "    (ko_dict['freq'].apply(lambda x: x > 0))\n",
    "]\n",
    "\n",
    "ko_dict_filtered = (\n",
    "    ko_dict_filtered.groupby('word', as_index=False)\n",
    "    .agg({\n",
    "        'freq': 'sum',  # Sum up the 'freq' column for duplicates\n",
    "        **{col: 'first' for col in ko_dict_filtered.columns if col not in ['word', 'freq']}  # Keep the first occurrence for other columns\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>morphs_okt</th>\n",
       "      <th>pos_okt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>우리</td>\n",
       "      <td>18207</td>\n",
       "      <td>['우리']</td>\n",
       "      <td>[('우리', 'Noun')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>무슨</td>\n",
       "      <td>16460</td>\n",
       "      <td>['무슨']</td>\n",
       "      <td>[('무슨', 'Noun')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그냥</td>\n",
       "      <td>15446</td>\n",
       "      <td>['그냥']</td>\n",
       "      <td>[('그냥', 'Noun')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>정말</td>\n",
       "      <td>14968</td>\n",
       "      <td>['정말']</td>\n",
       "      <td>[('정말', 'Noun')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>여기</td>\n",
       "      <td>13454</td>\n",
       "      <td>['여기']</td>\n",
       "      <td>[('여기', 'Noun')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46144</th>\n",
       "      <td>크누센</td>\n",
       "      <td>1</td>\n",
       "      <td>['크누센']</td>\n",
       "      <td>[('크누센', 'Noun')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46145</th>\n",
       "      <td>랜덜</td>\n",
       "      <td>1</td>\n",
       "      <td>['랜덜']</td>\n",
       "      <td>[('랜덜', 'Noun')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46146</th>\n",
       "      <td>크라블</td>\n",
       "      <td>1</td>\n",
       "      <td>['크라블']</td>\n",
       "      <td>[('크라블', 'Noun')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46147</th>\n",
       "      <td>오염수</td>\n",
       "      <td>1</td>\n",
       "      <td>['오염수']</td>\n",
       "      <td>[('오염수', 'Noun')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46148</th>\n",
       "      <td>일배</td>\n",
       "      <td>1</td>\n",
       "      <td>['일배']</td>\n",
       "      <td>[('일배', 'Noun')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46149 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word   freq morphs_okt            pos_okt\n",
       "0       우리  18207     ['우리']   [('우리', 'Noun')]\n",
       "1       무슨  16460     ['무슨']   [('무슨', 'Noun')]\n",
       "2       그냥  15446     ['그냥']   [('그냥', 'Noun')]\n",
       "3       정말  14968     ['정말']   [('정말', 'Noun')]\n",
       "4       여기  13454     ['여기']   [('여기', 'Noun')]\n",
       "...    ...    ...        ...                ...\n",
       "46144  크누센      1    ['크누센']  [('크누센', 'Noun')]\n",
       "46145   랜덜      1     ['랜덜']   [('랜덜', 'Noun')]\n",
       "46146  크라블      1    ['크라블']  [('크라블', 'Noun')]\n",
       "46147  오염수      1    ['오염수']  [('오염수', 'Noun')]\n",
       "46148   일배      1     ['일배']   [('일배', 'Noun')]\n",
       "\n",
       "[46149 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_dict_filtered = pd.read_csv(\"../data/korean_dict-hyunwoongko_korean-word-dict.csv\")\n",
    "ko_dict_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_num</th>\n",
       "      <th>freq</th>\n",
       "      <th>freq_quantile</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>교수</td>\n",
       "      <td>['êµĲ', 'ìĪĺ']</td>\n",
       "      <td>2</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>버스</td>\n",
       "      <td>['ë²Ħ', 'ìĬ¤']</td>\n",
       "      <td>2</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>인천</td>\n",
       "      <td>['ìĿ¸', 'ì²ľ']</td>\n",
       "      <td>2</td>\n",
       "      <td>839.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>사단</td>\n",
       "      <td>['ìĤ¬', 'ëĭ¨']</td>\n",
       "      <td>2</td>\n",
       "      <td>792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>요청</td>\n",
       "      <td>['ìļĶ', 'ì²Ń']</td>\n",
       "      <td>2</td>\n",
       "      <td>579.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>채웅석</td>\n",
       "      <td>['ì±Ħ', 'ìĽħ', 'ìĦĿ']</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>간계</td>\n",
       "      <td>['ê°Ħ', 'ê³Ħ']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>셔벗</td>\n",
       "      <td>['ìħĶ', 'ë²Ĺ']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>건설역</td>\n",
       "      <td>['ê±´ìĦ¤', 'ìĹŃ']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>염좌</td>\n",
       "      <td>['ìĹ¼', 'ì¢Į']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>realword</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    word                 tokens  token_num    freq  freq_quantile     label\n",
       "395   교수         ['êµĲ', 'ìĪĺ']          2  1740.0            0.0  realword\n",
       "349   버스         ['ë²Ħ', 'ìĬ¤']          2  1001.0            0.0  realword\n",
       "472   인천         ['ìĿ¸', 'ì²ľ']          2   839.0            0.0  realword\n",
       "304   사단         ['ìĤ¬', 'ëĭ¨']          2   792.0            0.0  realword\n",
       "418   요청         ['ìļĶ', 'ì²Ń']          2   579.0            0.0  realword\n",
       "..   ...                    ...        ...     ...            ...       ...\n",
       "720  채웅석  ['ì±Ħ', 'ìĽħ', 'ìĦĿ']          3     1.0            0.0  realword\n",
       "343   간계         ['ê°Ħ', 'ê³Ħ']          2     1.0            0.0  realword\n",
       "344   셔벗         ['ìħĶ', 'ë²Ĺ']          2     1.0            0.0  realword\n",
       "346  건설역      ['ê±´ìĦ¤', 'ìĹŃ']          2     1.0            0.0  realword\n",
       "500   염좌         ['ìĹ¼', 'ì¢Į']          2     1.0            0.0  realword\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/hyujang/multilingual-inner-lexicon/data/r1_dataset_Babel-9B-Chat_Korean.csv\")\n",
    "df[df['label']==\"realword\"].sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## German\n",
    "# german_synsets = list(wordnet.all_synsets(lang=\"deu\"))  # 'deu' is the ISO 639-3 code for German -> empty list\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "from lxml import etree\n",
    "import re\n",
    "\n",
    "def get_wordnet_lexicon_local(wnfile):\n",
    "     loc_wn = open(wnfile,\"r\",encoding=\"utf-8\")\n",
    "     wntree = ET.parse(loc_wn)\n",
    "     wnroot = wntree.getroot()\n",
    "     lexicon = wnroot.find('Lexicon')\n",
    "     return lexicon\n",
    " \n",
    "def extract_all_nouns(wordnet_file):\n",
    "    lexicon = get_wordnet_lexicon_local(wordnet_file)\n",
    "    nouns = []\n",
    "    for lexentry in lexicon.iter('LexicalEntry'):\n",
    "        lemma = lexentry.find('Lemma')\n",
    "        lemma_value = lemma.attrib['writtenForm']\n",
    "        pos = lemma.attrib.get('partOfSpeech')\n",
    "        if pos == 'n':\n",
    "            nouns.append(lemma_value)\n",
    "    return nouns\n",
    " \n",
    "wordnet_file = '/work/hyujang/miniconda3/envs/thesis/lib/python3.12/site-packages/odenet/wordnet/deWordNet.xml'\n",
    "nouns = extract_all_nouns(wordnet_file)\n",
    "print(len(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='안녕', tag='NNG', start=0, len=2),\n",
       " Token(form='하', tag='XSA', start=2, len=1),\n",
       " Token(form='세요', tag='EF', start=3, len=2),\n",
       " Token(form='.', tag='SF', start=5, len=1),\n",
       " Token(form='저', tag='NP', start=7, len=1),\n",
       " Token(form='는', tag='JX', start=8, len=1),\n",
       " Token(form='한국어', tag='NNP', start=10, len=3),\n",
       " Token(form='를', tag='JKO', start=13, len=1),\n",
       " Token(form='공부', tag='NNG', start=15, len=2),\n",
       " Token(form='하', tag='XSV', start=17, len=1),\n",
       " Token(form='고', tag='EC', start=18, len=1),\n",
       " Token(form='있', tag='VX', start=20, len=1),\n",
       " Token(form='습니다', tag='EF', start=21, len=3),\n",
       " Token(form='.', tag='SF', start=24, len=1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "# ko_noun_freq_df = pd.read_csv(\"../data/ko_wiki_noun_frequencies.csv\")\n",
    "# df = load_wikipedia_data(lang=\"ko\")\n",
    "\n",
    "kiwi = Kiwi()\n",
    "# l = kiwi.tokenize(df['text'][0])\n",
    "kiwi.tokenize(\"안녕하세요. 저는 한국어를 공부하고 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia data for language: ko\n",
      "Extracting nouns using Okt tokenizer for ko...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text: 100%|██████████| 20000/20000 [11:49<00:00, 28.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summing noun frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summing: 100%|██████████| 20000/20000 [06:42<00:00, 49.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved noun frequencies to ../data/ko_wiki_noun_frequencies_kiwi.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def load_wikipedia_data(lang, sample_size=20000):\n",
    "    wiki = load_dataset(\"wikimedia/wikipedia\", f\"20231101.{lang}\", split=\"train\", columns=[\"text\"])\n",
    "    return wiki.shuffle(seed=2025).select(range(sample_size)).to_pandas()\n",
    "\n",
    "def extract_nouns_with_frequency(text, lang, lemmatizer=None, nlp=None, kiwi=None):\n",
    "    if lang == \"en\":\n",
    "        \"\"\"\n",
    "        NN = Singular or mass noun\n",
    "        NNS = Plural noun\n",
    "        NNP = Singular proper noun (capitalized names)\n",
    "        NNPS = Plural proper noun (capitalized plural names)\n",
    "        \"\"\"\n",
    "        # doc = nlp(text)\n",
    "        # nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged = pos_tag(tokens)\n",
    "        nouns = [lemmatizer.lemmatize(word.lower()) for word, tag in tagged if tag in ['NN', 'NNS']]\n",
    "    elif lang == \"de\":\n",
    "        doc = nlp(text)\n",
    "        nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    elif lang == \"ko\":\n",
    "        # nouns = okt.nouns(text)\n",
    "        doc = kiwi.tokenize(text)\n",
    "        nouns = [token.form for token in doc if token.tag==\"NNG\"]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "    return Counter(nouns)\n",
    "\n",
    "def process_wikipedia_nouns(lang):\n",
    "    global df\n",
    "    \n",
    "    output_file = f\"../data/{lang}_wiki_noun_frequencies_lemmatized.csv\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file '{output_file}' already exists. Skipping processing.\")\n",
    "        return  # Stop the function if the file exists\n",
    "\n",
    "    print(f\"Loading Wikipedia data for language: {lang}\")\n",
    "    df = load_wikipedia_data(lang)\n",
    "    \n",
    "    if lang == \"en\":\n",
    "        print(\"Extracting nouns using nltk for English...\")\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_frequency(text, lang, lemmatizer=lemmatizer))\n",
    "\n",
    "    elif lang == \"de\":\n",
    "        print(f\"Extracting nouns using spaCy model for {lang}...\")\n",
    "        nlp = spacy.load(f\"{lang}_core_news_sm\")\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_frequency(text, lang, nlp=nlp))\n",
    "\n",
    "    elif lang == \"ko\":\n",
    "        print(f\"Extracting nouns using Okt tokenizer for {lang}...\")\n",
    "        # okt = Okt()\n",
    "        kiwi = Kiwi()\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_frequency(text, lang, kiwi=kiwi))\n",
    "    \n",
    "    print(\"Summing noun frequencies...\")\n",
    "    combined_noun_frequencies = sum((Counter(freq_dict) for freq_dict in tqdm(df[\"noun_frequencies\"], desc=\"Summing\")), Counter())\n",
    "    noun_frequencies_df = pd.DataFrame.from_dict(combined_noun_frequencies, orient=\"index\", columns=[\"frequency\"])\n",
    "    noun_frequencies_df.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "    noun_frequencies_df.reset_index(inplace=True)\n",
    "    noun_frequencies_df.columns = [\"word\", \"freq\"]\n",
    "    # output_file = f\"../data/{lang}_wiki_noun_frequencies.csv\"\n",
    "    noun_frequencies_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved noun frequencies to {output_file}\")\n",
    "\n",
    "\n",
    "process_wikipedia_nouns(\"en\")\n",
    "# process_wikipedia_nouns(\"de\")\n",
    "# process_wikipedia_nouns(\"ko\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
