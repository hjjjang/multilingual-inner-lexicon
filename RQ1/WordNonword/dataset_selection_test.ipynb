{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Dictionaries\n",
    "- no frequency check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## English\n",
    "# nltk.download('averaged_perceptron_tagger_eng', download_dir='/work/hyujang/miniconda3/envs/thesis/nltk_data') # for en-pos tagging\n",
    "nltk.download('wordnet', download_dir='/work/hyujang/miniconda3/envs/thesis/nltk_data') # en vocabulary\n",
    "# en_ds = load_dataset(\"manu/project_gutenberg\", split=\"en\", streaming=True)\n",
    "# en_ds = load_dataset(\"deepmind/pg19\", split=None, streaming=True, trust_remote_code=True)\n",
    "# en_ds = load_dataset(\"cambridge-climb/BabyLM\", split=None, streaming=True, trust_remote_code=True) # smaller size\n",
    "# en_ds = load_dataset(\"marksverdhei/wordnet-definitions-en-2021\")\n",
    "# en_df = en_ds['train'].to_pandas()\n",
    "# data_list = list(en_ds.take(100))\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "noun_synsets = list(wordnet.all_synsets(pos=wordnet.NOUN))\n",
    "en_nouns = set(lemma.name() for synset in noun_synsets for lemma in synset.lemmas() if \"_\" not in lemma.name())\n",
    "print(len(en_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Korean\n",
    "# ds = load_dataset(\"wicho/kor_3i4k\")\n",
    "# ds = load_dataset(\"binjang/NIKL-korean-english-dictionary\")\n",
    "import sys\n",
    "# sys.setdefaultencoding(\"utf-8\")\n",
    "ds = load_dataset(\"hyunwoongko/korean-word-dict\")\n",
    "ko_dict = ds['train'].to_pandas()\n",
    "# ds = load_dataset(\"hac541309/woori_spring_dict\")\n",
    "# df = ds['train'].to_pandas()\n",
    "# df_split = df['text'].str.split(\", \", expand=True) # takes too long\n",
    "\n",
    "# from konlpy.tag import Okt # TODO: Mecab\n",
    "# import unicodedata\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "# okt = Okt()\n",
    "# ko_dict['word'] = ko_dict['word'].progress_apply(lambda x: unicodedata.normalize(\"NFC\", x)) # ㄴㅐ -> 내\n",
    "# ko_dict['pos_okt'] = ko_dict['word'].progress_apply(lambda x: okt.pos(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1508680/1508680 [02:02<00:00, 12305.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from tqdm import tqdm  # Make sure tqdm is installed and imported\n",
    "\n",
    "kiwi = Kiwi()\n",
    "ko_dict['kiwi'] = None\n",
    "\n",
    "for i, text in tqdm(enumerate(ko_dict['word']), total=len(ko_dict)):\n",
    "    ko_dict.at[i, 'kiwi'] = kiwi.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_dict.to_csv(\"ko_dict.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>kiwi</th>\n",
       "      <th>kiwi_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>내가</td>\n",
       "      <td>39801</td>\n",
       "      <td>[(내가, NNG, 0, 4)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그</td>\n",
       "      <td>35563</td>\n",
       "      <td>[(그, NNG, 0, 2)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>난</td>\n",
       "      <td>35163</td>\n",
       "      <td>[(난, NNG, 0, 3)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>내</td>\n",
       "      <td>33774</td>\n",
       "      <td>[(내, NNG, 0, 2)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>안</td>\n",
       "      <td>32915</td>\n",
       "      <td>[(안, NNG, 0, 3)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480627</th>\n",
       "      <td>흥창</td>\n",
       "      <td>0</td>\n",
       "      <td>[(흥창, NNG, 0, 6)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480628</th>\n",
       "      <td>흥화공업</td>\n",
       "      <td>0</td>\n",
       "      <td>[(흥화공업, NNG, 0, 11)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480629</th>\n",
       "      <td>희성건설</td>\n",
       "      <td>0</td>\n",
       "      <td>[(희성건설, NNG, 0, 11)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480630</th>\n",
       "      <td>희성금속</td>\n",
       "      <td>0</td>\n",
       "      <td>[(희성금속, NNG, 0, 11)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480631</th>\n",
       "      <td>힐튼재단</td>\n",
       "      <td>0</td>\n",
       "      <td>[(힐튼재단, NNG, 0, 11)]</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1480632 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word   freq                         kiwi kiwi_pos\n",
       "0               내가  39801          [(내가, NNG, 0, 4)]      NNG\n",
       "1                 그  35563            [(그, NNG, 0, 2)]      NNG\n",
       "2                난  35163           [(난, NNG, 0, 3)]      NNG\n",
       "3                 내  33774            [(내, NNG, 0, 2)]      NNG\n",
       "4                안  32915           [(안, NNG, 0, 3)]      NNG\n",
       "...              ...    ...                          ...      ...\n",
       "1480627       흥창      0        [(흥창, NNG, 0, 6)]      NNG\n",
       "1480628  흥화공업      0  [(흥화공업, NNG, 0, 11)]      NNG\n",
       "1480629  희성건설      0  [(희성건설, NNG, 0, 11)]      NNG\n",
       "1480630  희성금속      0  [(희성금속, NNG, 0, 11)]      NNG\n",
       "1480631  힐튼재단      0  [(힐튼재단, NNG, 0, 11)]      NNG\n",
       "\n",
       "[1480632 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ko_dict = ko_dict[ko_dict['kiwi'].apply(lambda x: len(x) == 1)].reset_index(drop=True)\n",
    "ko_dict['kiwi_pos' ] = ko_dict['kiwi'].apply(lambda x: x[0].tag)\n",
    "# ko_dict[ko_dict['kiwi_pos'] == 'SL']\n",
    "ko_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_dict_filtered = ko_dict[\n",
    "    (ko_dict['pos_okt'].apply(len) == 1) & \n",
    "    (ko_dict['pos_okt'].apply(lambda x: x[0][1] == 'Noun')) &\n",
    "    (ko_dict['word'].apply(lambda x: len(x) > 1)) &\n",
    "    (ko_dict['freq'].apply(lambda x: x > 0))\n",
    "]\n",
    "\n",
    "ko_dict_filtered = (\n",
    "    ko_dict_filtered.groupby('word', as_index=False)\n",
    "    .agg({\n",
    "        'freq': 'sum',  # Sum up the 'freq' column for duplicates\n",
    "        **{col: 'first' for col in ko_dict_filtered.columns if col not in ['word', 'freq']}  # Keep the first occurrence for other columns\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ko_dict_filtered = pd.read_csv(\"../data/korean_dict-hyunwoongko_korean-word-dict.csv\")\n",
    "ko_dict_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/hyujang/multilingual-inner-lexicon/data/r1_dataset_Babel-9B-Chat_Korean.csv\")\n",
    "df[df['label']==\"realword\"].sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## German\n",
    "# german_synsets = list(wordnet.all_synsets(lang=\"deu\"))  # 'deu' is the ISO 639-3 code for German -> empty list\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "from lxml import etree\n",
    "import re\n",
    "\n",
    "def get_wordnet_lexicon_local(wnfile):\n",
    "     loc_wn = open(wnfile,\"r\",encoding=\"utf-8\")\n",
    "     wntree = ET.parse(loc_wn)\n",
    "     wnroot = wntree.getroot()\n",
    "     lexicon = wnroot.find('Lexicon')\n",
    "     return lexicon\n",
    " \n",
    "def extract_all_nouns(wordnet_file):\n",
    "    lexicon = get_wordnet_lexicon_local(wordnet_file)\n",
    "    nouns = []\n",
    "    for lexentry in lexicon.iter('LexicalEntry'):\n",
    "        lemma = lexentry.find('Lemma')\n",
    "        lemma_value = lemma.attrib['writtenForm']\n",
    "        pos = lemma.attrib.get('partOfSpeech')\n",
    "        if pos == 'n':\n",
    "            nouns.append(lemma_value)\n",
    "    return nouns\n",
    " \n",
    "wordnet_file = '/work/hyujang/miniconda3/envs/thesis/lib/python3.12/site-packages/odenet/wordnet/deWordNet.xml'\n",
    "nouns = extract_all_nouns(wordnet_file)\n",
    "print(len(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/olastor/german-word-frequencies/refs/heads/main/opensubtitles/opensubtitles_cistem_freq.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.to_csv(\"opensubtitles_cistem_freq.csv\", index=False)  # Save it locally if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "# ko_noun_freq_df = pd.read_csv(\"../data/ko_wiki_noun_frequencies.csv\")\n",
    "# df = load_wikipedia_data(lang=\"ko\")\n",
    "\n",
    "kiwi = Kiwi()\n",
    "# l = kiwi.tokenize(df['text'][0])\n",
    "kiwi.tokenize(\"안녕하세요. 저는 한국어를 공부하고 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang=\"de\", processors=\"tokenize,pos,lemma\")\n",
    "text = \"Angela Merkel besuchte das große Museum.\"\n",
    "doc = nlp(text)\n",
    "doc.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df_de = load_wikipedia_data(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import stanza\n",
    "\n",
    "def load_wikipedia_data(lang, sample_size=20000):\n",
    "    wiki = load_dataset(\"wikimedia/wikipedia\", f\"20231101.{lang}\", split=\"train\", columns=[\"text\"])\n",
    "    return wiki.shuffle(seed=2025).select(range(sample_size)).to_pandas()\n",
    "\n",
    "def extract_nouns_with_frequency(text, lang, lemmatizer=None, nlp=None, kiwi=None):\n",
    "    if lang == \"en\":\n",
    "        \"\"\"\n",
    "        NN = Singular or mass noun\n",
    "        NNS = Plural noun\n",
    "        NNP = Singular proper noun (capitalized names)\n",
    "        NNPS = Plural proper noun (capitalized plural names)\n",
    "        \"\"\"\n",
    "        # doc = nlp(text)\n",
    "        # nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged = pos_tag(tokens)\n",
    "        nouns = [lemmatizer.lemmatize(word.lower()) for word, tag in tagged if tag in ['NN', 'NNS']]\n",
    "        nouns = [\n",
    "            lemmatizer.lemmatize(word.lower()) \n",
    "            for word, tag in tagged \n",
    "            if tag in ['NN', 'NNS']\n",
    "            # if tag in ['NN', 'NNS'] and not re.search(r'[\\W\\d]', word)\n",
    "        ]\n",
    "    elif lang == \"de\":\n",
    "        doc = nlp(text)\n",
    "        nouns = [\n",
    "            word.lemma  # Use lemma instead of the original word\n",
    "            for sentence in doc.sentences\n",
    "            for word in sentence.words\n",
    "            if word.upos == \"NOUN\"  # Filter only nouns\n",
    "        ]\n",
    "    elif lang == \"ko\":\n",
    "        # nouns = okt.nouns(text)\n",
    "        doc = kiwi.tokenize(text)\n",
    "        nouns = [\n",
    "            token.form \n",
    "            for token in doc \n",
    "            if token.tag == \"NNG\"\n",
    "            # if token.tag == \"NNG\" and not re.search(r'[\\W\\d]', token.form)\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "    return Counter(nouns)\n",
    "\n",
    "def process_wikipedia_nouns(lang):\n",
    "    # global df\n",
    "    \n",
    "    output_file = f\"../data/{lang}_wiki_noun_frequencies_lemmatized.csv\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file '{output_file}' already exists. Skipping processing.\")\n",
    "        return  # Stop the function if the file exists\n",
    "\n",
    "    print(f\"Loading Wikipedia data for language: {lang}\")\n",
    "    # df = load_wikipedia_data(lang)\n",
    "    df = df_de.iloc[:10000]\n",
    "    \n",
    "    if lang == \"en\":\n",
    "        print(\"Extracting nouns using nltk for English...\")\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_frequency(text, lang, lemmatizer=lemmatizer))\n",
    "\n",
    "    elif lang == \"de\":\n",
    "        print(f\"Extracting nouns using Stanza model for {lang}...\")\n",
    "        nlp = stanza.Pipeline(lang=\"de\", processors=\"tokenize,pos,lemma\", use_gpu=True)  # Initialize Stanza pipeline\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_frequency(text, lang, nlp=nlp))\n",
    "\n",
    "    elif lang == \"ko\":\n",
    "        print(f\"Extracting nouns using Okt tokenizer for {lang}...\")\n",
    "        # okt = Okt()\n",
    "        kiwi = Kiwi()\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_frequency(text, lang, kiwi=kiwi))\n",
    "    \n",
    "    print(\"Summing noun frequencies...\")\n",
    "    combined_noun_frequencies = sum((Counter(freq_dict) for freq_dict in tqdm(df[\"noun_frequencies\"], desc=\"Summing\")), Counter())\n",
    "    noun_frequencies_df = pd.DataFrame.from_dict(combined_noun_frequencies, orient=\"index\", columns=[\"frequency\"])\n",
    "    noun_frequencies_df.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "    noun_frequencies_df.reset_index(inplace=True)\n",
    "    noun_frequencies_df.columns = [\"word\", \"freq\"]\n",
    "    if lang == \"en\":\n",
    "        noun_frequencies_df = noun_frequencies_df[~noun_frequencies_df['word'].str.contains(r'[^a-zA-Z]', na=False)]\n",
    "        noun_frequencies_df.reset_index(inplace=True, drop=True)\n",
    "        noun_frequencies_df.drop_duplicates(subset=['word'], inplace=True)\n",
    "        noun_frequencies_df.dropna(inplace=True)\n",
    "    if lang == \"de\":\n",
    "        noun_frequencies_df = noun_frequencies_df[~noun_frequencies_df['word'].str.contains(r'[^a-zA-ZäöüÄÖÜß]', na=False)]\n",
    "        noun_frequencies_df.reset_index(inplace=True, drop=True)\n",
    "        noun_frequencies_df.drop_duplicates(subset=['word'], inplace=True)\n",
    "        noun_frequencies_df.dropna(inplace=True)\n",
    "    if lang == \"ko\":\n",
    "        noun_frequencies_df = noun_frequencies_df[~noun_frequencies_df['word'].str.contains(r'[^\\uac00-\\ud7a3]', na=False)]\n",
    "        noun_frequencies_df.reset_index(inplace=True, drop=True)\n",
    "        noun_frequencies_df.drop_duplicates(subset=['word'], inplace=True)\n",
    "        noun_frequencies_df.dropna(inplace=True)\n",
    "    # output_file = f\"../data/{lang}_wiki_noun_frequencies.csv\"\n",
    "    noun_frequencies_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved noun frequencies to {output_file}\")\n",
    "\n",
    "\n",
    "# process_wikipedia_nouns(\"en\")\n",
    "process_wikipedia_nouns(\"de\")\n",
    "# process_wikipedia_nouns(\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import stanza\n",
    "\n",
    "def load_wikipedia_data(lang, sample_size=20000):\n",
    "    wiki = load_dataset(\"wikimedia/wikipedia\", f\"20231101.{lang}\", split=\"train\", columns=[\"text\"])\n",
    "    return wiki.shuffle(seed=2025).select(range(sample_size)).to_pandas()\n",
    "\n",
    "def extract_nouns_with_frequency(text, lang, lemmatizer=None, nlp=None, kiwi=None):\n",
    "    if lang == \"en\":\n",
    "        \"\"\"\n",
    "        NN = Singular or mass noun\n",
    "        NNS = Plural noun\n",
    "        NNP = Singular proper noun (capitalized names)\n",
    "        NNPS = Plural proper noun (capitalized plural names)\n",
    "        \"\"\"\n",
    "        # doc = nlp(text)\n",
    "        # nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged = pos_tag(tokens)\n",
    "        nouns = [lemmatizer.lemmatize(word.lower()) for word, tag in tagged if tag in ['NN', 'NNS']]\n",
    "        nouns = [\n",
    "            lemmatizer.lemmatize(word.lower()) \n",
    "            for word, tag in tagged \n",
    "            if tag in ['NN', 'NNS']\n",
    "            # if tag in ['NN', 'NNS'] and not re.search(r'[\\W\\d]', word)\n",
    "        ]\n",
    "    elif lang == \"de\":\n",
    "        doc = nlp(text)\n",
    "        nouns = [\n",
    "            word.lemma  # Use lemma instead of the original word\n",
    "            for sentence in doc.sentences\n",
    "            for word in sentence.words\n",
    "            if word.upos == \"NOUN\"  # Filter only nouns\n",
    "        ]\n",
    "    elif lang == \"ko\":\n",
    "        # nouns = okt.nouns(text)\n",
    "        doc = kiwi.tokenize(text)\n",
    "        nouns = [\n",
    "            token.form \n",
    "            for token in doc \n",
    "            if token.tag == \"NNG\"\n",
    "            # if token.tag == \"NNG\" and not re.search(r'[\\W\\d]', token.form)\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "    return Counter(nouns)\n",
    "\n",
    "def process_wikipedia_nouns(lang):\n",
    "    # global df\n",
    "    \n",
    "    output_file = f\"../data/{lang}_wiki_noun_frequencies_lemmatized.csv\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file '{output_file}' already exists. Skipping processing.\")\n",
    "        return  # Stop the function if the file exists\n",
    "\n",
    "    print(f\"Loading Wikipedia data for language: {lang}\")\n",
    "    # df = load_wikipedia_data(lang)\n",
    "    df = df_de.iloc[:10000]\n",
    "    \n",
    "    if lang == \"en\":\n",
    "        print(\"Extracting nouns using nltk for English...\")\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_frequency(text, lang, lemmatizer=lemmatizer))\n",
    "\n",
    "    elif lang == \"de\":\n",
    "        print(f\"Extracting nouns using Stanza model for {lang}...\")\n",
    "        nlp = stanza.Pipeline(lang=\"de\", processors=\"tokenize,pos,lemma\", use_gpu=True)  # Initialize Stanza pipeline\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_frequency(text, lang, nlp=nlp))\n",
    "\n",
    "    elif lang == \"ko\":\n",
    "        print(f\"Extracting nouns using Okt tokenizer for {lang}...\")\n",
    "        # okt = Okt()\n",
    "        kiwi = Kiwi()\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_frequency(text, lang, kiwi=kiwi))\n",
    "    \n",
    "    print(\"Summing noun frequencies...\")\n",
    "    combined_noun_frequencies = sum((Counter(freq_dict) for freq_dict in tqdm(df[\"noun_frequencies\"], desc=\"Summing\")), Counter())\n",
    "    noun_frequencies_df = pd.DataFrame.from_dict(combined_noun_frequencies, orient=\"index\", columns=[\"frequency\"])\n",
    "    noun_frequencies_df.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "    noun_frequencies_df.reset_index(inplace=True)\n",
    "    noun_frequencies_df.columns = [\"word\", \"freq\"]\n",
    "    if lang == \"en\":\n",
    "        noun_frequencies_df = noun_frequencies_df[~noun_frequencies_df['word'].str.contains(r'[^a-zA-Z]', na=False)]\n",
    "        noun_frequencies_df.reset_index(inplace=True, drop=True)\n",
    "        noun_frequencies_df.drop_duplicates(subset=['word'], inplace=True)\n",
    "        noun_frequencies_df.dropna(inplace=True)\n",
    "    if lang == \"de\":\n",
    "        noun_frequencies_df = noun_frequencies_df[~noun_frequencies_df['word'].str.contains(r'[^a-zA-ZäöüÄÖÜß]', na=False)]\n",
    "        noun_frequencies_df.reset_index(inplace=True, drop=True)\n",
    "        noun_frequencies_df.drop_duplicates(subset=['word'], inplace=True)\n",
    "        noun_frequencies_df.dropna(inplace=True)\n",
    "    if lang == \"ko\":\n",
    "        noun_frequencies_df = noun_frequencies_df[~noun_frequencies_df['word'].str.contains(r'[^\\uac00-\\ud7a3]', na=False)]\n",
    "        noun_frequencies_df.reset_index(inplace=True, drop=True)\n",
    "        noun_frequencies_df.drop_duplicates(subset=['word'], inplace=True)\n",
    "        noun_frequencies_df.dropna(inplace=True)\n",
    "    # output_file = f\"../data/{lang}_wiki_noun_frequencies.csv\"\n",
    "    noun_frequencies_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved noun frequencies to {output_file}\")\n",
    "\n",
    "\n",
    "# process_wikipedia_nouns(\"en\")\n",
    "process_wikipedia_nouns(\"de\")\n",
    "# process_wikipedia_nouns(\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df_de.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "lang = \"de\"\n",
    "noun_df = pd.read_csv(f\"../data/{lang}_wiki_noun_frequencies_lemmatized.csv\")\n",
    "# noun_df = noun_df[~noun_df['word'].str.contains(r'[^a-zA-Z]', na=False)]\n",
    "# noun_df.reset_index(inplace=True, drop=True)\n",
    "# noun_df.to_csv(f\"../data/{lang}_wiki_noun_frequencies_lemmatized.csv\", index=False)\n",
    "# noun_df[noun_df['word'].str.contains(r'[^a-zA-Z]', na=False)]\n",
    "noun_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noun_df.drop_duplicates(subset=['word'], inplace=True)\n",
    "noun_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
