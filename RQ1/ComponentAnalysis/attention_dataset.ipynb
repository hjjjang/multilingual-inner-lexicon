{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import stanza\n",
    "from kiwipiepy import Kiwi \n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_wikipedia_data(lang, sample_size=20000):\n",
    "    wiki = load_dataset(\"wikimedia/wikipedia\", f\"20231101.{lang}\", split=\"train\", columns=[\"text\"])\n",
    "    return wiki.shuffle(seed=2025).select(range(sample_size)).to_pandas()\n",
    "\n",
    "def clean_text_list(text_list):\n",
    "    cleaned = []\n",
    "    for item in text_list:\n",
    "        # Split lines to handle \\n cleanly\n",
    "        # Remove extra spaces, tabs, etc.\n",
    "        item = item.strip()                         # Strip leading/trailing space\n",
    "        item = re.sub(r'\\s+', ' ', item)            # Normalize all whitespace            line = re.sub(r'[^\\S\\r\\n]+', ' ', line)     # Remove extra non-visible spaces\n",
    "        if item:                                    # Remove empty strings\n",
    "            cleaned.append(item)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def extract_nouns_with_sentences_and_frequency(text, lang, lemmatizer=None, nlp=None, kiwi=None):\n",
    "    \"\"\"\n",
    "    Extract nouns, their frequencies, and the sentences containing them from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        lang (str): The language of the text ('en', 'de', 'ko').\n",
    "        lemmatizer (WordNetLemmatizer, optional): Lemmatizer for English.\n",
    "        nlp (stanza.Pipeline, optional): Stanza pipeline for German.\n",
    "        kiwi (Kiwi, optional): Kiwi tokenizer for Korean.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are nouns and values are tuples of (frequency, list of sentences containing the noun).\n",
    "    \"\"\"\n",
    "    noun_to_data = {}\n",
    "\n",
    "    # Split the text into sentences\n",
    "    # sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "    if lang == \"en\":\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged = pos_tag(tokens)\n",
    "        nouns = [\n",
    "            lemmatizer.lemmatize(word.lower()) \n",
    "            for word, tag in tagged \n",
    "            if tag in ['NN', 'NNS']\n",
    "        ]\n",
    "        sentences = sent_tokenize(text)\n",
    "    elif lang == \"de\":\n",
    "        doc = nlp(text)\n",
    "        nouns = [\n",
    "            word.lemma\n",
    "            for sentence in doc.sentences\n",
    "            for word in sentence.words\n",
    "            if word.upos == \"NOUN\"\n",
    "        ]\n",
    "        sentences = [sentence.text for sentence in doc.sentences]\n",
    "    elif lang == \"ko\":\n",
    "        doc = kiwi.tokenize(text)\n",
    "        nouns = [\n",
    "            token.form\n",
    "            for token in doc\n",
    "            if token.tag == \"NNG\"\n",
    "        ]\n",
    "        sentences = kiwi.split_into_sents(text)\n",
    "        sentences = [s.text for s in sentences]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "    \n",
    "    sentences = clean_text_list(sentences)\n",
    "\n",
    "    # Count noun frequencies\n",
    "    noun_frequencies = Counter(nouns)\n",
    "\n",
    "    # Map nouns to sentences\n",
    "    for sentence in sentences:\n",
    "        for noun in noun_frequencies.keys():\n",
    "            if re.search(rf'\\b{re.escape(noun)}\\b', sentence, re.IGNORECASE):\n",
    "                if noun not in noun_to_data:\n",
    "                    noun_to_data[noun] = {\"frequency\": noun_frequencies[noun], \"sentences\": []}\n",
    "                noun_to_data[noun][\"sentences\"].append(sentence)\n",
    "\n",
    "    return noun_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wikipedia_nouns(lang):\n",
    "    df = load_wikipedia_data(lang)\n",
    "    \n",
    "    if lang == \"en\":\n",
    "        print(\"Extracting nouns using nltk for English...\")\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_sentences_and_frequency(text, lang, lemmatizer=lemmatizer))\n",
    "\n",
    "    elif lang == \"de\":\n",
    "        print(f\"Extracting nouns using Stanza model for {lang}...\")\n",
    "        nlp = stanza.Pipeline(lang=\"de\", processors=\"tokenize,pos,lemma\", use_gpu=True)  # Initialize Stanza pipeline\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_sentences_and_frequency(text, lang, nlp=nlp))\n",
    "\n",
    "    elif lang == \"ko\":\n",
    "        print(f\"Extracting nouns using Kiwi tokenizer for {lang}...\")\n",
    "        kiwi = Kiwi()\n",
    "        tqdm.pandas(desc=\"Processing text\")\n",
    "        df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_sentences_and_frequency(text, lang, kiwi=kiwi))\n",
    "\n",
    "    final_data = defaultdict(lambda: {\"frequency\": 0, \"sentences\": []})\n",
    "\n",
    "    # Iterate over the rows of df[\"noun_frequencies\"]\n",
    "    for noun_data in df[\"noun_frequencies\"]:\n",
    "        for noun, data in noun_data.items():\n",
    "            # Add the frequency of the noun\n",
    "            final_data[noun][\"frequency\"] += data[\"frequency\"]\n",
    "            # Extend the list of sentences containing the noun\n",
    "            final_data[noun][\"sentences\"].extend(data[\"sentences\"])\n",
    "\n",
    "    # Convert the defaultdict to a regular dictionary\n",
    "    final_data = {noun: {\"frequency\": data[\"frequency\"], \"sentences\": list(set(data[\"sentences\"]))} \n",
    "                for noun, data in final_data.items()}\n",
    "\n",
    "    # Convert the final data into a DataFrame (optional)\n",
    "    noun_frequencies_df = pd.DataFrame([\n",
    "        {\"noun\": noun, \"frequency\": data[\"frequency\"], \"sentences\": data[\"sentences\"]}\n",
    "        for noun, data in final_data.items()\n",
    "    ])\n",
    "\n",
    "    # Display the DataFrame\n",
    "    noun_frequencies_df.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "    if lang == \"en\":\n",
    "        noun_frequencies_df = noun_frequencies_df[~noun_frequencies_df['word'].str.contains(r'[^a-zA-Z]', na=False)]\n",
    "    if lang == \"de\":\n",
    "        noun_frequencies_df = noun_frequencies_df[~noun_frequencies_df['word'].str.contains(r'[^a-zA-ZäöüÄÖÜß]', na=False)]\n",
    "    if lang == \"ko\":\n",
    "        noun_frequencies_df = noun_frequencies_df[~noun_frequencies_df['word'].str.contains(r'[^\\uac00-\\ud7a3]', na=False)]\n",
    "\n",
    "    noun_frequencies_df = noun_frequencies_df[noun_frequencies_df[\"frequency\"] >= 4]\n",
    "    noun_frequencies_df = noun_frequencies_df.drop_duplicates(subset=\"noun\").reset_index(drop=True)\n",
    "\n",
    "    selected_sentences = {}\n",
    "    for word, texts, frequency in tqdm(noun_frequencies_df[[\"noun\", \"sentences\", \"frequency\"]].values):\n",
    "        # Filter sentences with word length between 10 and 20\n",
    "        valid_sentences = [text for text in texts if 10 <= len(text.split()) <= 20]\n",
    "        \n",
    "        if valid_sentences:\n",
    "            # If there are valid sentences, choose the first one\n",
    "            selected_sentence = valid_sentences[0]\n",
    "        else:\n",
    "            # If no valid sentences, choose the one closest to the range\n",
    "            selected_sentence = min(texts, key=lambda text: abs(len(text.split()) - 15))  # Closest to the midpoint (15)\n",
    "\n",
    "        # Save the selected sentence, its word length, and the original frequency\n",
    "        selected_sentences[word] = {\n",
    "            \"selected_sentence\": selected_sentence,\n",
    "            \"sentence_length\": len(selected_sentence.split()),\n",
    "            \"original_frequency\": frequency\n",
    "        }\n",
    "\n",
    "    # Convert the selected sentences into a DataFrame\n",
    "    selected_sentences_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"word\": word,\n",
    "                \"selected_sentence\": data[\"selected_sentence\"],\n",
    "                \"sentence_length\": data[\"sentence_length\"],\n",
    "                \"original_frequency\": data[\"original_frequency\"]\n",
    "            }\n",
    "            for word, data in selected_sentences.items()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return selected_sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tqdm.pandas(desc=\"Processing text\")\n",
    "df[\"noun_frequencies\"] = df[\"text\"].progress_apply(lambda text: extract_nouns_with_sentences_and_frequency(text, lang=\"en\", lemmatizer=lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to store the aggregated data\n",
    "final_data = defaultdict(lambda: {\"frequency\": 0, \"sentences\": []})\n",
    "\n",
    "# Iterate over the rows of df[\"noun_frequencies\"]\n",
    "for noun_data in df[\"noun_frequencies\"]:\n",
    "    for noun, data in noun_data.items():\n",
    "        # Add the frequency of the noun\n",
    "        final_data[noun][\"frequency\"] += data[\"frequency\"]\n",
    "        # Extend the list of sentences containing the noun\n",
    "        final_data[noun][\"sentences\"].extend(data[\"sentences\"])\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary\n",
    "final_data = {noun: {\"frequency\": data[\"frequency\"], \"sentences\": list(set(data[\"sentences\"]))} \n",
    "              for noun, data in final_data.items()}\n",
    "\n",
    "# Convert the final data into a DataFrame (optional)\n",
    "final_df = pd.DataFrame([\n",
    "    {\"noun\": noun, \"frequency\": data[\"frequency\"], \"sentences\": data[\"sentences\"]}\n",
    "    for noun, data in final_data.items()\n",
    "])\n",
    "\n",
    "# Display the DataFrame\n",
    "final_df.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[~final_df['noun'].str.contains(r'[^a-zA-Z]', na=False)].drop_duplicates(subset=\"noun\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[final_df[\"frequency\"] >= 4]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentences = {}\n",
    "for word, texts in tqdm(final_df[[\"noun\",\"sentences\"]].values):\n",
    "\n",
    "    # Filter sentences with word length between 10 and 20\n",
    "    valid_sentences = [text for text in texts if 10 <= len(text.split()) <= 20]\n",
    "    \n",
    "    if valid_sentences:\n",
    "        # If there are valid sentences, choose the first one\n",
    "        selected_sentence = valid_sentences[0]\n",
    "    else:\n",
    "        # If no valid sentences, choose the one closest to the range\n",
    "        selected_sentence = min(texts, key=lambda text: abs(len(text.split()) - 15))  # Closest to the midpoint (15)\n",
    "\n",
    "    # Save the selected sentence and its word length\n",
    "    selected_sentences[word] = {\n",
    "        \"selected_sentence\": selected_sentence,\n",
    "        \"sentence_length\": len(selected_sentence.split())\n",
    "    }\n",
    "\n",
    "# # Convert the selected sentences into a DataFrame\n",
    "# selected_sentences_df = pd.DataFrame(\n",
    "#     [{\"word\": word, \"selected_sentence\": data[\"selected_sentence\"], \"sentence_length\": data[\"sentence_length\"]}\n",
    "#      for word, data in selected_sentences.items()]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"English\"\n",
    "LANGUAGE_MAP = {\n",
    "    \"English\": \"en\",\n",
    "    \"German\": \"de\",\n",
    "    \"Korean\": \"ko\"}\n",
    "# MODEL_NAME = \"Babel-9B-Chat\"\n",
    "MODEL_NAME = \"gemma-3-12b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_wikipedia_data(lang, sample_size=20000):\n",
    "    wiki = load_dataset(\"wikimedia/wikipedia\", f\"20231101.{lang}\", split=\"train\", columns=[\"text\"])\n",
    "    return wiki.shuffle(seed=2025).select(range(sample_size)).to_pandas()\n",
    "\n",
    "df = load_wikipedia_data(lang=LANGUAGE_MAP[LANGUAGE])\n",
    "\n",
    "path = f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordNonword/r1_dataset_{MODEL_NAME}_{LANGUAGE}-wiki-2token.csv\"\n",
    "word_list = pd.read_csv(path)[\"word\"].tolist()\n",
    "\n",
    "# Preprocess the text column to tokenize and normalize words\n",
    "df['processed_text'] = df['text'].str.lower().str.findall(r'\\b\\w+\\b')\n",
    "\n",
    "# Create an inverted index: a mapping from each word to the rows it appears in\n",
    "inverted_index = defaultdict(list)\n",
    "for idx, words in enumerate(df['processed_text']):\n",
    "    for word in set(words):  # Use set to avoid duplicate entries for the same word in a row\n",
    "        inverted_index[word].append(idx)\n",
    "\n",
    "# Build the word_to_texts dictionary using the inverted index\n",
    "word_to_texts = {}\n",
    "for word in tqdm(word_list):\n",
    "    normalized_word = word.lower()  # Normalize the word\n",
    "    matching_indices = inverted_index.get(normalized_word, [])  # Get matching row indices\n",
    "    matching_texts = df.loc[matching_indices, 'text'].tolist()  # Retrieve matching texts\n",
    "\n",
    "    # Extract sentences containing the word\n",
    "    sentences_with_word = []\n",
    "    for text in matching_texts:\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)  # Split text into sentences\n",
    "        for sentence in sentences:\n",
    "            if re.search(rf'\\b{normalized_word}\\b', sentence, re.IGNORECASE):  # Check if the word is in the sentence\n",
    "                sentences_with_word.append(sentence)\n",
    "\n",
    "    word_to_texts[word] = sentences_with_word  # Save only the sentences containing the word\n",
    "    \n",
    "    \n",
    "# # Create a DataFrame from word_to_texts\n",
    "# data = []\n",
    "# for word, texts in word_to_texts.items():\n",
    "#     num_texts = len(texts)  # Length of the value list\n",
    "#     avg_word_len = sum(len(text.split()) for text in texts) / num_texts if num_texts > 0 else 0  # Average word length\n",
    "#     min_word_len = min(len(text.split()) for text in texts) if texts else 0  # Minimum word length\n",
    "#     data.append({\"word\": word, \"num_texts\": num_texts, \"avg_word_len\": avg_word_len, \"min_word_len\": min_word_len, \"texts\": texts})\n",
    "\n",
    "# # Convert the list of dictionaries into a DataFrame\n",
    "# word_to_texts_df = pd.DataFrame(data)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# word_to_texts_df.head()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dictionary to store the selected sentence and its word length for each word\n",
    "selected_sentences = {}\n",
    "\n",
    "for word, texts in tqdm(word_to_texts.items()):\n",
    "    if not texts:\n",
    "        # If the texts list is empty, assign None or a placeholder\n",
    "        selected_sentences[word] = {\"selected_sentence\": None, \"sentence_length\": 0}\n",
    "        continue\n",
    "\n",
    "    # Filter sentences with word length between 10 and 20\n",
    "    valid_sentences = [text for text in texts if 10 <= len(text.split()) <= 20]\n",
    "    \n",
    "    if valid_sentences:\n",
    "        # If there are valid sentences, choose the first one\n",
    "        selected_sentence = valid_sentences[0]\n",
    "    else:\n",
    "        # If no valid sentences, choose the one closest to the range\n",
    "        selected_sentence = min(texts, key=lambda text: abs(len(text.split()) - 15))  # Closest to the midpoint (15)\n",
    "\n",
    "    # Save the selected sentence and its word length\n",
    "    selected_sentences[word] = {\n",
    "        \"selected_sentence\": selected_sentence,\n",
    "        \"sentence_length\": len(selected_sentence.split())\n",
    "    }\n",
    "\n",
    "# Convert the selected sentences into a DataFrame\n",
    "selected_sentences_df = pd.DataFrame(\n",
    "    [{\"word\": word, \"selected_sentence\": data[\"selected_sentence\"], \"sentence_length\": data[\"sentence_length\"]}\n",
    "     for word, data in selected_sentences.items()]\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "selected_sentences_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentences_df[\"sentence_length\"].value_counts().sort_index().plot(kind='bar', figsize=(12, 6), title='Distribution of Sentence Lengths')\n",
    "selected_sentences_df[(selected_sentences_df[\"sentence_length\"] > 5) & (selected_sentences_df[\"sentence_length\"] < 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from word_to_texts\n",
    "data = []\n",
    "for word, texts in word_to_texts.items():\n",
    "    num_texts = len(texts)  # Length of the value list\n",
    "    avg_word_len = sum(len(text.split()) for text in texts) / num_texts if num_texts > 0 else 0  # Average word length\n",
    "    min_word_len = min(len(text.split()) for text in texts) if texts else 0  # Minimum word length\n",
    "    data.append({\"word\": word, \"num_texts\": num_texts, \"avg_word_len\": avg_word_len, \"min_word_len\": min_word_len, \"texts\": texts})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "word_to_texts_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "word_to_texts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dictionary to store the selected sentence and its word length for each word\n",
    "selected_sentences = {}\n",
    "\n",
    "for word, texts in tqdm(word_to_texts.items()):\n",
    "    if not texts:\n",
    "        # If the texts list is empty, assign None or a placeholder\n",
    "        selected_sentences[word] = {\"selected_sentence\": None, \"sentence_length\": 0}\n",
    "        continue\n",
    "\n",
    "    # Filter sentences with word length between 10 and 20\n",
    "    valid_sentences = [text for text in texts if 10 <= len(text.split()) <= 20]\n",
    "    \n",
    "    if valid_sentences:\n",
    "        # If there are valid sentences, choose the first one\n",
    "        selected_sentence = valid_sentences[0]\n",
    "    else:\n",
    "        # If no valid sentences, choose the one closest to the range\n",
    "        selected_sentence = min(texts, key=lambda text: abs(len(text.split()) - 15))  # Closest to the midpoint (15)\n",
    "\n",
    "    # Save the selected sentence and its word length\n",
    "    selected_sentences[word] = {\n",
    "        \"selected_sentence\": selected_sentence,\n",
    "        \"sentence_length\": len(selected_sentence.split())\n",
    "    }\n",
    "\n",
    "# Convert the selected sentences into a DataFrame\n",
    "selected_sentences_df = pd.DataFrame(\n",
    "    [{\"word\": word, \"selected_sentence\": data[\"selected_sentence\"], \"sentence_length\": data[\"sentence_length\"]}\n",
    "     for word, data in selected_sentences.items()]\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "selected_sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentences_df[\"sentence_length\"].value_counts().sort_index().plot(kind='bar', figsize=(12, 6), title='Distribution of Sentence Lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentences_df[(selected_sentences_df[\"sentence_length\"] > 5) & (selected_sentences_df[\"sentence_length\"] < 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/hyujang/multilingual-inner-lexicon/RQ1/WordIdentity\")\n",
    "from logitlens import LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"Tower-Babel/Babel-9B-Chat\"\n",
    "MODEL_NAME = \"google/gemma-3-27b-it\"\n",
    "LANGUAGE = \"English\"\n",
    "logit_lens = LogitLens(LANGUAGE, MODEL_NAME, output_type=\"ffn_hidden_states\", retreival_type=\"cumulative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "language = logit_lens.language\n",
    "logit_lens.model.eval()  # Set the model to evaluation mode\n",
    "for word in word_list[:10]:\n",
    "    print(word)\n",
    "    # prompt = f\"Write a single, well-formed sentence between 10 and 20 words long that uses the word '{word}' in a clear and meaningful context.\"\n",
    "    # prompt = f\"Write exactly one complete sentence that is 10 to 20 words long and includes the word '{word}'. Only output the sentence—no explanations or extra text. Do not use the word '{word}' as a verb or adjective; it should be used as a noun.\"\n",
    "    # prompt = f\"Generate a sentence that includes the word '{word}' in a meaningful context with length 10-20 words. Only output the sentence without any additional text. The word '{word}' should be used as a noun.\"\n",
    "    prompt = f\"\"\"Generate excatly one single, self-contained {language} sentence (10–20 words) that includes the word \"{word}\" (without changing form) in a meaningful context. \n",
    "Respond ONLY with a JSON object in the following format:\n",
    "\n",
    "{{\"sentence\": \"<your sentence here>\"}}\n",
    "\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = logit_lens.tokenizer(prompt, return_tensors=\"pt\").to(logit_lens.model.device)\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output_ids = logit_lens.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,      # deterministic\n",
    "            top_p=0.9,\n",
    "            temperature=0.3,\n",
    "            eos_token_id=logit_lens.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode output\n",
    "    output_text = logit_lens.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    if output_text.startswith(prompt):\n",
    "        generated_sentence = output_text[len(prompt):].strip()\n",
    "    else:\n",
    "        generated_sentence = output_text.strip()\n",
    "\n",
    "    print(\"Generated:\", generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
