{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    \"Tower-Babel/Babel-9B-Chat\",\n",
    "    \"google/gemma-3-12b-it\",\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "]\n",
    "\n",
    "language_list = [\n",
    "    \"English\",\n",
    "    \"Korean\",\n",
    "    \"German\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "version_colors = {\n",
    "    \"\": \"#1f77b4\",  # Blue\n",
    "    \"v2\": \"#ff7f0e\",  # Orange\n",
    "    \"v3\": \"#165a16\",  # Green\n",
    "    \"v4\": \"#d62728\"   # Red\n",
    "}\n",
    "\n",
    "version_linestyles = {\n",
    "    \"\": \":\",\n",
    "    \"v2\": \"--\",\n",
    "    \"v3\": \"-.\",\n",
    "    \"v4\": \":\"\n",
    "}\n",
    "\n",
    "\n",
    "model_colors = {\n",
    "    \"Babel-9B-Chat\": \"#66c2a5\",\n",
    "    \"gemma-3-12b-it\": \"#fc8d62\",\n",
    "    \"Llama-2-7b-chat-hf\": \"#e78ac3\"\n",
    "}\n",
    "\n",
    "# Define colors for each language\n",
    "language_colors = {\n",
    "    \"English\": \"#1f77b4\",\n",
    "    \"Korean\": \"#ff7f0e\",\n",
    "    \"German\": \"#2ca02c\"\n",
    "}\n",
    "\n",
    "# Define line styles for each model\n",
    "model_styles = {\n",
    "    \"Babel-9B-Chat\": \"-\",\n",
    "    \"gemma-3-12b-it\": \"--\",\n",
    "    \"Llama-2-7b-chat-hf\": \":\"\n",
    "}\n",
    "\n",
    "languague_styles = {\n",
    "    \"English\": \"-\",\n",
    "    \"Korean\": \"--\",\n",
    "    \"German\": \":\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Set your directory path here\n",
    "directory_path = \"/home/hyujang/multilingual-inner-lexicon/output/RQ1/ComponentAnalysis/attention_weights\"\n",
    "\n",
    "# Find all CSV files ending with \"_2token.csv\"\n",
    "csv_files = glob.glob(os.path.join(directory_path, \"*_2token.csv\"))\n",
    "\n",
    "# Load and concatenate all DataFrames\n",
    "# df_list = [pd.read_csv(file) for file in csv_files]\n",
    "min_length = 10000  # Set your minimum length here\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    print(f\"File: {file}, Length: {df.shape[0]}, max_context_length: {df['context'].apply(len).max()}\")\n",
    "    if df.shape[0] < min_length:   \n",
    "        min_length = df.shape[0]\n",
    "\n",
    "print(f\"Minimum length across all files: {min_length}\")\n",
    "\n",
    "# filter the \n",
    "df_dict = {}\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.sample(n=min_length, random_state=2025)  # Sample min_length rows\n",
    "    df_dict[file.split('/')[-1]] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = \"/home/hyujang/multilingual-inner-lexicon/output/RQ1/ComponentAnalysis/attention_weights\"\n",
    "\n",
    "# Iterate over each model and language pair\n",
    "for model in model_list:\n",
    "    for language in language_list:\n",
    "        # Find the corresponding files for _2token and _single-token\n",
    "        file_2token = glob.glob(os.path.join(directory_path, f\"{model.split('/')[-1]}_{language}_2token.csv\"))\n",
    "        if not file_2token:\n",
    "            print(f\"File not fount: {f\"{model.split('/')[-1]}_{language}__2token.csv\"}\")\n",
    "            \n",
    "        file_single_token = glob.glob(os.path.join(directory_path, f\"{model.split('/')[-1]}_{language}_single-token.csv\"))\n",
    "        if not file_single_token:\n",
    "            print(f\"File not fount: {f\"{model.split('/')[-1]}_{language}_*_single-token.csv\"}\")\n",
    "            continue\n",
    "\n",
    "        # Load the DataFrames\n",
    "        df_2token = pd.read_csv(file_2token[0])\n",
    "        df_single_token = pd.read_csv(file_single_token[0])\n",
    "\n",
    "        # Extract layer-wise attention columns\n",
    "        layer_columns = [col for col in df_2token.columns if col.startswith(\"layer_\")]\n",
    "\n",
    "        # Compute the average attention scores for each layer\n",
    "        avg_attention_2token = df_2token[layer_columns].mean()\n",
    "        avg_attention_single_token = df_single_token[layer_columns].mean()\n",
    "\n",
    "        # Plot the attention scores\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(\n",
    "            range(1, len(avg_attention_2token) + 1),\n",
    "            avg_attention_2token,\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            color='b',\n",
    "            label=f'two-token words (n={len(df_2token)}))'\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(1, len(avg_attention_single_token) + 1),\n",
    "            avg_attention_single_token,\n",
    "            marker='o',\n",
    "            linestyle='--',\n",
    "            color='r',\n",
    "            label=f'single-token words (n={len(df_single_token)}))'\n",
    "        )\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f\"Attention Scores Across Layers ({model.split('/')[-1]}, {language})\", fontsize=16)\n",
    "        plt.xlabel(\"Layer\", fontsize=14)\n",
    "        plt.ylabel(\"Average Attention Score\", fontsize=14)\n",
    "        plt.xticks(range(1, len(avg_attention_2token) + 1))  # Ensure x-axis ticks match layer numbers\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your directory path here\n",
    "directory_path = \"/home/hyujang/multilingual-inner-lexicon/output/RQ1/ComponentAnalysis/attention_weights\"\n",
    "\n",
    "# Find all CSV files ending with \"_2token.csv\"\n",
    "csv_files = glob.glob(os.path.join(directory_path, \"*_single-token.csv\"))\n",
    "\n",
    "# Load and concatenate all DataFrames\n",
    "# df_list = [pd.read_csv(file) for file in csv_files]\n",
    "min_length = 10000  # Set your minimum length here\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    print(f\"File: {file}, Length: {df.shape[0]}, max_context_length: {df['context'].apply(len).max()}\")\n",
    "    if df.shape[0] < min_length:   \n",
    "        min_length = df.shape[0]\n",
    "\n",
    "print(f\"Minimum length across all files: {min_length}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for path in csv_files:\n",
    "    # Extract model and language from the file name\n",
    "    df = pd.read_csv(path)\n",
    "    df.sample(n=1467, random_state=2025, replace=True)  # Sample min_length rows\n",
    "    filename = os.path.basename(path)\n",
    "    model_short = filename.split(\"_\")[0]\n",
    "    lang = filename.split(\"_\")[1]\n",
    "    \n",
    "    # Extract layer-wise attention columns\n",
    "    layer_columns = [col for col in df.columns if col.startswith(\"layer_\")]\n",
    "\n",
    "    # Compute the average attention score for each layer\n",
    "    avg_attention_scores = df[layer_columns].mean()\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(\n",
    "        range(1, len(avg_attention_scores) + 1),\n",
    "        avg_attention_scores,\n",
    "        marker='o',\n",
    "        label = filename.split(\".\")[0],\n",
    "        color=language_colors.get(lang, \"black\"),  # Use the color for the language\n",
    "        linestyle=model_styles.get(model_short, \"dashdot\"),  # Use the line style for the model\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Average Attention Score\")\n",
    "plt.title(\"Attention Scores Across Layers (n=1467)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Melt the DataFrame to make it suitable for seaborn\n",
    "layer_columns = [col for col in df.columns if col.startswith(\"layer_\")]\n",
    "melted_df = df.melt(\n",
    "    id_vars=[\"word\", \"context\"],  # Keep these columns as identifiers\n",
    "    value_vars=layer_columns,    # Columns to unpivot\n",
    "    var_name=\"layer\",            # Name for the new 'layer' column\n",
    "    value_name=\"attention_score\" # Name for the new 'attention_score' column\n",
    ")\n",
    "\n",
    "# Convert layer names to integers for proper sorting\n",
    "melted_df[\"layer\"] = melted_df[\"layer\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Plot the lineplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=melted_df, x=\"layer\", y=\"attention_score\", ci=\"sd\", marker=\"o\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Attention Scores Across Layers\", fontsize=16)\n",
    "plt.xlabel(\"Layer\", fontsize=14)\n",
    "plt.ylabel(\"Attention Score\", fontsize=14)\n",
    "plt.xticks(range(1, melted_df[\"layer\"].max() + 1))  # Ensure x-axis ticks match layer numbers\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract layer-wise attention columns\n",
    "layer_columns = [col for col in df.columns if col.startswith(\"layer_\")]\n",
    "\n",
    "# Compute the average attention score for each layer\n",
    "avg_attention_scores = df[layer_columns].mean()\n",
    "\n",
    "# Plot the attention scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    range(1, len(avg_attention_scores) + 1),  # X-axis: Layer numbers\n",
    "    avg_attention_scores,                    # Y-axis: Average attention scores\n",
    "    marker='o', linestyle='-', color='b', label='Average Attention Score'\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Attention Scores Across Layers\", fontsize=16)\n",
    "plt.xlabel(\"Layer\", fontsize=14)\n",
    "plt.ylabel(\"Attention Score\", fontsize=14)\n",
    "plt.xticks(range(1, len(avg_attention_scores) + 1))  # Ensure x-axis ticks match layer numbers\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the attention weights DataFrame\n",
    "attn_df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/output/RQ1/ComponentAnalysis/attention_weights/{MODEL_NAME.split('/')[-1]}_{LANGUAGE}_all.csv\")\n",
    "\n",
    "# Extract layer-wise attention columns\n",
    "layer_columns = [col for col in attn_df.columns if col.startswith(\"layer_\")]\n",
    "\n",
    "# Compute the average attention weight for each layer\n",
    "avg_attn_weights = attn_df[layer_columns].mean()\n",
    "\n",
    "# Plot the average attention weights\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(avg_attn_weights) + 1), avg_attn_weights, marker='o', linestyle='-', color='b')\n",
    "plt.title(f\"Average Attention Weights Across Layers ({MODEL_NAME}, {LANGUAGE})\", fontsize=14)\n",
    "plt.xlabel(\"Layer\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention Weight\", fontsize=12)\n",
    "plt.xticks(range(1, len(avg_attn_weights) + 1))  # Ensure x-axis ticks match layer numbers\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
