{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the RQ1 directory to the path\n",
    "# sys.path.append(os.path.abspath(\"../\"))\n",
    "from patchscope import PatchScope\n",
    "\n",
    "# model_name = \"google/gemma-3-12b-it\"\n",
    "# model_name = \"google/gemma-3-12b-pt\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name = \"Tower-Babel/Babel-9B-Chat\"\n",
    "model_name = \"google/gemma-2-9b-it\"  # Use Gemma 2 for the experiment\n",
    "word_nonword_cls = PatchScope(\"English\", model_name) # language is not used in the model name, but it is required by the class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Anisotropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def cos_contrib(emb1, emb2):\n",
    "    \"\"\"Cosine contribution per dimension\"\"\"\n",
    "    numerator_terms = emb1 * emb2\n",
    "    denom = torch.norm(emb1) * torch.norm(emb2)\n",
    "    return (numerator_terms / denom).numpy()\n",
    "\n",
    "\n",
    "def compute_crosslingual_anisotropy(hidden_en, hidden_ko, sample_size=5000, pair=\"aligned\"):\n",
    "    results = {}\n",
    "\n",
    "    for layer in hidden_en:\n",
    "        en_vecs = hidden_en[layer]\n",
    "        ko_vecs = hidden_ko[layer]\n",
    "\n",
    "        n = en_vecs.shape[0]\n",
    "        dim = en_vecs.shape[1]\n",
    "        if pair==\"aligned\":\n",
    "            contribs = []\n",
    "            for i in range(n):\n",
    "                emb1 = en_vecs[i]\n",
    "                emb2 = ko_vecs[i]\n",
    "                contribs.append(cos_contrib(emb1, emb2))  # shape: (D,)\n",
    "                    \n",
    "        elif pair==\"random\":\n",
    "            # Sample random pairs (not necessarily aligned)\n",
    "            idx_pairs = [(np.random.randint(0, n), np.random.randint(0, n)) for _ in range(sample_size)]\n",
    "\n",
    "            contribs = []\n",
    "            for i, j in idx_pairs:\n",
    "                emb1 = en_vecs[i]\n",
    "                emb2 = ko_vecs[j]\n",
    "                contribs.append(cos_contrib(emb1, emb2))  # (D,)\n",
    "\n",
    "        contribs = np.stack(contribs)  # (sample_size, D)\n",
    "        mean_contrib = contribs.mean(axis=0)  # (D,)\n",
    "        anisotropy = mean_contrib.sum()\n",
    "\n",
    "        results[layer] = {\n",
    "            \"anisotropy\": anisotropy,\n",
    "            \"mean_contrib\": mean_contrib,\n",
    "            \"top_dims\": np.flip(np.argsort(mean_contrib)[-10:])\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_anisotropy(results, model_name, lang1, lang2):\n",
    "    layers = list(results.keys())\n",
    "    anisotropy_values = [results[l][\"anisotropy\"] for l in layers]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(layers, anisotropy_values, marker='o')\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(\"Anisotropy\")\n",
    "    plt.title(f\"Anisotropy across layers ({model_name} - {lang1}&{lang2})\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"anisotropy_{model_name}_{lang1}&{lang2}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"en\"\n",
    "lang2 = \"ko\"\n",
    "\n",
    "model_name = \"Tower-Babel/Babel-9B-Chat\"\n",
    "hidden_1 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang1}_1.pt\")\n",
    "hidden_2 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang2}_1.pt\")\n",
    "results_babel = compute_crosslingual_anisotropy(hidden_1, hidden_2)\n",
    "\n",
    "\n",
    "model_name = \"google/gemma-3-12b-it\"\n",
    "hidden_1 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang1}_1.pt\")\n",
    "hidden_2 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang2}_1.pt\")\n",
    "results_gemma = compute_crosslingual_anisotropy(hidden_1, hidden_2)\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "hidden_1 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang1}_1.pt\")\n",
    "hidden_2 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang2}_1.pt\")\n",
    "results_llama = compute_crosslingual_anisotropy(hidden_1, hidden_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: results_babel, results_gemma, results_llama\n",
    "layers_babel = set(results_babel.keys())\n",
    "layers_gemma = set(results_gemma.keys())\n",
    "layers_llama = set(results_llama.keys())\n",
    "\n",
    "# Union of all layers, sorted\n",
    "all_layers = sorted(layers_babel | layers_gemma | layers_llama)\n",
    "\n",
    "def get_acc(results, all_layers):\n",
    "    # Return a list of values for all_layers, np.nan if missing\n",
    "    return [results[l]['anisotropy'] if l in results else np.nan for l in all_layers]\n",
    "\n",
    "acc_babel = get_acc(results_babel, all_layers)\n",
    "acc_gemma = get_acc(results_gemma, all_layers)\n",
    "acc_llama = get_acc(results_llama, all_layers)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(all_layers, acc_babel, label='Babel-9B-Chat', marker='o')\n",
    "plt.plot(all_layers, acc_gemma, label='gemma-3-12b-it', marker='o')\n",
    "plt.plot(all_layers, acc_llama, label='Llama-2-7b-chat-hf', marker='o')\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Anisotropy\")\n",
    "plt.title(\"Cross-Lingual Alignment over Layers (en-ko)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_dims(results, model_name, lang1, lang2):\n",
    "    layers = list(results.keys())\n",
    "    dims = len(results[layers[0]][\"mean_contrib\"])\n",
    "\n",
    "    # Create a matrix: layers x dims\n",
    "    contrib_matrix = np.stack([results[l][\"mean_contrib\"] for l in layers])\n",
    "\n",
    "    top_k = 10\n",
    "    top_dims = np.argsort(np.mean(contrib_matrix, axis=0))[-top_k:][::-1]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for d in top_dims:\n",
    "        plt.plot(layers, contrib_matrix[:, d], label=f\"Dim {d}\")\n",
    "\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(\"Cosine Contribution\")\n",
    "    plt.title(f\"Top-{top_k} Dim Contributions Across Layers ({model_name} - {lang1}â†”{lang2})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"topdims_{model_name}_{lang1}_{lang2}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_dims(results_gemma, \"gemma3\", \"en\", \"ko\")\n",
    "plot_top_dims(results_babel, \"babel\", \"en\", \"ko\")\n",
    "plot_top_dims(results_llama, \"llama2\", \"en\", \"ko\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "hidden_1 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang1}_1.pt\")\n",
    "hidden_2 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang2}_1.pt\")\n",
    "results_llama = compute_crosslingual_anisotropy(hidden_1, hidden_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compute_crosslingual_anisotropy(hidden_2, hidden_2,pair=\"random\")\n",
    "plot_anisotropy(results, model_name=model_name.split(\"/\")[-1], lang1=\"ko\", lang2=\"ko\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Cross-lingual Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def prepare_dataset(lang1, lang2):\n",
    "    dataset_en_ko = load_dataset(\"tatoeba\", lang1=lang1, lang2=lang2)\n",
    "    dataset_en_ko = dataset_en_ko['train'].to_pandas()\n",
    "    dataset_en_ko[lang1] = dataset_en_ko['translation'].str[lang1]\n",
    "    dataset_en_ko[lang2] = dataset_en_ko['translation'].str[lang2]\n",
    "    dataset_en_ko = dataset_en_ko[[lang1, lang2]]\n",
    "    dataset_en_ko = dataset_en_ko[dataset_en_ko[lang1].str.len() >= 3]\n",
    "    dataset_en_ko = dataset_en_ko[dataset_en_ko[lang2].str.len() >= 3]\n",
    "    dataset_en_ko = dataset_en_ko.sample(n=1000, random_state=2025).reset_index(drop=True)\n",
    "    dataset_en_ko[f'{lang1}_tokens'] = dataset_en_ko[lang1].apply(word_nonword_cls.tokenizer.tokenize)\n",
    "    dataset_en_ko[f'{lang2}_tokens'] = dataset_en_ko[lang2].apply(word_nonword_cls.tokenizer.tokenize)\n",
    "    return dataset_en_ko\n",
    "\n",
    "def compute_crosslingual_cosine(hidden_en, hidden_ko, top_k=5):\n",
    "    results = {}\n",
    "\n",
    "    for layer in range(len(hidden_en)):\n",
    "        en_vecs = hidden_en[layer]  # shape: (N, D)\n",
    "        ko_vecs = hidden_ko[layer]  # shape: (N, D)\n",
    "        en_vecs = en_vecs.reshape(1000, -1)\n",
    "        ko_vecs = ko_vecs.reshape(1000, -1)\n",
    "\n",
    "        # Normalize to unit vectors for cosine similarity\n",
    "        en_norm = F.normalize(en_vecs, p=2, dim=1)  # (N, D)\n",
    "        ko_norm = F.normalize(ko_vecs, p=2, dim=1)  # (N, D)\n",
    "\n",
    "        # Compute cosine similarity: (N x D) @ (D x N) = (N x N)\n",
    "        global sim_matrix\n",
    "        sim_matrix = en_norm @ ko_norm.T  # (N, N)\n",
    "\n",
    "        # For each English vector, get top-k most similar Korean vectors\n",
    "        topk_values, topk_indices = torch.topk(sim_matrix, k=top_k, dim=1)  # (N, top_k)\n",
    "        \n",
    "        # Check if correct alignment exists in top-k (optional accuracy check)\n",
    "        correct = torch.arange(sim_matrix.size(0)).to(topk_indices.device)\n",
    "        hits = (topk_indices == correct.unsqueeze(1)).any(dim=1).float()  # 1 if correct in top-k\n",
    "\n",
    "        results[layer] = {\n",
    "            \"similarity_matrix\": sim_matrix,\n",
    "            \"topk_indices\": topk_indices,\n",
    "            \"topk_values\": topk_values,\n",
    "            \"topk_accuracy\": hits.mean().item(),  # overall top-k accuracy\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"en\"\n",
    "lang2 = \"ko\"\n",
    "\n",
    "dataset_en_ko = prepare_dataset(lang1, lang2)\n",
    "\n",
    "hidden_1 = word_nonword_cls.extract_token_i_hidden_states_original(inputs = dataset_en_ko[f'{lang1}'].tolist())\n",
    "hidden_2 = word_nonword_cls.extract_token_i_hidden_states_original(inputs = dataset_en_ko[f'{lang2}'].tolist())\n",
    "# torch.save(hidden_1, f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang1}_2.pt\")\n",
    "# torch.save(hidden_2, f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang2}_2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_k = 3\n",
    "results = compute_crosslingual_cosine(hidden_1, hidden_2, top_k=top_k)\n",
    "\n",
    "layers = sorted(results.keys())\n",
    "accuracies = [results[l]['topk_accuracy'] for l in layers]\n",
    "plt.plot(layers, accuracies)\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(f\"Top-{top_k} Accuracy\")\n",
    "plt.title(f\"Cross-Lingual Alignment over Layers ({lang1}-{lang2})\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"de\"\n",
    "lang2 = \"en\"\n",
    "\n",
    "dataset_de_en = prepare_dataset(lang1, lang2)\n",
    "\n",
    "hidden_1 = word_nonword_cls.extract_token_i_hidden_states_original(inputs = dataset_de_en[f'{lang1}'].tolist())\n",
    "hidden_2 = word_nonword_cls.extract_token_i_hidden_states_original(inputs = dataset_de_en[f'{lang2}'].tolist())\n",
    "# torch.save(hidden_1, f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang1}_2.pt\")\n",
    "# torch.save(hidden_2, f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang2}_2.pt\")\n",
    "\n",
    "top_k = 3\n",
    "results = compute_crosslingual_cosine(hidden_1, hidden_2, top_k=top_k)\n",
    "\n",
    "layers = sorted(results.keys())\n",
    "accuracies = [results[l]['topk_accuracy'] for l in layers]\n",
    "plt.plot(layers, accuracies)\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(f\"Top-{top_k} Accuracy\")\n",
    "plt.title(f\"Cross-Lingual Alignment over Layers ({lang1}-{lang2})\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"de\"\n",
    "lang2 = \"ko\"\n",
    "\n",
    "dataset_de_ko = prepare_dataset(lang1, lang2)\n",
    "\n",
    "# hidden_1 = word_nonword_cls.extract_token_hidden_states(dataset_de_ko[f'{lang1}_tokens'].tolist())\n",
    "# hidden_2 = word_nonword_cls.extract_token_hidden_states(dataset_de_ko[f'{lang2}_tokens'].tolist())\n",
    "hidden_1 = word_nonword_cls.extract_token_i_hidden_states_original(inputs = dataset_de_ko[f'{lang1}'].tolist())\n",
    "hidden_2 = word_nonword_cls.extract_token_i_hidden_states_original(inputs = dataset_de_ko[f'{lang2}'].tolist())\n",
    "\n",
    "top_k = 3\n",
    "results = compute_crosslingual_cosine(hidden_1, hidden_2, top_k=top_k)\n",
    "\n",
    "layers = sorted(results.keys())\n",
    "accuracies = [results[l]['topk_accuracy'] for l in layers]\n",
    "plt.plot(layers, accuracies)\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(f\"Top-{top_k} Accuracy\")\n",
    "plt.title(f\"Cross-Lingual Alignment over Layers ({lang1}-{lang2})\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"en\"\n",
    "lang2 = \"ko\"\n",
    "\n",
    "# model_name = \"google/gemma-3-12b-it\"\n",
    "# model_name = \"Tower-Babel/Babel-9B-Chat\"\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "hidden_1 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang1}_1.pt\")\n",
    "hidden_2 = torch.load(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang2}_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = []\n",
    "for layer in hidden_1:\n",
    "    layer_hiddens = hidden_1[layer]  # shape: (batch, hidden_dim)\n",
    "    layer_hiddens = F.normalize(layer_hiddens, p=2, dim=1)\n",
    "    var = layer_hiddens.var(dim=-1).mean().item()\n",
    "    variances.append(var)\n",
    "    \n",
    "layers = sorted(hidden_1.keys())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(layers, variances, marker='o')\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Hidden State Variance\")\n",
    "plt.title(\"Hidden State Variance per Layer\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "pd.Series(variances).to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_variance_{model_name.split('/')[-1]}_{lang1}_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = []\n",
    "for layer in hidden_2:\n",
    "    layer_hiddens = hidden_2[layer]  # shape: (batch, hidden_dim)\n",
    "    layer_hiddens = F.normalize(layer_hiddens, p=2, dim=1)\n",
    "\n",
    "    var = layer_hiddens.var(dim=-1).mean().item()\n",
    "    variances.append(var)\n",
    "    \n",
    "layers = sorted(hidden_2.keys())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(layers, variances, marker='o')\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Hidden State Variance\")\n",
    "plt.title(\"Hidden State Variance per Layer\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.ticklabel_format(style='plain', axis='y')  # <-- Add this line\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "pd.Series(variances).to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_variance_{model_name.split('/')[-1]}_{lang2}_1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
