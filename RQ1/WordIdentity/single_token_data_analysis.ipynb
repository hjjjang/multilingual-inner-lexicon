{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# from logitlens import LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Tower-Babel/Babel-9B-Chat\"\n",
    "LANGUAGE = \"English\"\n",
    "logit_lens = LogitLens(LANGUAGE, MODEL_NAME)\n",
    "MODEL_NAME = MODEL_NAME.split(\"/\")[-1]\n",
    "path1 = f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{MODEL_NAME}_{LANGUAGE}.csv\"\n",
    "df = pd.read_csv(path1)\n",
    "df[\"splitted_tokens\"] = df[\"splitted_tokens\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(self):\n",
    "    if self.tokenizer_name == \"Tower-Babel/Babel-9B-Chat\":\n",
    "        self.tokenizer.add_special_tokens({'unk_token': 'UNK'})\n",
    "        self.tokenizer.unk_token_id = self.tokenizer.convert_tokens_to_ids('UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Tower-Babel/Babel-9B-Chat\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_list = [\n",
    "    \"Tower-Babel/Babel-9B-Chat\",\n",
    "    \"google/gemma-3-12b-it\",\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "]\n",
    "language_list = [\n",
    "    \"English\",\n",
    "    \"German\",\n",
    "    # \"Korean\",\n",
    "]\n",
    "df_dict = {}\n",
    "# MODEL_NAME = \"Tower-Babel/Babel-9B-Chat\"\n",
    "# LANGUAGE = \"English\"\n",
    "for MODEL_NAME in model_list:\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "        \n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=None)\n",
    "    if tokenizer.unk_token is None:\n",
    "        tokenizer.add_special_tokens({'unk_token': 'UNK'})\n",
    "        tokenizer.unk_token_id = tokenizer.convert_tokens_to_ids('UNK')\n",
    "    else:\n",
    "        print(tokenizer.unk_token)\n",
    "        \n",
    "\n",
    "    for LANGUAGE in language_list:\n",
    "        MODEL_NAME = MODEL_NAME.split(\"/\")[-1]\n",
    "        path1 = f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{MODEL_NAME}_{LANGUAGE}_v2.csv\"\n",
    "        df = pd.read_csv(path1)\n",
    "        df[\"splitted_tokens\"] = df[\"splitted_tokens\"].apply(ast.literal_eval)\n",
    "        splitted_input_tokens = []\n",
    "        for tokens in df[\"splitted_tokens\"]:\n",
    "            ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            final_toknes = tokenizer.convert_ids_to_tokens(ids)\n",
    "            splitted_input_tokens.append(final_toknes)\n",
    "        df[\"splitted_input_tokens\"] = splitted_input_tokens\n",
    "        unk_token = tokenizer.unk_token\n",
    "        df[\"include_UNK\"] = df['splitted_input_tokens'].apply(lambda x: unk_token in x)\n",
    "        df['num_UNK'] = df['splitted_input_tokens'].apply(lambda x: x.count(unk_token))\n",
    "        df['proportion_UNK'] = df['splitted_input_tokens'].apply(lambda x: x.count(unk_token)/len(x))\n",
    "        df_dict[(MODEL_NAME, LANGUAGE)] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, df in df_dict.items():\n",
    "    MODEL_NAME, LANGUAGE = k\n",
    "    # path2 = f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{MODEL_NAME}_{LANGUAGE}_with_UNK.csv\"\n",
    "    # v.to_csv(path2, index=False)\n",
    "    # print(f\"Saved {path2}\")\n",
    "    print(f\"Model: {MODEL_NAME}, Language: {LANGUAGE}\")\n",
    "    print(df['include_UNK'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dict[('gemma-3-12b-it', 'English')]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize(word):\n",
    "    return re.sub(r'\\W+', '', word).lower()  # Remove non-alphanumeric chars and lowercase\n",
    "\n",
    "def get_retrieval_rates(results_df):\n",
    "    token_str_cols = [col for col in results_df.columns if col.endswith(\"_top_token_str\")]\n",
    "    retrieval_rates = [] \n",
    "    layers = []\n",
    "\n",
    "    for col in token_str_cols:\n",
    "        pred_word = results_df[col].apply(lambda x: [normalize(word) for word in x])\n",
    "        original_word = results_df[\"word\"].apply(normalize)\n",
    "        retrieval_match = pred_word.combine(original_word, lambda preds, orig: orig in preds)\n",
    "        retrieval_rate = retrieval_match.mean()\n",
    "        retrieval_rates.append(retrieval_rate)\n",
    "        # Extract layer number from column name, e.g., \"layer_1_top1_token_str\" -> 1\n",
    "        layer_num = int(col.split(\"_\")[1])\n",
    "        layers.append(layer_num)\n",
    "\n",
    "    # Sort by layer number\n",
    "    sorted_indices = sorted(range(len(layers)), key=lambda i: layers[i])\n",
    "    layers = [layers[i] for i in sorted_indices]\n",
    "    retrieval_rates = [retrieval_rates[i] for i in sorted_indices]\n",
    "    \n",
    "    return layers, retrieval_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dict[('Babel-9B-Chat', 'English')]\n",
    "df_result = pd.read_csv(\"/home/hyujang/multilingual-inner-lexicon/output/RQ1/WordIdentity/single_token_simple_split_Babel-9B-Chat_English_v2.csv\")\n",
    "from ast import literal_eval\n",
    "\n",
    "for col in df_result.columns:\n",
    "    try:\n",
    "        # Check if at least one value looks like a list\n",
    "        if df_result[col].apply(lambda x: isinstance(x, str) and x.strip().startswith(\"[\")).any():\n",
    "            df_result[col] = df_result[col].apply(literal_eval)\n",
    "    except (ValueError, SyntaxError):\n",
    "        continue  # Skip columns that don't parse correctly\n",
    "df_final = pd.concat([df, df_result.iloc[:,1:]], axis=1)\n",
    "\n",
    "df_1 = df_final[~df_final.include_UNK]\n",
    "layers, retrieval_rates_1 = get_retrieval_rates(results_df=df_1)\n",
    "\n",
    "df_2 = df_final[df_final.include_UNK]\n",
    "layers, retrieval_rates_2 = get_retrieval_rates(results_df=df_2)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(layers, retrieval_rates_1, marker='o', label='simple split - without UNK')\n",
    "plt.plot(layers, retrieval_rates_2, marker='s', label='simple split - with UNK')\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Original Word Retrieval Rate (Top-3)\")\n",
    "plt.title(f\"Top-3 Retrieval Rate per Layer ({\"Babel-9B-Chat\"}, {\"English\"})\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_input_tokens = []\n",
    "for tokens in df[\"splitted_tokens\"]:\n",
    "    ids = logit_lens.tokenizer.convert_tokens_to_ids(tokens)\n",
    "    final_toknes = logit_lens.tokenizer.convert_ids_to_tokens(ids)\n",
    "    splitted_input_tokens.append(final_toknes)\n",
    "df[\"splitted_input_tokens\"] = splitted_input_tokens\n",
    "df[\"include_UNK\"] = df['splitted_input_tokens'].apply(lambda x: \"UNK\" in x)\n",
    "df['num_UNK'] = df['splitted_input_tokens'].apply(lambda x: x.count(\"UNK\"))\n",
    "df['proportion_UNK'] = df['splitted_input_tokens'].apply(lambda x: x.count(\"UNK\")/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"proportion_UNK\"].value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
