{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "BASE_DIR = \"/home/hyujang/multilingual-inner-lexicon\"\n",
    "with open(os.path.join(BASE_DIR, \"RQ1/config.json\"), \"r\") as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Configuration variables\n",
    "model_name_map = {\n",
    "    \"llama_2_7b\": \"Llama-2-7b-chat-hf\",\n",
    "    \"babel_9b\": \"Babel-9B-Chat\",\n",
    "    \"gemma_12b\": \"gemma-3-12b-it\"\n",
    "}\n",
    "\n",
    "MIN_WORD_LEN = 3\n",
    "MIN_JAMO_LEN = 2\n",
    "MIN_WORD_FREQ = CONFIG[\"min_freq\"]\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "NUM_QUANTILES = CONFIG[\"num_quantiles\"]\n",
    "\n",
    "RANDOM_SEED = CONFIG[\"seed\"]\n",
    "random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_by_freq(df):\n",
    "    df['freq_quantile'], bins = pd.qcut(df['freq'], NUM_QUANTILES, labels=False, duplicates='drop', retbins=True)\n",
    "    num_quantiles = df['freq_quantile'].nunique()\n",
    "    samples_per_quantile = NUM_SAMPLES // num_quantiles\n",
    "    \n",
    "    sampled = []\n",
    "    for quantile in range(num_quantiles):\n",
    "        quantile_df = df[df['freq_quantile'] == quantile]\n",
    "        if len(quantile_df) > 0:\n",
    "            sampled.append(quantile_df.sample(min(len(quantile_df), samples_per_quantile), replace=False, random_state=RANDOM_SEED))\n",
    "    sampled_df = pd.concat(sampled, ignore_index=False).drop_duplicates(subset=['word'])\n",
    "    \n",
    "    if len(sampled_df) < NUM_SAMPLES:\n",
    "        remaining = NUM_SAMPLES - len(sampled_df)\n",
    "        other_df = df.drop(sampled_df.index, errors='ignore')\n",
    "        print(f\"remaining: {remaining}, other_df: {len(other_df)}\")\n",
    "        additional_samples = other_df.sample(min(len(other_df), remaining), replace=False, random_state=RANDOM_SEED)\n",
    "        print(f\"additional_samples: {len(additional_samples)}\")\n",
    "        sampled_df = pd.concat([sampled_df, additional_samples]).drop_duplicates(subset=['word'])\n",
    "\n",
    "    print(f\"sampled_df: {len(sampled_df)}\")\n",
    "    \n",
    "    return sampled_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"English\"\n",
    "TOKENIZER = \"babel_9b\"\n",
    "MODEL_NAME = model_name_map[TOKENIZER]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df = df[df['freq'] >= MIN_WORD_FREQ]\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]>1)].reset_index(drop=True)\n",
    "print(df[f'token_num_{TOKENIZER}'].value_counts())\n",
    "sampled_df = sample_by_freq(df)\n",
    "print(sampled_df[f'token_num_{TOKENIZER}'].value_counts(normalize=True))\n",
    "# sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/multi_token_{MODEL_NAME}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"English\"\n",
    "TOKENIZER = \"gemma_12b\"\n",
    "MODEL_NAME = model_name_map[TOKENIZER]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df = df[df['freq'] >= MIN_WORD_FREQ]\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]>1)].reset_index(drop=True)\n",
    "print(df[f'token_num_{TOKENIZER}'].value_counts())\n",
    "sampled_df = sample_by_freq(df)\n",
    "print(sampled_df[f'token_num_{TOKENIZER}'].value_counts(normalize=True))\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/multi_token_{MODEL_NAME}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"English\"\n",
    "TOKENIZER = \"llama_2_7b\"\n",
    "MODEL_NAME = model_name_map[TOKENIZER]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df = df[df['freq'] >= MIN_WORD_FREQ]\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]>1)].reset_index(drop=True)\n",
    "print(df[f'token_num_{TOKENIZER}'].value_counts())\n",
    "sampled_df = sample_by_freq(df)\n",
    "print(sampled_df[f'token_num_{TOKENIZER}'].value_counts(normalize=True))\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/multi_token_{MODEL_NAME}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## KOREAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"Korean\"\n",
    "TOKENIZER = \"babel_9b\"\n",
    "MODEL_NAME = model_name_map[TOKENIZER]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df = df[df['freq'] >= MIN_WORD_FREQ]\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]>1)].reset_index(drop=True)\n",
    "print(df[f'token_num_{TOKENIZER}'].value_counts())\n",
    "sampled_df = sample_by_freq(df)\n",
    "print(sampled_df[f'token_num_{TOKENIZER}'].value_counts(normalize=True))\n",
    "print(sampled_df['word_len'].value_counts(normalize=True))\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/multi_token_{MODEL_NAME}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"Korean\"\n",
    "TOKENIZER = \"gemma_12b\"\n",
    "MODEL_NAME = model_name_map[TOKENIZER]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df = df[df['freq'] >= MIN_WORD_FREQ]\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]>1)].reset_index(drop=True)\n",
    "print(df[f'token_num_{TOKENIZER}'].value_counts())\n",
    "sampled_df = sample_by_freq(df)\n",
    "print(sampled_df[f'token_num_{TOKENIZER}'].value_counts(normalize=True))\n",
    "print(sampled_df['word_len'].value_counts(normalize=True))\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/multi_token_{MODEL_NAME}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"Korean\"\n",
    "TOKENIZER = \"llama_2_7b\"\n",
    "MODEL_NAME = model_name_map[TOKENIZER]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df = df[df['freq'] >= MIN_WORD_FREQ]\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]>2)].reset_index(drop=True) # Adjusted to >2 for llama_2_7b\n",
    "print(df[f'token_num_{TOKENIZER}'].value_counts())\n",
    "sampled_df = sample_by_freq(df)\n",
    "print(sampled_df[f'token_num_{TOKENIZER}'].value_counts(normalize=True))\n",
    "print(sampled_df['word_len'].value_counts(normalize=True))\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/multi_token_{MODEL_NAME}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## GERMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"German\"\n",
    "TOKENIZER = \"babel_9b\"\n",
    "MODEL_NAME = model_name_map[TOKENIZER]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df = df[df['freq'] >= MIN_WORD_FREQ]\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]>1) & (df[f\"token_num_{TOKENIZER}\"]<2)].reset_index(drop=True) # Adjusted to >2 for llama_2_7b\n",
    "print(df[f'token_num_{TOKENIZER}'].value_counts())\n",
    "sampled_df = sample_by_freq(df)\n",
    "print(sampled_df[f'token_num_{TOKENIZER}'].value_counts(normalize=True))\n",
    "print(sampled_df['word_len'].value_counts(normalize=True))\n",
    "# sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/multi_token_{MODEL_NAME}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"German\"\n",
    "TOKENIZER = \"gemma_12b\"\n",
    "MODEL_NAME = model_name_map[TOKENIZER]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df = df[df['freq'] >= MIN_WORD_FREQ]\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]>2)].reset_index(drop=True) # Adjusted to >2 for llama_2_7b\n",
    "print(df[f'token_num_{TOKENIZER}'].value_counts())\n",
    "sampled_df = sample_by_freq(df)\n",
    "print(sampled_df[f'token_num_{TOKENIZER}'].value_counts(normalize=True))\n",
    "print(sampled_df['word_len'].value_counts(normalize=True))\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/multi_token_{MODEL_NAME}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"German\"\n",
    "TOKENIZER = \"llama_2_7b\"\n",
    "MODEL_NAME = model_name_map[TOKENIZER]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df = df[df['freq'] >= MIN_WORD_FREQ]\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]>2)].reset_index(drop=True) # Adjusted to >2 for llama_2_7b\n",
    "print(df[f'token_num_{TOKENIZER}'].value_counts())\n",
    "sampled_df = sample_by_freq(df)\n",
    "print(sampled_df[f'token_num_{TOKENIZER}'].value_counts(normalize=True))\n",
    "print(sampled_df['word_len'].value_counts(normalize=True))\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/multi_token_{MODEL_NAME}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the RQ1 directory to the path\n",
    "# sys.path.append(os.path.abspath(\"../\"))\n",
    "# from ..WordNonword.classification import WordNonwordClassifier\n",
    "from logitlens import LogitLens\n",
    "\n",
    "# model_name = \"google/gemma-3-12b-it\"\n",
    "# model_name = \"google/gemma-3-12b-pt\"\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name = \"Tower-Babel/Babel-9B-Chat\"\n",
    "word_nonword_cls = LogitLens(\"English\", model_name) # language is not used in the model name, but it is required by the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "word_nonword_cls.tokenizer.convert_tokens_to_string([ast.literal_eval(df['tokens_babel_9b'][0])[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_nonword_cls.tokenizer.convert_tokens_to_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
