{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b1aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from jamo import h2j, j2h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94290f45",
   "metadata": {},
   "source": [
    "# ENGLISH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607fdd84",
   "metadata": {},
   "source": [
    "### Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1300eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(word, MIN_WORD_LEN):\n",
    "    if len(word) <= 1:\n",
    "        return [word]\n",
    "    # num_splits = random.randint(1, min(4, len(word) - MIN_WORD_LEN))\n",
    "    try:\n",
    "        num_splits = random.randint(1, min(4, len(word) - MIN_WORD_LEN - 1))\n",
    "    except:\n",
    "        num_splits = 1\n",
    "    split_points = sorted(random.sample(range(1, len(word)), num_splits))\n",
    "    tokens = [word[i:j] for i, j in zip([0] + split_points, split_points + [None])]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fd7b87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted_tokens\n",
      "2    2567\n",
      "3     879\n",
      "4     567\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZER = \"babel_9b\"\n",
    "# TOKENIZER = \"gemma_12b\"\n",
    "TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"English\"\n",
    "MIN_WORD_LEN = 3\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "\n",
    "# for English\n",
    "if LANGUAGE == \"English\":\n",
    "    df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "    df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"word_len\"]>MIN_WORD_LEN)].reset_index(drop=True)\n",
    "    df[f\"splitted_tokens\"] = df[\"word\"].apply(lambda x: random_split(x, MIN_WORD_LEN))\n",
    "    print(df[f\"splitted_tokens\"].apply(len).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48d27c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted_tokens\n",
      "2    645\n",
      "3    223\n",
      "4    132\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "num_quantiles = 5\n",
    "df['freq_quantile'], bins = pd.qcut(df['freq'], num_quantiles, labels=False,  duplicates='drop', retbins=True)\n",
    "\n",
    "num_quantiles = df['freq_quantile'].nunique()\n",
    "samples_per_quantile = num_samples // num_quantiles\n",
    "\n",
    "sampled = []\n",
    "for quantile in range(num_quantiles):\n",
    "    quantile_df = df[df['freq_quantile'] == quantile]\n",
    "    if len(quantile_df) > 0:\n",
    "        sampled.append(quantile_df.sample(min(len(quantile_df), samples_per_quantile), \n",
    "                                          replace=False, random_state=RANDOM_SEED))\n",
    "\n",
    "sampled_df = pd.concat(sampled, ignore_index=False).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "\n",
    "model_name = \"Llama-2-7b-chat-hf\"\n",
    "# Display the sampled DataFrame\n",
    "print(sampled_df[f\"splitted_tokens\"].apply(len).value_counts())\n",
    "sampled_df = sampled_df[['word' , f\"splitted_tokens\", \"same_token_num\", \"same_token_num2\", \"freq\", \"freq_quantile\", \"word_len\"]]\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{model_name}_{LANGUAGE}_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040a434",
   "metadata": {},
   "source": [
    "### Typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "368bb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_typo(word, typo_type=None):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    if typo_type is None:\n",
    "        # typo_type = random.choice([\"substitution\", \"deletion\", \"insertion\", \"transposition\"])\n",
    "        typo_type = random.choice([\"substitution\", \"deletion\", \"insertion\"])\n",
    "\n",
    "    if typo_type == \"substitution\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        original_char = word[position]\n",
    "        typo_char = random.choice([c for c in letters if c != original_char])\n",
    "        return word[:position] + typo_char + word[position + 1:], typo_type\n",
    "    elif typo_type == \"deletion\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        return word[:position] + word[position + 1:], typo_type\n",
    "    elif typo_type == \"insertion\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        typo_char = random.choice(letters)\n",
    "        return word[:position] + typo_char + word[position:], typo_type\n",
    "    elif typo_type == \"transposition\":\n",
    "        position = random.randint(1, len(word) - 2)\n",
    "        return word[:position] + word[position + 1] + word[position] + word[position + 2:], typo_type\n",
    "    else:\n",
    "        return word, typo_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ce86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typo_type_gemma_12b\n",
      "insertion       2105\n",
      "substitution    1618\n",
      "deletion        1190\n",
      "Name: count, dtype: int64\n",
      "splitted_typo_tokens\n",
      "2    3128\n",
      "3     967\n",
      "4     516\n",
      "5     302\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tokens_babel_9b</th>\n",
       "      <th>token_num_babel_9b</th>\n",
       "      <th>tokens_gemma_12b</th>\n",
       "      <th>token_num_gemma_12b</th>\n",
       "      <th>tokens_llama_2_7b</th>\n",
       "      <th>token_num_llama_2_7b</th>\n",
       "      <th>avg_token_num</th>\n",
       "      <th>same_token_num</th>\n",
       "      <th>avg_token_num_rounded</th>\n",
       "      <th>avg_token_num2</th>\n",
       "      <th>same_token_num2</th>\n",
       "      <th>avg_token_num2_rounded</th>\n",
       "      <th>any_token_num_is_1</th>\n",
       "      <th>freq</th>\n",
       "      <th>word_len</th>\n",
       "      <th>typo_tokens_gemma_12b</th>\n",
       "      <th>typo_type_gemma_12b</th>\n",
       "      <th>typo_word_len</th>\n",
       "      <th>splitted_typo_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>['year']</td>\n",
       "      <td>1</td>\n",
       "      <td>['year']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁year']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>15118</td>\n",
       "      <td>4</td>\n",
       "      <td>yuear</td>\n",
       "      <td>insertion</td>\n",
       "      <td>5</td>\n",
       "      <td>[yu, ear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "      <td>['people']</td>\n",
       "      <td>1</td>\n",
       "      <td>['people']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁people']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12856</td>\n",
       "      <td>6</td>\n",
       "      <td>peple</td>\n",
       "      <td>deletion</td>\n",
       "      <td>5</td>\n",
       "      <td>[p, eple]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player</td>\n",
       "      <td>['player']</td>\n",
       "      <td>1</td>\n",
       "      <td>['player']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁player']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12022</td>\n",
       "      <td>6</td>\n",
       "      <td>pllayer</td>\n",
       "      <td>insertion</td>\n",
       "      <td>7</td>\n",
       "      <td>[pll, a, yer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>member</td>\n",
       "      <td>['member']</td>\n",
       "      <td>1</td>\n",
       "      <td>['member']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁member']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8579</td>\n",
       "      <td>6</td>\n",
       "      <td>mmber</td>\n",
       "      <td>deletion</td>\n",
       "      <td>5</td>\n",
       "      <td>[mmb, er]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>season</td>\n",
       "      <td>['season']</td>\n",
       "      <td>1</td>\n",
       "      <td>['season']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁season']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8256</td>\n",
       "      <td>6</td>\n",
       "      <td>seasdn</td>\n",
       "      <td>substitution</td>\n",
       "      <td>6</td>\n",
       "      <td>[seasd, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4908</th>\n",
       "      <td>classified</td>\n",
       "      <td>['classified']</td>\n",
       "      <td>1</td>\n",
       "      <td>['classified']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁class', 'ified']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>cassified</td>\n",
       "      <td>deletion</td>\n",
       "      <td>9</td>\n",
       "      <td>[cas, sifie, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4909</th>\n",
       "      <td>modelo</td>\n",
       "      <td>['modelo']</td>\n",
       "      <td>1</td>\n",
       "      <td>['modelo']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁modelo']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>modelco</td>\n",
       "      <td>insertion</td>\n",
       "      <td>7</td>\n",
       "      <td>[m, o, delco]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4910</th>\n",
       "      <td>clave</td>\n",
       "      <td>['clave']</td>\n",
       "      <td>1</td>\n",
       "      <td>['clave']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁cla', 've']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>caave</td>\n",
       "      <td>substitution</td>\n",
       "      <td>5</td>\n",
       "      <td>[c, aave]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4911</th>\n",
       "      <td>mito</td>\n",
       "      <td>['mit', 'o']</td>\n",
       "      <td>2</td>\n",
       "      <td>['mito']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁mit', 'o']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>mitzo</td>\n",
       "      <td>insertion</td>\n",
       "      <td>5</td>\n",
       "      <td>[mit, zo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4912</th>\n",
       "      <td>clair</td>\n",
       "      <td>['clair']</td>\n",
       "      <td>1</td>\n",
       "      <td>['clair']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁cla', 'ir']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>claeir</td>\n",
       "      <td>insertion</td>\n",
       "      <td>6</td>\n",
       "      <td>[cla, eir]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4913 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word tokens_babel_9b  token_num_babel_9b tokens_gemma_12b  \\\n",
       "0           year        ['year']                   1         ['year']   \n",
       "1         people      ['people']                   1       ['people']   \n",
       "2         player      ['player']                   1       ['player']   \n",
       "3         member      ['member']                   1       ['member']   \n",
       "4         season      ['season']                   1       ['season']   \n",
       "...          ...             ...                 ...              ...   \n",
       "4908  classified  ['classified']                   1   ['classified']   \n",
       "4909      modelo      ['modelo']                   1       ['modelo']   \n",
       "4910       clave       ['clave']                   1        ['clave']   \n",
       "4911        mito    ['mit', 'o']                   2         ['mito']   \n",
       "4912       clair       ['clair']                   1        ['clair']   \n",
       "\n",
       "      token_num_gemma_12b    tokens_llama_2_7b  token_num_llama_2_7b  \\\n",
       "0                       1            ['▁year']                     1   \n",
       "1                       1          ['▁people']                     1   \n",
       "2                       1          ['▁player']                     1   \n",
       "3                       1          ['▁member']                     1   \n",
       "4                       1          ['▁season']                     1   \n",
       "...                   ...                  ...                   ...   \n",
       "4908                    1  ['▁class', 'ified']                     2   \n",
       "4909                    1          ['▁modelo']                     1   \n",
       "4910                    1       ['▁cla', 've']                     2   \n",
       "4911                    1        ['▁mit', 'o']                     2   \n",
       "4912                    1       ['▁cla', 'ir']                     2   \n",
       "\n",
       "      avg_token_num  same_token_num  avg_token_num_rounded  avg_token_num2  \\\n",
       "0          1.000000            True                      1             1.0   \n",
       "1          1.000000            True                      1             1.0   \n",
       "2          1.000000            True                      1             1.0   \n",
       "3          1.000000            True                      1             1.0   \n",
       "4          1.000000            True                      1             1.0   \n",
       "...             ...             ...                    ...             ...   \n",
       "4908       1.333333           False                      1             1.0   \n",
       "4909       1.000000            True                      1             1.0   \n",
       "4910       1.333333           False                      1             1.0   \n",
       "4911       1.666667           False                      2             1.5   \n",
       "4912       1.333333           False                      1             1.0   \n",
       "\n",
       "      same_token_num2  avg_token_num2_rounded  any_token_num_is_1   freq  \\\n",
       "0                True                       1                True  15118   \n",
       "1                True                       1                True  12856   \n",
       "2                True                       1                True  12022   \n",
       "3                True                       1                True   8579   \n",
       "4                True                       1                True   8256   \n",
       "...               ...                     ...                 ...    ...   \n",
       "4908             True                       1                True      1   \n",
       "4909             True                       1                True      1   \n",
       "4910             True                       1                True      1   \n",
       "4911            False                       2                True      1   \n",
       "4912             True                       1                True      1   \n",
       "\n",
       "      word_len typo_tokens_gemma_12b typo_type_gemma_12b  typo_word_len  \\\n",
       "0            4                 yuear           insertion              5   \n",
       "1            6                 peple            deletion              5   \n",
       "2            6               pllayer           insertion              7   \n",
       "3            6                 mmber            deletion              5   \n",
       "4            6                seasdn        substitution              6   \n",
       "...        ...                   ...                 ...            ...   \n",
       "4908        10             cassified            deletion              9   \n",
       "4909         6               modelco           insertion              7   \n",
       "4910         5                 caave        substitution              5   \n",
       "4911         4                 mitzo           insertion              5   \n",
       "4912         5                claeir           insertion              6   \n",
       "\n",
       "     splitted_typo_tokens  \n",
       "0               [yu, ear]  \n",
       "1               [p, eple]  \n",
       "2           [pll, a, yer]  \n",
       "3               [mmb, er]  \n",
       "4              [seasd, n]  \n",
       "...                   ...  \n",
       "4908      [cas, sifie, d]  \n",
       "4909        [m, o, delco]  \n",
       "4910            [c, aave]  \n",
       "4911            [mit, zo]  \n",
       "4912           [cla, eir]  \n",
       "\n",
       "[4913 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOKENIZER = \"babel_9b\"\n",
    "TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"English\"\n",
    "MIN_WORD_LEN = 4\n",
    "RANDOM_SEED = 2025  # Set a random seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df[\"word_len\"] = df[\"word\"].apply(lambda x: len(x))\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"word_len\"]>MIN_WORD_LEN-1)].reset_index(drop=True)    \n",
    "# df = df[df[f\"token_num_{TOKENIZER}\"]==1].reset_index(drop=True)\n",
    "df[[f\"typo_tokens_{TOKENIZER}\", f\"typo_type_{TOKENIZER}\"]] = df[\"word\"].apply(lambda x: pd.Series(introduce_typo(x, typo_type=None)))\n",
    "df[\"typo_word_len\"] = df[f\"typo_tokens_{TOKENIZER}\"].apply(lambda x: len(x))\n",
    "df = df[(df[\"typo_word_len\"]>MIN_WORD_LEN)].reset_index(drop=True)\n",
    "\n",
    "df[f\"splitted_typo_tokens\"] = df[f\"typo_tokens_{TOKENIZER}\"].apply(lambda x: random_split(x, MIN_WORD_LEN))\n",
    "print(df[f\"typo_type_{TOKENIZER}\"].value_counts())\n",
    "print(df[f\"splitted_typo_tokens\"].apply(len).value_counts())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "063a1895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typo_type_gemma_12b\n",
      "insertion       416\n",
      "substitution    346\n",
      "deletion        238\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "num_quantiles = 5\n",
    "df['freq_quantile'], bins = pd.qcut(df['freq'], num_quantiles, labels=False,  duplicates='drop', retbins=True)\n",
    "\n",
    "num_quantiles = df['freq_quantile'].nunique()\n",
    "samples_per_quantile = num_samples // num_quantiles\n",
    "\n",
    "sampled = []\n",
    "for quantile in range(num_quantiles):\n",
    "    quantile_df = df[df['freq_quantile'] == quantile]\n",
    "    if len(quantile_df) > 0:\n",
    "        sampled.append(quantile_df.sample(min(len(quantile_df), samples_per_quantile), \n",
    "                                          replace=False, random_state=RANDOM_SEED))\n",
    "\n",
    "sampled_df = pd.concat(sampled, ignore_index=False).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "print(sampled_df[f\"typo_type_{TOKENIZER}\"].value_counts())\n",
    "sampled_df = sampled_df[['word' , f\"typo_tokens_{TOKENIZER}\", f\"splitted_typo_tokens\", f\"typo_type_{TOKENIZER}\", \"same_token_num\", \"same_token_num2\", \"freq\", \"freq_quantile\", \"word_len\", \"typo_word_len\"]]\n",
    "sampled_df\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_typos_{TOKENIZER}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28bf76",
   "metadata": {},
   "source": [
    "# GERMAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aaed8f",
   "metadata": {},
   "source": [
    "### Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab5b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(word, MIN_WORD_LEN):\n",
    "    if len(word) <= 1:\n",
    "        return [word]\n",
    "    try:\n",
    "        num_splits = random.randint(1, min(4, len(word) - MIN_WORD_LEN - 1))\n",
    "    except:\n",
    "        num_splits = 1\n",
    "    split_points = sorted(random.sample(range(1, len(word)), num_splits))\n",
    "    tokens = [word[i:j] for i, j in zip([0] + split_points, split_points + [None])]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2174de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted_tokens\n",
      "2    1827\n",
      "3     449\n",
      "4     245\n",
      "5     129\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZER = \"babel_9b\"\n",
    "TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"German\"\n",
    "MIN_WORD_LEN = 3\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "\n",
    "# for English\n",
    "if LANGUAGE == \"German\":\n",
    "    df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "    df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"word_len\"]>MIN_WORD_LEN)].reset_index(drop=True)\n",
    "    df[f\"splitted_tokens\"] = df[\"word\"].apply(lambda x: random_split(x, MIN_WORD_LEN))\n",
    "    print(df[f\"splitted_tokens\"].apply(len).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9c923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted_tokens\n",
      "2    693\n",
      "3    166\n",
      "4     92\n",
      "5     49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "num_quantiles = 5\n",
    "df['freq_quantile'], bins = pd.qcut(df['freq'], num_quantiles, labels=False,  duplicates='drop', retbins=True)\n",
    "\n",
    "num_quantiles = df['freq_quantile'].nunique()\n",
    "samples_per_quantile = num_samples // num_quantiles\n",
    "\n",
    "sampled = []\n",
    "for quantile in range(num_quantiles):\n",
    "    quantile_df = df[df['freq_quantile'] == quantile]\n",
    "    if len(quantile_df) > 0:\n",
    "        sampled.append(quantile_df.sample(min(len(quantile_df), samples_per_quantile), \n",
    "                                          replace=False, random_state=RANDOM_SEED))\n",
    "sampled_df = pd.concat(sampled, ignore_index=False).drop_duplicates(subset=['word'])\n",
    "\n",
    "if len(sampled_df) < num_samples:\n",
    "    remaining = num_samples - len(sampled_df)\n",
    "    other_df = df.drop(sampled_df.index, errors='ignore')\n",
    "    print(f\"remaining: {remaining}, other_df: {len(other_df)}\")\n",
    "    additional_samples = other_df.sample(min(len(other_df), remaining), replace=False, random_state=RANDOM_SEED)\n",
    "    # sampled_indices += additional_samples.index.to_list()\n",
    "    sampled_df = pd.concat([sampled_df, additional_samples]).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "    print(f\"sampled_df: {len(sampled_df)}\")\n",
    "\n",
    "# Display the sampled DataFrame\n",
    "print(sampled_df[f\"splitted_tokens\"].apply(len).value_counts())\n",
    "sampled_df = sampled_df[['word' , f\"splitted_tokens\", \"same_token_num\", \"same_token_num2\", \"freq\", \"freq_quantile\", \"word_len\"]]\n",
    "sampled_df\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{TOKENIZER}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8d7ad",
   "metadata": {},
   "source": [
    "### Typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a3eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_typo_german(word, typo_type=None):\n",
    "    # Include German-specific characters\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyzäöüß'\n",
    "\n",
    "    if typo_type is None:\n",
    "        typo_type = random.choice([\"substitution\", \"deletion\", \"insertion\"])\n",
    "\n",
    "    if typo_type == \"substitution\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        original_char = word[position]\n",
    "        typo_char = random.choice([c for c in letters if c != original_char])\n",
    "        return word[:position] + typo_char + word[position + 1:], typo_type\n",
    "\n",
    "    elif typo_type == \"deletion\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        return word[:position] + word[position + 1:], typo_type\n",
    "\n",
    "    elif typo_type == \"insertion\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        typo_char = random.choice(letters)\n",
    "        return word[:position] + typo_char + word[position:], typo_type\n",
    "\n",
    "    elif typo_type == \"transposition\" and len(word) >= 3:\n",
    "        position = random.randint(1, len(word) - 2)\n",
    "        return word[:position] + word[position + 1] + word[position] + word[position + 2:], typo_type\n",
    "\n",
    "    else:\n",
    "        return word, typo_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e3888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typo_type_gemma_12b\n",
      "insertion       907\n",
      "substitution    601\n",
      "deletion        435\n",
      "Name: count, dtype: int64\n",
      "splitted_typo_tokens\n",
      "2    1386\n",
      "3     321\n",
      "4     153\n",
      "5      83\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tokens_babel_9b</th>\n",
       "      <th>token_num_babel_9b</th>\n",
       "      <th>tokens_gemma_12b</th>\n",
       "      <th>token_num_gemma_12b</th>\n",
       "      <th>tokens_llama_2_7b</th>\n",
       "      <th>token_num_llama_2_7b</th>\n",
       "      <th>avg_token_num</th>\n",
       "      <th>same_token_num</th>\n",
       "      <th>avg_token_num_rounded</th>\n",
       "      <th>avg_token_num2</th>\n",
       "      <th>same_token_num2</th>\n",
       "      <th>avg_token_num2_rounded</th>\n",
       "      <th>any_token_num_is_1</th>\n",
       "      <th>freq</th>\n",
       "      <th>word_len</th>\n",
       "      <th>typo_tokens_gemma_12b</th>\n",
       "      <th>typo_type_gemma_12b</th>\n",
       "      <th>typo_word_len</th>\n",
       "      <th>splitted_typo_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jahr</td>\n",
       "      <td>['J', 'ahr']</td>\n",
       "      <td>2</td>\n",
       "      <td>['Jahr']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁Jahr']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>31379</td>\n",
       "      <td>4</td>\n",
       "      <td>Juahr</td>\n",
       "      <td>insertion</td>\n",
       "      <td>5</td>\n",
       "      <td>[Juah, r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weblinks</td>\n",
       "      <td>['We', 'bl', 'inks']</td>\n",
       "      <td>3</td>\n",
       "      <td>['Weblinks']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁Weblinks']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>12234</td>\n",
       "      <td>8</td>\n",
       "      <td>Welinks</td>\n",
       "      <td>deletion</td>\n",
       "      <td>7</td>\n",
       "      <td>[W, e, links]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stadt</td>\n",
       "      <td>['St', 'adt']</td>\n",
       "      <td>2</td>\n",
       "      <td>['Stadt']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁Stadt']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>8535</td>\n",
       "      <td>5</td>\n",
       "      <td>Sltadt</td>\n",
       "      <td>insertion</td>\n",
       "      <td>6</td>\n",
       "      <td>[Slt, adt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Literatur</td>\n",
       "      <td>['Liter', 'atur']</td>\n",
       "      <td>2</td>\n",
       "      <td>['Literatur']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁Literatur']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>7820</td>\n",
       "      <td>9</td>\n",
       "      <td>Liteatur</td>\n",
       "      <td>deletion</td>\n",
       "      <td>8</td>\n",
       "      <td>[Lit, eatur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geschichte</td>\n",
       "      <td>['G', 'esch', 'ichte']</td>\n",
       "      <td>3</td>\n",
       "      <td>['Geschichte']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁Geschichte']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>6754</td>\n",
       "      <td>10</td>\n",
       "      <td>Geschicdte</td>\n",
       "      <td>substitution</td>\n",
       "      <td>10</td>\n",
       "      <td>[Gesc, hicdte]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>Flower</td>\n",
       "      <td>['Fl', 'ower']</td>\n",
       "      <td>2</td>\n",
       "      <td>['Flower']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁F', 'lower']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Fflower</td>\n",
       "      <td>insertion</td>\n",
       "      <td>7</td>\n",
       "      <td>[F, flowe, r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>Number</td>\n",
       "      <td>['Number']</td>\n",
       "      <td>1</td>\n",
       "      <td>['Number']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁Number']</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Nucmber</td>\n",
       "      <td>insertion</td>\n",
       "      <td>7</td>\n",
       "      <td>[Nucmbe, r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>Finance</td>\n",
       "      <td>['Finance']</td>\n",
       "      <td>1</td>\n",
       "      <td>['Finance']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁Fin', 'ance']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Financbe</td>\n",
       "      <td>insertion</td>\n",
       "      <td>8</td>\n",
       "      <td>[Fin, an, c, be]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>Finish</td>\n",
       "      <td>['Finish']</td>\n",
       "      <td>1</td>\n",
       "      <td>['Finish']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁Fin', 'ish']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Fiwish</td>\n",
       "      <td>substitution</td>\n",
       "      <td>6</td>\n",
       "      <td>[F, iwish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>Fixed</td>\n",
       "      <td>['Fixed']</td>\n",
       "      <td>1</td>\n",
       "      <td>['Fixed']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁F', 'ixed']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Fiped</td>\n",
       "      <td>substitution</td>\n",
       "      <td>5</td>\n",
       "      <td>[Fip, ed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1943 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word         tokens_babel_9b  token_num_babel_9b tokens_gemma_12b  \\\n",
       "0           Jahr            ['J', 'ahr']                   2         ['Jahr']   \n",
       "1       Weblinks    ['We', 'bl', 'inks']                   3     ['Weblinks']   \n",
       "2          Stadt           ['St', 'adt']                   2        ['Stadt']   \n",
       "3      Literatur       ['Liter', 'atur']                   2    ['Literatur']   \n",
       "4     Geschichte  ['G', 'esch', 'ichte']                   3   ['Geschichte']   \n",
       "...          ...                     ...                 ...              ...   \n",
       "1938      Flower          ['Fl', 'ower']                   2       ['Flower']   \n",
       "1939      Number              ['Number']                   1       ['Number']   \n",
       "1940     Finance             ['Finance']                   1      ['Finance']   \n",
       "1941      Finish              ['Finish']                   1       ['Finish']   \n",
       "1942       Fixed               ['Fixed']                   1        ['Fixed']   \n",
       "\n",
       "      token_num_gemma_12b tokens_llama_2_7b  token_num_llama_2_7b  \\\n",
       "0                       1         ['▁Jahr']                     1   \n",
       "1                       1     ['▁Weblinks']                     1   \n",
       "2                       1        ['▁Stadt']                     1   \n",
       "3                       1    ['▁Literatur']                     1   \n",
       "4                       1   ['▁Geschichte']                     1   \n",
       "...                   ...               ...                   ...   \n",
       "1938                    1   ['▁F', 'lower']                     2   \n",
       "1939                    1       ['▁Number']                     1   \n",
       "1940                    1  ['▁Fin', 'ance']                     2   \n",
       "1941                    1   ['▁Fin', 'ish']                     2   \n",
       "1942                    1    ['▁F', 'ixed']                     2   \n",
       "\n",
       "      avg_token_num  same_token_num  avg_token_num_rounded  avg_token_num2  \\\n",
       "0          1.333333           False                      1             1.5   \n",
       "1          1.666667           False                      2             2.0   \n",
       "2          1.333333           False                      1             1.5   \n",
       "3          1.333333           False                      1             1.5   \n",
       "4          1.666667           False                      2             2.0   \n",
       "...             ...             ...                    ...             ...   \n",
       "1938       1.666667           False                      2             1.5   \n",
       "1939       1.000000            True                      1             1.0   \n",
       "1940       1.333333           False                      1             1.0   \n",
       "1941       1.333333           False                      1             1.0   \n",
       "1942       1.333333           False                      1             1.0   \n",
       "\n",
       "      same_token_num2  avg_token_num2_rounded  any_token_num_is_1   freq  \\\n",
       "0               False                       2                True  31379   \n",
       "1               False                       2                True  12234   \n",
       "2               False                       2                True   8535   \n",
       "3               False                       2                True   7820   \n",
       "4               False                       2                True   6754   \n",
       "...               ...                     ...                 ...    ...   \n",
       "1938            False                       2                True      1   \n",
       "1939             True                       1                True      1   \n",
       "1940             True                       1                True      1   \n",
       "1941             True                       1                True      1   \n",
       "1942             True                       1                True      1   \n",
       "\n",
       "      word_len typo_tokens_gemma_12b typo_type_gemma_12b  typo_word_len  \\\n",
       "0            4                 Juahr           insertion              5   \n",
       "1            8               Welinks            deletion              7   \n",
       "2            5                Sltadt           insertion              6   \n",
       "3            9              Liteatur            deletion              8   \n",
       "4           10            Geschicdte        substitution             10   \n",
       "...        ...                   ...                 ...            ...   \n",
       "1938         6               Fflower           insertion              7   \n",
       "1939         6               Nucmber           insertion              7   \n",
       "1940         7              Financbe           insertion              8   \n",
       "1941         6                Fiwish        substitution              6   \n",
       "1942         5                 Fiped        substitution              5   \n",
       "\n",
       "     splitted_typo_tokens  \n",
       "0               [Juah, r]  \n",
       "1           [W, e, links]  \n",
       "2              [Slt, adt]  \n",
       "3            [Lit, eatur]  \n",
       "4          [Gesc, hicdte]  \n",
       "...                   ...  \n",
       "1938        [F, flowe, r]  \n",
       "1939          [Nucmbe, r]  \n",
       "1940     [Fin, an, c, be]  \n",
       "1941           [F, iwish]  \n",
       "1942            [Fip, ed]  \n",
       "\n",
       "[1943 rows x 20 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOKENIZER = \"babel_9b\"\n",
    "TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"German\"\n",
    "MIN_WORD_LEN = 4\n",
    "RANDOM_SEED = 2025  # Set a random seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df[\"word_len\"] = df[\"word\"].apply(lambda x: len(x))\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"word_len\"]>MIN_WORD_LEN-1)].reset_index(drop=True)    \n",
    "# df = df[df[f\"token_num_{TOKENIZER}\"]==1].reset_index(drop=True)\n",
    "df[[f\"typo_tokens_{TOKENIZER}\", f\"typo_type_{TOKENIZER}\"]] = df[\"word\"].apply(lambda x: pd.Series(introduce_typo_german(x, typo_type=None)))\n",
    "df[\"typo_word_len\"] = df[f\"typo_tokens_{TOKENIZER}\"].apply(lambda x: len(x))\n",
    "df = df[(df[\"typo_word_len\"]>MIN_WORD_LEN)].reset_index(drop=True)\n",
    "\n",
    "df[f\"splitted_typo_tokens\"] = df[f\"typo_tokens_{TOKENIZER}\"].apply(lambda x: random_split(x, MIN_WORD_LEN))\n",
    "print(df[f\"typo_type_{TOKENIZER}\"].value_counts())\n",
    "print(df[f\"splitted_typo_tokens\"].apply(len).value_counts())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4487a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typo_type_gemma_12b\n",
      "insertion       452\n",
      "substitution    318\n",
      "deletion        230\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "num_quantiles = 5\n",
    "df['freq_quantile'], bins = pd.qcut(df['freq'], num_quantiles, labels=False,  duplicates='drop', retbins=True)\n",
    "\n",
    "num_quantiles = df['freq_quantile'].nunique()\n",
    "samples_per_quantile = num_samples // num_quantiles\n",
    "\n",
    "sampled = []\n",
    "for quantile in range(num_quantiles):\n",
    "    quantile_df = df[df['freq_quantile'] == quantile]\n",
    "    if len(quantile_df) > 0:\n",
    "        sampled.append(quantile_df.sample(min(len(quantile_df), samples_per_quantile), \n",
    "                                          replace=False, random_state=RANDOM_SEED))\n",
    "\n",
    "sampled_df = pd.concat(sampled, ignore_index=False).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "\n",
    "if len(sampled_df) < num_samples:\n",
    "    remaining = num_samples - len(sampled_df)\n",
    "    other_df = df.drop(sampled_df.index, errors='ignore')\n",
    "    print(f\"remaining: {remaining}, other_df: {len(other_df)}\")\n",
    "    additional_samples = other_df.sample(min(len(other_df), remaining), replace=False, random_state=RANDOM_SEED)\n",
    "    # sampled_indices += additional_samples.index.to_list()\n",
    "    sampled_df = pd.concat([sampled_df, additional_samples]).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "    print(f\"sampled_df: {len(sampled_df)}\")\n",
    "\n",
    "# Display the sampled DataFrame\n",
    "print(sampled_df[f\"typo_type_{TOKENIZER}\"].value_counts())\n",
    "sampled_df = sampled_df[['word' , f\"typo_tokens_{TOKENIZER}\", f\"splitted_typo_tokens\", f\"typo_type_{TOKENIZER}\", \"same_token_num\", \"same_token_num2\", \"freq\", \"freq_quantile\", \"word_len\", \"typo_word_len\"]]\n",
    "sampled_df\n",
    "\n",
    "\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_typos_{TOKENIZER}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25763096",
   "metadata": {},
   "source": [
    "# KOREAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9642bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_jamos(word):\n",
    "    decomposed = h2j(word)  # Decomposes into jamos\n",
    "    return len(decomposed)\n",
    "\n",
    "def split_jamos(word):\n",
    "    return list(h2j(word))  # decompose Hangul syllables to jamos\n",
    "\n",
    "def join_jamos(jamos):\n",
    "    return j2h(''.join(jamos))  # compose jamos back to syllables\n",
    "\n",
    "def random_split_korean(word, MIN_JAMO_LEN):\n",
    "    jamos = list(split_jamos(word))\n",
    "    if len(jamos) <= 1:\n",
    "        return [word]\n",
    "    num_splits = random.randint(1, min(4, len(jamos) - MIN_JAMO_LEN))\n",
    "    split_points = sorted(random.sample(range(1, len(jamos)), num_splits))\n",
    "    jamo_tokens = [jamos[i:j] for i, j in zip([0] + split_points, split_points + [None])]\n",
    "    return [''.join(token) for token in jamo_tokens]\n",
    "\n",
    "# def random_split_korean(word, min_jamo_len):\n",
    "#     if not word or len(word) == 1:\n",
    "#         return [word]\n",
    "\n",
    "#     syllables = list(word)\n",
    "#     jamo_lengths = [count_jamos(syl) for syl in syllables]\n",
    "\n",
    "#     # Accumulate positions ensuring min_jamo_len per segment\n",
    "#     valid_indices = []\n",
    "#     total = 0\n",
    "#     for i in range(1, len(syllables)):\n",
    "#         total += jamo_lengths[i - 1]\n",
    "#         if total >= min_jamo_len:\n",
    "#             valid_indices.append(i)\n",
    "\n",
    "#     if not valid_indices:\n",
    "#         return [word]\n",
    "\n",
    "#     num_splits = random.randint(1, min(4, len(valid_indices)))\n",
    "#     split_points = sorted(random.sample(valid_indices, num_splits))\n",
    "\n",
    "#     segments = [syllables[i:j] for i, j in zip([0] + split_points, split_points + [None])]\n",
    "#     return [''.join(seg) for seg in segments]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df3829",
   "metadata": {},
   "source": [
    "### Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a55a81bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tokens_babel_9b</th>\n",
       "      <th>token_num_babel_9b</th>\n",
       "      <th>tokens_gemma_12b</th>\n",
       "      <th>token_num_gemma_12b</th>\n",
       "      <th>tokens_llama_2_7b</th>\n",
       "      <th>token_num_llama_2_7b</th>\n",
       "      <th>avg_token_num</th>\n",
       "      <th>same_token_num</th>\n",
       "      <th>avg_token_num_rounded</th>\n",
       "      <th>avg_token_num2</th>\n",
       "      <th>same_token_num2</th>\n",
       "      <th>avg_token_num2_rounded</th>\n",
       "      <th>any_token_num_is_1</th>\n",
       "      <th>freq</th>\n",
       "      <th>word_len</th>\n",
       "      <th>jamo_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>역</td>\n",
       "      <td>['ìĹŃ']</td>\n",
       "      <td>1</td>\n",
       "      <td>['역']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁', '역']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>16297</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>팀</td>\n",
       "      <td>['íĮĢ']</td>\n",
       "      <td>1</td>\n",
       "      <td>['팀']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁', '&lt;0xED&gt;', '&lt;0x8C&gt;', '&lt;0x80&gt;']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5551</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>말</td>\n",
       "      <td>['ë§Ĳ']</td>\n",
       "      <td>1</td>\n",
       "      <td>['말']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁', '&lt;0xEB&gt;', '&lt;0xA7&gt;', '&lt;0x90&gt;']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5115</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>군</td>\n",
       "      <td>['êµ°']</td>\n",
       "      <td>1</td>\n",
       "      <td>['군']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁', '군']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4356</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>전</td>\n",
       "      <td>['ìłĦ']</td>\n",
       "      <td>1</td>\n",
       "      <td>['전']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁', '전']</td>\n",
       "      <td>2</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>도록</td>\n",
       "      <td>['ëıĦë¡Ŀ']</td>\n",
       "      <td>1</td>\n",
       "      <td>['도록']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁', '도', '&lt;0xEB&gt;', '&lt;0xA1&gt;', '&lt;0x9D&gt;']</td>\n",
       "      <td>5</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>엤</td>\n",
       "      <td>['ìĹ¤']</td>\n",
       "      <td>1</td>\n",
       "      <td>['&lt;0xEC&gt;', '&lt;0x97&gt;', '&lt;0xA4&gt;']</td>\n",
       "      <td>3</td>\n",
       "      <td>['▁', '&lt;0xEC&gt;', '&lt;0x97&gt;', '&lt;0xA4&gt;']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>둡</td>\n",
       "      <td>['ëĳ¡']</td>\n",
       "      <td>1</td>\n",
       "      <td>['둡']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁', '&lt;0xEB&gt;', '&lt;0x91&gt;', '&lt;0xA1&gt;']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>둣</td>\n",
       "      <td>['ëĳ£']</td>\n",
       "      <td>1</td>\n",
       "      <td>['둣']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁', '&lt;0xEB&gt;', '&lt;0x91&gt;', '&lt;0xA3&gt;']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>옐</td>\n",
       "      <td>['ìĺĲ']</td>\n",
       "      <td>1</td>\n",
       "      <td>['옐']</td>\n",
       "      <td>1</td>\n",
       "      <td>['▁', '&lt;0xEC&gt;', '&lt;0x98&gt;', '&lt;0x90&gt;']</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>835 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    word tokens_babel_9b  token_num_babel_9b                tokens_gemma_12b  \\\n",
       "0      역         ['ìĹŃ']                   1                           ['역']   \n",
       "1      팀         ['íĮĢ']                   1                           ['팀']   \n",
       "2      말         ['ë§Ĳ']                   1                           ['말']   \n",
       "3      군         ['êµ°']                   1                           ['군']   \n",
       "4      전         ['ìłĦ']                   1                           ['전']   \n",
       "..   ...             ...                 ...                             ...   \n",
       "830   도록      ['ëıĦë¡Ŀ']                   1                          ['도록']   \n",
       "831    엤         ['ìĹ¤']                   1  ['<0xEC>', '<0x97>', '<0xA4>']   \n",
       "832    둡         ['ëĳ¡']                   1                           ['둡']   \n",
       "833    둣         ['ëĳ£']                   1                           ['둣']   \n",
       "834    옐         ['ìĺĲ']                   1                           ['옐']   \n",
       "\n",
       "     token_num_gemma_12b                         tokens_llama_2_7b  \\\n",
       "0                      1                                ['▁', '역']   \n",
       "1                      1       ['▁', '<0xED>', '<0x8C>', '<0x80>']   \n",
       "2                      1       ['▁', '<0xEB>', '<0xA7>', '<0x90>']   \n",
       "3                      1                                ['▁', '군']   \n",
       "4                      1                                ['▁', '전']   \n",
       "..                   ...                                       ...   \n",
       "830                    1  ['▁', '도', '<0xEB>', '<0xA1>', '<0x9D>']   \n",
       "831                    3       ['▁', '<0xEC>', '<0x97>', '<0xA4>']   \n",
       "832                    1       ['▁', '<0xEB>', '<0x91>', '<0xA1>']   \n",
       "833                    1       ['▁', '<0xEB>', '<0x91>', '<0xA3>']   \n",
       "834                    1       ['▁', '<0xEC>', '<0x98>', '<0x90>']   \n",
       "\n",
       "     token_num_llama_2_7b  avg_token_num  same_token_num  \\\n",
       "0                       2       1.333333           False   \n",
       "1                       4       2.000000           False   \n",
       "2                       4       2.000000           False   \n",
       "3                       2       1.333333           False   \n",
       "4                       2       1.333333           False   \n",
       "..                    ...            ...             ...   \n",
       "830                     5       2.333333           False   \n",
       "831                     4       2.666667           False   \n",
       "832                     4       2.000000           False   \n",
       "833                     4       2.000000           False   \n",
       "834                     4       2.000000           False   \n",
       "\n",
       "     avg_token_num_rounded  avg_token_num2  same_token_num2  \\\n",
       "0                        1             1.0             True   \n",
       "1                        2             1.0             True   \n",
       "2                        2             1.0             True   \n",
       "3                        1             1.0             True   \n",
       "4                        1             1.0             True   \n",
       "..                     ...             ...              ...   \n",
       "830                      2             1.0             True   \n",
       "831                      3             2.0            False   \n",
       "832                      2             1.0             True   \n",
       "833                      2             1.0             True   \n",
       "834                      2             1.0             True   \n",
       "\n",
       "     avg_token_num2_rounded  any_token_num_is_1   freq  word_len  jamo_len  \n",
       "0                         1                True  16297         1         3  \n",
       "1                         1                True   5551         1         3  \n",
       "2                         1                True   5115         1         3  \n",
       "3                         1                True   4356         1         3  \n",
       "4                         1                True   4316         1         3  \n",
       "..                      ...                 ...    ...       ...       ...  \n",
       "830                       1                True      1         2         5  \n",
       "831                       2                True      1         1         3  \n",
       "832                       1                True      1         1         3  \n",
       "833                       1                True      1         1         3  \n",
       "834                       1                True      1         1         3  \n",
       "\n",
       "[835 rows x 17 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER = \"babel_9b\"\n",
    "# TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"Korean\"\n",
    "MIN_JAMO_LEN = 2\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df['jamo_len'] = df['word'].apply(count_jamos)\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"jamo_len\"]>MIN_JAMO_LEN)].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "155ed8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted_tokens_babel_9b\n",
      "2    744\n",
      "3     57\n",
      "4     28\n",
      "5      6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = \"babel_9b\"\n",
    "# TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"Korean\"\n",
    "MIN_JAMO_LEN = 2\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "\n",
    "# for Korean\n",
    "if LANGUAGE == \"Korean\":\n",
    "    df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "    df['jamo_len'] = df['word'].apply(count_jamos)\n",
    "    df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"jamo_len\"]>MIN_JAMO_LEN)].reset_index(drop=True)\n",
    "    df[f\"splitted_tokens_{TOKENIZER}\"] = df[\"word\"].apply(lambda x: random_split_korean(x, MIN_JAMO_LEN))\n",
    "    print(df[f\"splitted_tokens_{TOKENIZER}\"].apply(len).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1a2c1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [여, ᆨ]\n",
       "1            [ᄐ, ᅵᆷ]\n",
       "2            [마, ᆯ]\n",
       "3            [ᄀ, ᅮᆫ]\n",
       "4            [저, ᆫ]\n",
       "           ...      \n",
       "830    [도, ᄅ, ᅩ, ᆨ]\n",
       "831          [에, ᆻ]\n",
       "832          [ᄃ, ᅮᆸ]\n",
       "833          [ᄃ, ᅮᆺ]\n",
       "834          [예, ᆯ]\n",
       "Name: splitted_tokens_babel_9b, Length: 835, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[f\"splitted_tokens_{TOKENIZER}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fab509",
   "metadata": {},
   "source": [
    "### Typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8da5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from jamo import h2j, j2h, hangul_to_jamo, jamo_to_hangul\n",
    "\n",
    "# Standard Jamo sets\n",
    "CHO = ['ㄱ','ㄲ','ㄴ','ㄷ','ㄸ','ㄹ','ㅁ','ㅂ','ㅃ','ㅅ','ㅆ','ㅇ','ㅈ','ㅉ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "JUN = ['ㅏ','ㅐ','ㅑ','ㅒ','ㅓ','ㅔ','ㅕ','ㅖ','ㅗ','ㅘ','ㅙ','ㅚ','ㅛ','ㅜ','ㅝ','ㅞ','ㅟ','ㅠ','ㅡ','ㅢ','ㅣ']\n",
    "JON = ['','ㄱ','ㄲ','ㄳ','ㄴ','ㄵ','ㄶ','ㄷ','ㄹ','ㄺ','ㄻ','ㄼ','ㄽ','ㄾ','ㄿ','ㅀ','ㅁ','ㅂ','ㅄ','ㅅ','ㅆ','ㅇ','ㅈ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "\n",
    "def decompose_syllable(s):\n",
    "    code = ord(s) - 0xAC00\n",
    "    cho = code // (21 * 28)\n",
    "    jun = (code % (21 * 28)) // 28\n",
    "    jon = code % 28\n",
    "    return cho, jun, jon\n",
    "\n",
    "def compose_syllable(cho, jun, jon):\n",
    "    return chr(0xAC00 + cho * 21 * 28 + jun * 28 + jon)\n",
    "\n",
    "def introduce_korean_syllable_typo(word, typo_type=None):\n",
    "    if typo_type is None:\n",
    "        typo_type = random.choice([\"substitution\", \"deletion\", \"insertion\", \"transposition\"])\n",
    "\n",
    "    chars = list(word)\n",
    "    if not chars:\n",
    "        return word, typo_type\n",
    "\n",
    "    idx = random.randint(0, len(chars) - 1)\n",
    "    c = chars[idx]\n",
    "    try:\n",
    "        cho, jun, jon = decompose_syllable(c)\n",
    "    except:\n",
    "        return word, typo_type  # Skip non-Hangul\n",
    "\n",
    "    if typo_type == \"substitution\":\n",
    "        part = random.choice(['cho', 'jun', 'jon'])\n",
    "        if part == 'cho':\n",
    "            cho = random.choice([i for i in range(len(CHO)) if i != cho])\n",
    "        elif part == 'jun':\n",
    "            jun = random.choice([i for i in range(len(JUN)) if i != jun])\n",
    "        elif part == 'jon':\n",
    "            jon = random.choice([i for i in range(len(JON)) if i != jon])\n",
    "    elif typo_type == \"deletion\":\n",
    "        part = random.choice(['cho', 'jun', 'jon'])\n",
    "        if part == 'jon':\n",
    "            jon = 0  # Remove final\n",
    "    elif typo_type == \"insertion\":\n",
    "        if jon == 0:\n",
    "            jon = random.randint(1, len(JON) - 1)  # Add a final\n",
    "    elif typo_type == \"transposition\":\n",
    "        # Only meaningful if jon exists — swap cho and jon\n",
    "        if jon != 0:\n",
    "            cho, jon = jon % len(CHO), cho % len(JON)\n",
    "\n",
    "    chars[idx] = compose_syllable(cho, jun, jon)\n",
    "    return ''.join(chars), typo_type\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
