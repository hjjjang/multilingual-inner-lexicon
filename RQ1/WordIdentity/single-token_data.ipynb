{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from jamo import h2j, j2h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# ENGLISH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(word, MIN_WORD_LEN):\n",
    "    if len(word) <= 1:\n",
    "        return [word]\n",
    "    # num_splits = random.randint(1, min(4, len(word) - MIN_WORD_LEN))\n",
    "    try:\n",
    "        num_splits = random.randint(1, min(4, len(word) - MIN_WORD_LEN - 1))\n",
    "    except:\n",
    "        num_splits = 1\n",
    "    split_points = sorted(random.sample(range(1, len(word)), num_splits))\n",
    "    tokens = [word[i:j] for i, j in zip([0] + split_points, split_points + [None])]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER = \"babel_9b\"\n",
    "# TOKENIZER = \"gemma_12b\"\n",
    "TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"English\"\n",
    "MIN_WORD_LEN = 3\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "\n",
    "# for English\n",
    "if LANGUAGE == \"English\":\n",
    "    df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "    df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"word_len\"]>MIN_WORD_LEN)].reset_index(drop=True)\n",
    "    df[f\"splitted_tokens\"] = df[\"word\"].apply(lambda x: random_split(x, MIN_WORD_LEN))\n",
    "    print(df[f\"splitted_tokens\"].apply(len).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "num_quantiles = 5\n",
    "df['freq_quantile'], bins = pd.qcut(df['freq'], num_quantiles, labels=False,  duplicates='drop', retbins=True)\n",
    "\n",
    "num_quantiles = df['freq_quantile'].nunique()\n",
    "samples_per_quantile = num_samples // num_quantiles\n",
    "\n",
    "sampled = []\n",
    "for quantile in range(num_quantiles):\n",
    "    quantile_df = df[df['freq_quantile'] == quantile]\n",
    "    if len(quantile_df) > 0:\n",
    "        sampled.append(quantile_df.sample(min(len(quantile_df), samples_per_quantile), \n",
    "                                          replace=False, random_state=RANDOM_SEED))\n",
    "\n",
    "sampled_df = pd.concat(sampled, ignore_index=False).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "\n",
    "model_name = \"Llama-2-7b-chat-hf\"\n",
    "# Display the sampled DataFrame\n",
    "print(sampled_df[f\"splitted_tokens\"].apply(len).value_counts())\n",
    "sampled_df = sampled_df[['word' , f\"splitted_tokens\", \"same_token_num\", \"same_token_num2\", \"freq\", \"freq_quantile\", \"word_len\"]]\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{model_name}_{LANGUAGE}_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_typo(word, typo_type=None):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    if typo_type is None:\n",
    "        # typo_type = random.choice([\"substitution\", \"deletion\", \"insertion\", \"transposition\"])\n",
    "        typo_type = random.choice([\"substitution\", \"deletion\", \"insertion\"])\n",
    "\n",
    "    if typo_type == \"substitution\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        original_char = word[position]\n",
    "        typo_char = random.choice([c for c in letters if c != original_char])\n",
    "        return word[:position] + typo_char + word[position + 1:], typo_type\n",
    "    elif typo_type == \"deletion\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        return word[:position] + word[position + 1:], typo_type\n",
    "    elif typo_type == \"insertion\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        typo_char = random.choice(letters)\n",
    "        return word[:position] + typo_char + word[position:], typo_type\n",
    "    elif typo_type == \"transposition\":\n",
    "        position = random.randint(1, len(word) - 2)\n",
    "        return word[:position] + word[position + 1] + word[position] + word[position + 2:], typo_type\n",
    "    else:\n",
    "        return word, typo_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER = \"babel_9b\"\n",
    "TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"English\"\n",
    "MIN_WORD_LEN = 4\n",
    "RANDOM_SEED = 2025  # Set a random seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df[\"word_len\"] = df[\"word\"].apply(lambda x: len(x))\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"word_len\"]>MIN_WORD_LEN-1)].reset_index(drop=True)    \n",
    "# df = df[df[f\"token_num_{TOKENIZER}\"]==1].reset_index(drop=True)\n",
    "df[[f\"typo_tokens_{TOKENIZER}\", f\"typo_type_{TOKENIZER}\"]] = df[\"word\"].apply(lambda x: pd.Series(introduce_typo(x, typo_type=None)))\n",
    "df[\"typo_word_len\"] = df[f\"typo_tokens_{TOKENIZER}\"].apply(lambda x: len(x))\n",
    "df = df[(df[\"typo_word_len\"]>MIN_WORD_LEN)].reset_index(drop=True)\n",
    "\n",
    "df[f\"splitted_typo_tokens\"] = df[f\"typo_tokens_{TOKENIZER}\"].apply(lambda x: random_split(x, MIN_WORD_LEN))\n",
    "print(df[f\"typo_type_{TOKENIZER}\"].value_counts())\n",
    "print(df[f\"splitted_typo_tokens\"].apply(len).value_counts())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "num_quantiles = 5\n",
    "df['freq_quantile'], bins = pd.qcut(df['freq'], num_quantiles, labels=False,  duplicates='drop', retbins=True)\n",
    "\n",
    "num_quantiles = df['freq_quantile'].nunique()\n",
    "samples_per_quantile = num_samples // num_quantiles\n",
    "\n",
    "sampled = []\n",
    "for quantile in range(num_quantiles):\n",
    "    quantile_df = df[df['freq_quantile'] == quantile]\n",
    "    if len(quantile_df) > 0:\n",
    "        sampled.append(quantile_df.sample(min(len(quantile_df), samples_per_quantile), \n",
    "                                          replace=False, random_state=RANDOM_SEED))\n",
    "\n",
    "sampled_df = pd.concat(sampled, ignore_index=False).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "print(sampled_df[f\"typo_type_{TOKENIZER}\"].value_counts())\n",
    "sampled_df = sampled_df[['word' , f\"typo_tokens_{TOKENIZER}\", f\"splitted_typo_tokens\", f\"typo_type_{TOKENIZER}\", \"same_token_num\", \"same_token_num2\", \"freq\", \"freq_quantile\", \"word_len\", \"typo_word_len\"]]\n",
    "sampled_df\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_typos_{TOKENIZER}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# GERMAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(word, MIN_WORD_LEN):\n",
    "    if len(word) <= 1:\n",
    "        return [word]\n",
    "    try:\n",
    "        num_splits = random.randint(1, min(4, len(word) - MIN_WORD_LEN - 1))\n",
    "    except:\n",
    "        num_splits = 1\n",
    "    split_points = sorted(random.sample(range(1, len(word)), num_splits))\n",
    "    tokens = [word[i:j] for i, j in zip([0] + split_points, split_points + [None])]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER = \"babel_9b\"\n",
    "TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"German\"\n",
    "MIN_WORD_LEN = 3\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "\n",
    "# for English\n",
    "if LANGUAGE == \"German\":\n",
    "    df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "    df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"word_len\"]>MIN_WORD_LEN)].reset_index(drop=True)\n",
    "    df[f\"splitted_tokens\"] = df[\"word\"].apply(lambda x: random_split(x, MIN_WORD_LEN))\n",
    "    print(df[f\"splitted_tokens\"].apply(len).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "num_quantiles = 5\n",
    "df['freq_quantile'], bins = pd.qcut(df['freq'], num_quantiles, labels=False,  duplicates='drop', retbins=True)\n",
    "\n",
    "num_quantiles = df['freq_quantile'].nunique()\n",
    "samples_per_quantile = num_samples // num_quantiles\n",
    "\n",
    "sampled = []\n",
    "for quantile in range(num_quantiles):\n",
    "    quantile_df = df[df['freq_quantile'] == quantile]\n",
    "    if len(quantile_df) > 0:\n",
    "        sampled.append(quantile_df.sample(min(len(quantile_df), samples_per_quantile), \n",
    "                                          replace=False, random_state=RANDOM_SEED))\n",
    "sampled_df = pd.concat(sampled, ignore_index=False).drop_duplicates(subset=['word'])\n",
    "\n",
    "if len(sampled_df) < num_samples:\n",
    "    remaining = num_samples - len(sampled_df)\n",
    "    other_df = df.drop(sampled_df.index, errors='ignore')\n",
    "    print(f\"remaining: {remaining}, other_df: {len(other_df)}\")\n",
    "    additional_samples = other_df.sample(min(len(other_df), remaining), replace=False, random_state=RANDOM_SEED)\n",
    "    # sampled_indices += additional_samples.index.to_list()\n",
    "    sampled_df = pd.concat([sampled_df, additional_samples]).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "    print(f\"sampled_df: {len(sampled_df)}\")\n",
    "\n",
    "# Display the sampled DataFrame\n",
    "print(sampled_df[f\"splitted_tokens\"].apply(len).value_counts())\n",
    "sampled_df = sampled_df[['word' , f\"splitted_tokens\", \"same_token_num\", \"same_token_num2\", \"freq\", \"freq_quantile\", \"word_len\"]]\n",
    "sampled_df\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{TOKENIZER}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_typo_german(word, typo_type=None):\n",
    "    # Include German-specific characters\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyzäöüß'\n",
    "\n",
    "    if typo_type is None:\n",
    "        typo_type = random.choice([\"substitution\", \"deletion\", \"insertion\"])\n",
    "\n",
    "    if typo_type == \"substitution\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        original_char = word[position]\n",
    "        typo_char = random.choice([c for c in letters if c != original_char])\n",
    "        return word[:position] + typo_char + word[position + 1:], typo_type\n",
    "\n",
    "    elif typo_type == \"deletion\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        return word[:position] + word[position + 1:], typo_type\n",
    "\n",
    "    elif typo_type == \"insertion\":\n",
    "        position = random.randint(1, len(word) - 1)\n",
    "        typo_char = random.choice(letters)\n",
    "        return word[:position] + typo_char + word[position:], typo_type\n",
    "\n",
    "    elif typo_type == \"transposition\" and len(word) >= 3:\n",
    "        position = random.randint(1, len(word) - 2)\n",
    "        return word[:position] + word[position + 1] + word[position] + word[position + 2:], typo_type\n",
    "\n",
    "    else:\n",
    "        return word, typo_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER = \"babel_9b\"\n",
    "TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"German\"\n",
    "MIN_WORD_LEN = 4\n",
    "RANDOM_SEED = 2025  # Set a random seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df[\"word_len\"] = df[\"word\"].apply(lambda x: len(x))\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"word_len\"]>MIN_WORD_LEN-1)].reset_index(drop=True)    \n",
    "# df = df[df[f\"token_num_{TOKENIZER}\"]==1].reset_index(drop=True)\n",
    "df[[f\"typo_tokens_{TOKENIZER}\", f\"typo_type_{TOKENIZER}\"]] = df[\"word\"].apply(lambda x: pd.Series(introduce_typo_german(x, typo_type=None)))\n",
    "df[\"typo_word_len\"] = df[f\"typo_tokens_{TOKENIZER}\"].apply(lambda x: len(x))\n",
    "df = df[(df[\"typo_word_len\"]>MIN_WORD_LEN)].reset_index(drop=True)\n",
    "\n",
    "df[f\"splitted_typo_tokens\"] = df[f\"typo_tokens_{TOKENIZER}\"].apply(lambda x: random_split(x, MIN_WORD_LEN))\n",
    "print(df[f\"typo_type_{TOKENIZER}\"].value_counts())\n",
    "print(df[f\"splitted_typo_tokens\"].apply(len).value_counts())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "num_quantiles = 5\n",
    "df['freq_quantile'], bins = pd.qcut(df['freq'], num_quantiles, labels=False,  duplicates='drop', retbins=True)\n",
    "\n",
    "num_quantiles = df['freq_quantile'].nunique()\n",
    "samples_per_quantile = num_samples // num_quantiles\n",
    "\n",
    "sampled = []\n",
    "for quantile in range(num_quantiles):\n",
    "    quantile_df = df[df['freq_quantile'] == quantile]\n",
    "    if len(quantile_df) > 0:\n",
    "        sampled.append(quantile_df.sample(min(len(quantile_df), samples_per_quantile), \n",
    "                                          replace=False, random_state=RANDOM_SEED))\n",
    "\n",
    "sampled_df = pd.concat(sampled, ignore_index=False).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "\n",
    "if len(sampled_df) < num_samples:\n",
    "    remaining = num_samples - len(sampled_df)\n",
    "    other_df = df.drop(sampled_df.index, errors='ignore')\n",
    "    print(f\"remaining: {remaining}, other_df: {len(other_df)}\")\n",
    "    additional_samples = other_df.sample(min(len(other_df), remaining), replace=False, random_state=RANDOM_SEED)\n",
    "    # sampled_indices += additional_samples.index.to_list()\n",
    "    sampled_df = pd.concat([sampled_df, additional_samples]).drop_duplicates(subset=['word']).reset_index(drop=True)\n",
    "    print(f\"sampled_df: {len(sampled_df)}\")\n",
    "\n",
    "# Display the sampled DataFrame\n",
    "print(sampled_df[f\"typo_type_{TOKENIZER}\"].value_counts())\n",
    "sampled_df = sampled_df[['word' , f\"typo_tokens_{TOKENIZER}\", f\"splitted_typo_tokens\", f\"typo_type_{TOKENIZER}\", \"same_token_num\", \"same_token_num2\", \"freq\", \"freq_quantile\", \"word_len\", \"typo_word_len\"]]\n",
    "sampled_df\n",
    "\n",
    "\n",
    "sampled_df.to_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_typos_{TOKENIZER}_{LANGUAGE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# KOREAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_jamos(word):\n",
    "    decomposed = h2j(word)  # Decomposes into jamos\n",
    "    return len(decomposed)\n",
    "\n",
    "def split_jamos(word):\n",
    "    return list(h2j(word))  # decompose Hangul syllables to jamos\n",
    "\n",
    "def join_jamos(jamos):\n",
    "    return j2h(''.join(jamos))  # compose jamos back to syllables\n",
    "\n",
    "def random_split_korean(word, MIN_JAMO_LEN):\n",
    "    jamos = list(split_jamos(word))\n",
    "    if len(jamos) <= 1:\n",
    "        return [word]\n",
    "    num_splits = random.randint(1, min(4, len(jamos) - MIN_JAMO_LEN))\n",
    "    split_points = sorted(random.sample(range(1, len(jamos)), num_splits))\n",
    "    jamo_tokens = [jamos[i:j] for i, j in zip([0] + split_points, split_points + [None])]\n",
    "    return [''.join(token) for token in jamo_tokens]\n",
    "\n",
    "# def random_split_korean(word, min_jamo_len):\n",
    "#     if not word or len(word) == 1:\n",
    "#         return [word]\n",
    "\n",
    "#     syllables = list(word)\n",
    "#     jamo_lengths = [count_jamos(syl) for syl in syllables]\n",
    "\n",
    "#     # Accumulate positions ensuring min_jamo_len per segment\n",
    "#     valid_indices = []\n",
    "#     total = 0\n",
    "#     for i in range(1, len(syllables)):\n",
    "#         total += jamo_lengths[i - 1]\n",
    "#         if total >= min_jamo_len:\n",
    "#             valid_indices.append(i)\n",
    "\n",
    "#     if not valid_indices:\n",
    "#         return [word]\n",
    "\n",
    "#     num_splits = random.randint(1, min(4, len(valid_indices)))\n",
    "#     split_points = sorted(random.sample(valid_indices, num_splits))\n",
    "\n",
    "#     segments = [syllables[i:j] for i, j in zip([0] + split_points, split_points + [None])]\n",
    "#     return [''.join(seg) for seg in segments]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"babel_9b\"\n",
    "# TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"Korean\"\n",
    "MIN_JAMO_LEN = 2\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "df['jamo_len'] = df['word'].apply(count_jamos)\n",
    "df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"jamo_len\"]>MIN_JAMO_LEN)].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"babel_9b\"\n",
    "# TOKENIZER = \"gemma_12b\"\n",
    "# TOKENIZER = \"llama_2_7b\"\n",
    "LANGUAGE = \"Korean\"\n",
    "MIN_JAMO_LEN = 2\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/data/{LANGUAGE}_tokenizers_comparison.csv\")\n",
    "\n",
    "# for Korean\n",
    "if LANGUAGE == \"Korean\":\n",
    "    df[\"word_len\"] = df[\"word\"].apply(len)\n",
    "    df['jamo_len'] = df['word'].apply(count_jamos)\n",
    "    df = df[(df[f\"token_num_{TOKENIZER}\"]==1) & (df[\"jamo_len\"]>MIN_JAMO_LEN)].reset_index(drop=True)\n",
    "    df[f\"splitted_tokens_{TOKENIZER}\"] = df[\"word\"].apply(lambda x: random_split_korean(x, MIN_JAMO_LEN))\n",
    "    print(df[f\"splitted_tokens_{TOKENIZER}\"].apply(len).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[f\"splitted_tokens_{TOKENIZER}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from jamo import h2j, j2h, hangul_to_jamo, jamo_to_hangul\n",
    "\n",
    "# Standard Jamo sets\n",
    "CHO = ['ㄱ','ㄲ','ㄴ','ㄷ','ㄸ','ㄹ','ㅁ','ㅂ','ㅃ','ㅅ','ㅆ','ㅇ','ㅈ','ㅉ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "JUN = ['ㅏ','ㅐ','ㅑ','ㅒ','ㅓ','ㅔ','ㅕ','ㅖ','ㅗ','ㅘ','ㅙ','ㅚ','ㅛ','ㅜ','ㅝ','ㅞ','ㅟ','ㅠ','ㅡ','ㅢ','ㅣ']\n",
    "JON = ['','ㄱ','ㄲ','ㄳ','ㄴ','ㄵ','ㄶ','ㄷ','ㄹ','ㄺ','ㄻ','ㄼ','ㄽ','ㄾ','ㄿ','ㅀ','ㅁ','ㅂ','ㅄ','ㅅ','ㅆ','ㅇ','ㅈ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "\n",
    "def decompose_syllable(s):\n",
    "    code = ord(s) - 0xAC00\n",
    "    cho = code // (21 * 28)\n",
    "    jun = (code % (21 * 28)) // 28\n",
    "    jon = code % 28\n",
    "    return cho, jun, jon\n",
    "\n",
    "def compose_syllable(cho, jun, jon):\n",
    "    return chr(0xAC00 + cho * 21 * 28 + jun * 28 + jon)\n",
    "\n",
    "def introduce_korean_syllable_typo(word, typo_type=None):\n",
    "    if typo_type is None:\n",
    "        typo_type = random.choice([\"substitution\", \"deletion\", \"insertion\", \"transposition\"])\n",
    "\n",
    "    chars = list(word)\n",
    "    if not chars:\n",
    "        return word, typo_type\n",
    "\n",
    "    idx = random.randint(0, len(chars) - 1)\n",
    "    c = chars[idx]\n",
    "    try:\n",
    "        cho, jun, jon = decompose_syllable(c)\n",
    "    except:\n",
    "        return word, typo_type  # Skip non-Hangul\n",
    "\n",
    "    if typo_type == \"substitution\":\n",
    "        part = random.choice(['cho', 'jun', 'jon'])\n",
    "        if part == 'cho':\n",
    "            cho = random.choice([i for i in range(len(CHO)) if i != cho])\n",
    "        elif part == 'jun':\n",
    "            jun = random.choice([i for i in range(len(JUN)) if i != jun])\n",
    "        elif part == 'jon':\n",
    "            jon = random.choice([i for i in range(len(JON)) if i != jon])\n",
    "    elif typo_type == \"deletion\":\n",
    "        part = random.choice(['cho', 'jun', 'jon'])\n",
    "        if part == 'jon':\n",
    "            jon = 0  # Remove final\n",
    "    elif typo_type == \"insertion\":\n",
    "        if jon == 0:\n",
    "            jon = random.randint(1, len(JON) - 1)  # Add a final\n",
    "    elif typo_type == \"transposition\":\n",
    "        # Only meaningful if jon exists — swap cho and jon\n",
    "        if jon != 0:\n",
    "            cho, jon = jon % len(CHO), cho % len(JON)\n",
    "\n",
    "    chars[idx] = compose_syllable(cho, jun, jon)\n",
    "    return ''.join(chars), typo_type\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
