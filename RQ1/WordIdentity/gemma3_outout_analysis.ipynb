{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Frequent Token Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from logitlens import LogitLens\n",
    "\n",
    "# model_name = \"Tower-Babel/Babel-9B-Chat\"\n",
    "model_name = \"google/gemma-3-12b-it\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "logit_lens = LogitLens(\"English\", model_name)\n",
    "model_name_short = model_name.split(\"/\")[-1]\n",
    "\n",
    "df = pd.read_csv(f\"/home/hyujang/multilingual-inner-lexicon/output/RQ1/WordIdentity/single_token_simple_split_{model_name_short}_English_v3.csv\")\n",
    "\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        # Check if at least one value looks like a list\n",
    "        if df[col].apply(lambda x: isinstance(x, str) and x.strip().startswith(\"[\")).any():\n",
    "            df[col] = df[col].apply(literal_eval)\n",
    "    except (ValueError, SyntaxError):\n",
    "        continue  # Skip columns that don't parse correctly\n",
    "\n",
    "# Get all token ID and string columns\n",
    "id_cols = [col for col in df.columns if col.startswith(\"layer_\") and col.endswith(\"_top_token_id\")]\n",
    "str_cols = [col for col in df.columns if col.startswith(\"layer_\") and col.endswith(\"_top_token_str\")]\n",
    "\n",
    "# Make sure they're in the same order\n",
    "id_cols.sort()\n",
    "str_cols.sort()\n",
    "\n",
    "# Initialize the mapping dictionary\n",
    "token_id_to_str = {}\n",
    "\n",
    "# Iterate through layers\n",
    "for id_col, str_col in zip(id_cols, str_cols):\n",
    "    for id_list, str_list in zip(df[id_col], df[str_col]):\n",
    "        for token_id, token_str in zip(id_list, str_list):\n",
    "            if token_id not in token_id_to_str:\n",
    "                token_id_to_str[token_id] = token_str\n",
    "\n",
    "\n",
    "token_str_cols = [col for col in df.columns if col.startswith(\"layer_\") and col.endswith(\"_top_token_id\")]\n",
    "\n",
    "# Flatten all top-3 token outputs across layers and words\n",
    "all_tokens = []\n",
    "\n",
    "for col in token_str_cols:\n",
    "    for token_list_str in df[col]:\n",
    "        try:\n",
    "            all_tokens.extend(token_list_str)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "top_20_tokens = [tok for tok, _ in Counter(all_tokens).most_common(20)]\n",
    "\n",
    "# Step 2: Count frequency of each top token per layer\n",
    "layer_token_freq = pd.DataFrame(index=top_20_tokens, columns=token_str_cols)\n",
    "\n",
    "for col in token_str_cols:\n",
    "    token_counts = Counter()\n",
    "    for token_list_str in df[col]:\n",
    "        try:\n",
    "            token_counts.update(token_list_str)\n",
    "        except:\n",
    "            continue\n",
    "    for tok in top_20_tokens:\n",
    "        layer_token_freq.at[tok, col] = token_counts.get(tok, 0)\n",
    "\n",
    "# Convert to numeric\n",
    "layer_token_freq = layer_token_freq.fillna(0).astype(int)\n",
    "\n",
    "# Rename index using token strings\n",
    "layer_token_freq.index = layer_token_freq.index.astype(int)\n",
    "token_id_to_str = {int(k): logit_lens.tokenizer.convert_ids_to_tokens(k).encode('utf-8').decode('utf-8') for k, v in token_id_to_str.items()}\n",
    "layer_token_freq.rename(index=token_id_to_str, inplace=True)\n",
    "\n",
    "# Optional: clean column names (layer_1 -> 1, etc.)\n",
    "# layer_token_freq.columns = [int(col.split(\"_\")[1]) for col in layer_token_freq.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Optional: ensure layers are ordered numerically even if theyâ€™re strings like \"L0\", \"Layer 1\", etc.\n",
    "def _layer_key(s):\n",
    "    m = re.search(r'\\d+', str(s))\n",
    "    return int(m.group()) if m else 0\n",
    "\n",
    "layer_order = sorted(layer_token_freq.columns, key=_layer_key)\n",
    "token_order  = list(layer_token_freq.index)  # keep your current token order (top-to-bottom)\n",
    "\n",
    "# --- Build the heatmap\n",
    "fig = px.imshow(\n",
    "    layer_token_freq.loc[token_order, layer_order].values,\n",
    "    x=layer_order,                # Layers\n",
    "    y=token_order,                # Tokens\n",
    "    text_auto=True,               # show counts in cells\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"YlOrBr\",\n",
    ")\n",
    "\n",
    "# --- Trace-level tweaks: grid-like borders & nicer hover\n",
    "fig.update_traces(\n",
    "    xgap=1, ygap=1,  # thin gaps read like gridlines\n",
    "    hovertemplate=\"Layer: %{x}<br>Token: %{y}<br>Count: %{z}<extra></extra>\",\n",
    "    textfont=dict(size=12)  # larger in-cell labels\n",
    ")\n",
    "\n",
    "# --- Layout: fonts, titles, axes, margins\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    title=f\"Top 20 Most Frequent Predicted Tokens by Layer ({model_name_short}, English)\",\n",
    "    title_font=dict(size=20),\n",
    "    width=1200,\n",
    "    height=500,\n",
    "    margin=dict(l=80, r=80, t=80, b=60),\n",
    ")\n",
    "\n",
    "# Axis titles + fonts (like your Matplotlib fontsize choices)\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Layer\",\n",
    "    title_font=dict(size=18),\n",
    "    tickfont=dict(size=14),\n",
    "    tickangle=0,\n",
    "    showgrid=False\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Predicted Token\",\n",
    "    title_font=dict(size=18),\n",
    "    tickfont=dict(size=14),\n",
    "    autorange=\"reversed\",   # top token stays at the top\n",
    "    showgrid=False\n",
    ")\n",
    "\n",
    "# Colorbar: make it clear & compact\n",
    "fig.update_coloraxes(\n",
    "    colorbar_title=\"Count\",\n",
    "    colorbar_thickness=16,\n",
    "    colorbar_len=0.8,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Diagnosing Rogue Diemnsion via Anisotropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from logitlens import LogitLens\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_colors = {\n",
    "    \"Babel-9B-Chat\": \"#66c2a5\",\n",
    "    \"gemma-3-12b-it\": \"#fc8d62\",\n",
    "    \"Llama-2-7b-chat-hf\": \"#e78ac3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the models and language\n",
    "models = [\n",
    "    \"google/gemma-3-12b-it\",\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"Tower-Babel/Babel-9B-Chat\"\n",
    "]\n",
    "language = \"English\"\n",
    "\n",
    "# Function to measure anisotropy\n",
    "def measure_anisotropy(hidden_matrix: torch.Tensor, sample_size=1000):\n",
    "    normed = F.normalize(hidden_matrix, dim=-1)\n",
    "    N = normed.size(0)\n",
    "    idx1 = torch.randint(0, N, (sample_size,))\n",
    "    idx2 = torch.randint(0, N, (sample_size,))\n",
    "    sims = (normed[idx1] * normed[idx2]).sum(dim=1)\n",
    "    return sims.mean().item()\n",
    "\n",
    "# Initialize a dictionary to store anisotropy values for each model\n",
    "anisotropy_data = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    logit_lens = LogitLens(language, model_name)\n",
    "    model_short_name = model_name.split(\"/\")[-1]\n",
    "    \n",
    "    # Load the data for the model\n",
    "    path = f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{model_short_name}_{language}_v2.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Run the logit lens and get hidden states\n",
    "    hidden_states_results = logit_lens.run_logit_lens(df, split_type=\"simple_split\", return_hidden_states=True)\n",
    "    all_hiddens_dict = {result[\"word\"]: result[\"all_hidden_states\"] for result in hidden_states_results}\n",
    "    \n",
    "    num_layers = len(next(iter(all_hiddens_dict.values())))  # Get the number of layers\n",
    "    anisotropy_per_layer = []\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        # Collect hidden states for the current layer across all words\n",
    "        layer_hiddens = torch.stack([h[layer_idx] for h in all_hiddens_dict.values()])\n",
    "        anisotropy = measure_anisotropy(layer_hiddens)\n",
    "        anisotropy_per_layer.append(anisotropy)\n",
    "    anisotropy_data[model_short_name] = anisotropy_per_layer\n",
    "    \n",
    "# Plot anisotropy for all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for model_short_name, anisotropy_per_layer in anisotropy_data.items():\n",
    "    plt.plot(\n",
    "        range(1, len(anisotropy_per_layer) + 1),\n",
    "        anisotropy_per_layer,\n",
    "        marker='o',\n",
    "        label=model_short_name,\n",
    "        color=model_colors[model_short_name]\n",
    "    )\n",
    "\n",
    "# Customize the plot\n",
    "# plt.title(\"Anisotropy of Hidden States per Layer (English)\", fontsize=20)\n",
    "plt.xlabel(\"Layer\", fontsize=18)\n",
    "plt.ylabel(\"Anisotropy\", fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "plt.legend(handles, labels, title=\"Model\", fontsize=14, loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the models and language\n",
    "models = [\n",
    "    \"google/gemma-3-12b-it\",\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"Tower-Babel/Babel-9B-Chat\"\n",
    "]\n",
    "language = \"English\"\n",
    "\n",
    "# Function to compute rogue dimension strength\n",
    "def compute_rogue_dimension_strength(layer_hiddens):\n",
    "    \"\"\"\n",
    "    layer_hiddens: torch.Tensor of shape [num_words, hidden_dim]\n",
    "    returns: float, rogue dimension strength\n",
    "    \"\"\"\n",
    "    layer_hiddens = layer_hiddens.float()\n",
    "    # Ensure at least 2D\n",
    "    if layer_hiddens.ndim == 1:\n",
    "        layer_hiddens = layer_hiddens.unsqueeze(0)\n",
    "    # Center hidden states\n",
    "    hidden_centered = layer_hiddens - layer_hiddens.mean(dim=0, keepdim=True)\n",
    "    # Covariance matrix\n",
    "    cov = (hidden_centered.T @ hidden_centered) / (hidden_centered.shape[0] - 1)\n",
    "    # Eigenvalues\n",
    "    eigvals, _ = torch.linalg.eigh(cov)\n",
    "    eigvals = eigvals.flip(dims=[0])\n",
    "    # Rogue dimension strength\n",
    "    ratio = eigvals[0] / eigvals.sum()\n",
    "    return ratio.item()\n",
    "\n",
    "# Initialize a dictionary to store rogue dimension strengths for each model\n",
    "rogue_strength_data = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    logit_lens = LogitLens(language, model_name)\n",
    "    model_short_name = model_name.split(\"/\")[-1]\n",
    "    \n",
    "    # Load the data for the model\n",
    "    path = f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{model_short_name}_{language}_v2.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Run the logit lens and get hidden states\n",
    "    hidden_states_results = logit_lens.run_logit_lens(df, split_type=\"simple_split\", return_hidden_states=True)\n",
    "    all_hiddens_dict = {result[\"word\"]: result[\"all_hidden_states\"] for result in hidden_states_results}\n",
    "    \n",
    "    num_layers = len(next(iter(all_hiddens_dict.values())))  # Get the number of layers\n",
    "    rogue_strengths = []\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        # Collect hidden states for the current layer across all words\n",
    "        layer_hiddens = torch.stack([h[layer_idx] for h in all_hiddens_dict.values()])\n",
    "        rogue_strength = compute_rogue_dimension_strength(layer_hiddens)\n",
    "        rogue_strengths.append(rogue_strength)\n",
    "    rogue_strength_data[model_short_name] = rogue_strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rogue dimension strength for all models\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model_short_name, rogue_strengths in rogue_strength_data.items():\n",
    "    plt.plot(\n",
    "        range(1, len(rogue_strengths) + 1),\n",
    "        rogue_strengths,\n",
    "        marker='o',\n",
    "        label=model_short_name,\n",
    "        color=model_colors[model_short_name]\n",
    "    )\n",
    "\n",
    "# Customize the plot\n",
    "# plt.title(\"Rogue Dimension Strength Across Layers (English)\", fontsize=16)\n",
    "plt.xlabel(\"Layer\", fontsize=18)\n",
    "plt.ylabel(\"Rogue Dimension Strength\", fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "plt.legend(handles, labels, title=\"Model\", fontsize=14, loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# MODEL_NAME = \"Tower-Babel/Babel-9B-Chat\"\n",
    "\n",
    "LANGUAGE = \"English\"\n",
    "logit_lens = LogitLens(LANGUAGE, MODEL_NAME)\n",
    "MODEL_NAME = MODEL_NAME.split(\"/\")[-1]\n",
    "path1 = f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/WordIdentity/single_token_splitted_{MODEL_NAME}_{LANGUAGE}_v2.csv\"\n",
    "df = pd.read_csv(path1)\n",
    "all_hiddens, words = logit_lens.run_logit_lens(df, type=\"simple_split\", return_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def measure_anisotropy(hidden_matrix: torch.Tensor, sample_size=1000):\n",
    "    normed = F.normalize(hidden_matrix, dim=-1)\n",
    "    N = normed.size(0)\n",
    "    idx1 = torch.randint(0, N, (sample_size,))\n",
    "    idx2 = torch.randint(0, N, (sample_size,))\n",
    "    sims = (normed[idx1] * normed[idx2]).sum(dim=1)\n",
    "    return sims.mean().item()\n",
    "\n",
    "anisotropy_per_layer = [measure_anisotropy(h) for h in all_hiddens]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(anisotropy_per_layer)+1), anisotropy_per_layer)\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Average Cosine Similarity (Anisotropy)\")\n",
    "plt.title(\"Anisotropy of Hidden States per Layer\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_layer = []\n",
    "top_tokens_per_layer = []\n",
    "\n",
    "for layer_idx, hidden in enumerate(all_hiddens):\n",
    "    # hidden: [700, 3840]\n",
    "    # Logits: [700, vocab_size]\n",
    "    logits = torch.matmul(hidden.to(logit_lens.device), logit_lens.embedding_matrix.T)  # matmul hidden * embedding.T\n",
    "    logits_per_layer.append(logits.cpu())\n",
    "\n",
    "    # Top token ids per example in batch for this layer\n",
    "    top_tokens = torch.argmax(logits, dim=-1).cpu().numpy()  # shape: (700,)\n",
    "    top_tokens_per_layer.append(top_tokens)\n",
    "\n",
    "def entropy(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return (-probs * probs.log()).sum(dim=-1).mean().item()  # mean entropy over batch\n",
    "\n",
    "entropy_per_layer = [entropy(logits) for logits in logits_per_layer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "top_tokens_per_layer = np.array(top_tokens_per_layer)  # shape: (48, 700)\n",
    "\n",
    "# For each word, check if predicted token is the same across all layers\n",
    "same_token_across_layers = np.all(top_tokens_per_layer == top_tokens_per_layer[0, :], axis=0)\n",
    "batch_size = all_hiddens[0].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue_strengths = []\n",
    "\n",
    "for layer_idx, hidden in enumerate(all_hiddens):\n",
    "    # Convert to float32 if needed\n",
    "    hidden = hidden.float()  # avoid bfloat16 issue\n",
    "    \n",
    "    hidden_centered = hidden - hidden.mean(dim=0, keepdim=True)  # [700, 3840]\n",
    "    # The covariance matrix captures how each dimension of the hidden vectors varies and co-varies with others across tokens\n",
    "    cov = (hidden_centered.T @ hidden_centered) / (hidden_centered.shape[0] - 1)  # [3840, 3840] \n",
    "    \n",
    "    eigvals, eigvecs = torch.linalg.eigh(cov)\n",
    "    eigvals = eigvals.flip(dims=[0])  # descending order\n",
    "    \n",
    "    ratio = eigvals[0] / eigvals.sum()\n",
    "    rogue_strengths.append(ratio.item())\n",
    "\n",
    "# Plot rogue_strengths with matplotlib (using Python floats, so no np needed)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(rogue_strengths)+1), rogue_strengths, marker='o')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Rogue Dimension Strength')\n",
    "plt.title('Rogue Dimension Strength Across Layers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
