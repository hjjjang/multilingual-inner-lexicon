{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "folder_path = \"/home/hyujang/multilingual-inner-lexicon/output/RQ1/WordIdentity/layer_hidden_states2\"\n",
    "dataset_paths = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "dataset_paths = [path for path in dataset_paths if \"multi_token\" in path]\n",
    "dataset_paths.sort()\n",
    "\n",
    "model_colors = {\n",
    "    \"Babel-9B-Chat\": \"#66c2a5\",\n",
    "    \"gemma-3-12b-it\": \"#fc8d62\",\n",
    "    \"Llama-2-7b-chat-hf\": \"#e78ac3\"\n",
    "}\n",
    "\n",
    "# Define colors for each language\n",
    "language_colors = {\n",
    "    \"English\": \"#1f77b4\",\n",
    "    \"Korean\": \"#ff7f0e\",\n",
    "    \"German\": \"#2ca02c\"\n",
    "}\n",
    "\n",
    "# Define line styles for each model\n",
    "model_styles = {\n",
    "    \"Babel-9B-Chat\": \"-\",\n",
    "    \"gemma-3-12b-it\": \"--\",\n",
    "    \"Llama-2-7b-chat-hf\": \":\"\n",
    "}\n",
    "\n",
    "language_styles = {\n",
    "    \"English\": \"-\",\n",
    "    \"Korean\": \"--\",\n",
    "    \"German\": \":\"\n",
    "}\n",
    "\n",
    "version_colors = {\n",
    "    \"\": \"#1f77b4\",  # Blue\n",
    "    \"v2\": \"#ff7f0e\",  # Orange\n",
    "    \"v3\": \"#165a16\",  # Green\n",
    "    \"v4\": \"#d62728\"   # Red\n",
    "}\n",
    "\n",
    "version_linestyles = {\n",
    "    \"\": \":\",\n",
    "    \"v2\": \"--\",\n",
    "    \"v3\": \"-.\",\n",
    "    \"v4\": \":\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for path in dataset_paths:\n",
    "    # Extract model and language from the file name\n",
    "    filename = os.path.basename(path)\n",
    "    model_short = filename.split(\"_\")[2]\n",
    "    lang = filename.split(\"_\")[3].split(\".\")[0]\n",
    "    version = filename.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(path)\n",
    "    df['retrieved'] = df.apply(lambda row: str(row['word']) in str(row['patchscope_result']), axis=1)\n",
    "    retrieval_rate = df.groupby('layer')['retrieved'].mean()\n",
    "\n",
    "    # Plot\n",
    "    plt.plot(\n",
    "        retrieval_rate.index,\n",
    "        retrieval_rate.values,\n",
    "        marker='o',\n",
    "        # label=f\"{model_short}-{lang}\",\n",
    "        label = filename,\n",
    "        color=language_colors.get(lang, \"black\"),  # Use the color for the language\n",
    "        # color=version_colors.get(version, \"black\"),  # Use the color for the version\n",
    "        # color = model_colors.get(model_short, \"black\"),  # Use the color for the model\n",
    "        # color=model_styles.get(model_short, \"black\"),  # Use the color for the language\n",
    "        linestyle=model_styles.get(model_short, \"dashdot\"),  # Use the line style for the model\n",
    "        # linestyle=version_linestyles.get(version, \"-\")  # Use the line style for the version\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Retrieval Rate\")\n",
    "plt.title(\"PatchScope Retrieval Rate per Layer (All Models & Languages)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Group dataset paths by language\n",
    "language_datasets = {\"English\": [], \"German\": [], \"Korean\": []}\n",
    "for path in dataset_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    lang = filename.split(\"_\")[3].split(\".\")[0]\n",
    "    language_datasets[lang].append(path)\n",
    "\n",
    "# Create subplots for each language\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
    "\n",
    "for ax, (lang, paths) in zip(axes, language_datasets.items()):\n",
    "    for path in paths:\n",
    "        filename = os.path.basename(path)\n",
    "        model_short = filename.split(\"_\")[2]\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(path)\n",
    "        df['retrieved'] = df.apply(lambda row: str(row['word']) in str(row['patchscope_result']), axis=1)\n",
    "        retrieval_rate = df.groupby('layer')['retrieved'].mean()\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(\n",
    "            retrieval_rate.index,\n",
    "            retrieval_rate.values,\n",
    "            marker='o',\n",
    "            label=model_short,\n",
    "            color=model_colors.get(model_short, \"black\"),\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"{lang} Retrieval Rate\")\n",
    "    ax.set_xlabel(\"Layer\")\n",
    "    ax.set_ylabel(\"Word Retrieval Rate\")\n",
    "    ax.legend(title=\"Model\")\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group dataset paths by model\n",
    "model_datasets = {}\n",
    "for path in dataset_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    model_short = filename.split(\"_\")[2]\n",
    "    if model_short not in model_datasets:\n",
    "        model_datasets[model_short] = []\n",
    "    model_datasets[model_short].append(path)\n",
    "\n",
    "desired_order = [\"Babel-9B-Chat\", \"gemma-3-12b-it\", \"Llama-2-7b-chat-hf\"]\n",
    "\n",
    "# Sort the model_datasets dictionary based on the desired order\n",
    "model_datasets = {model: model_datasets[model] for model in desired_order if model in model_datasets}\n",
    "\n",
    "# Create subplots for each model\n",
    "fig, axes = plt.subplots(len(model_datasets), 1, figsize=(10, 5 * len(model_datasets)), sharex=False)\n",
    "\n",
    "for ax, (model, paths) in zip(axes, model_datasets.items()):\n",
    "    all_layers = []  # To track all layers for setting x-axis limits\n",
    "    for path in paths:\n",
    "        filename = os.path.basename(path)\n",
    "        lang = filename.split(\"_\")[3].split(\".\")[0]\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(path)\n",
    "        df['retrieved'] = df.apply(lambda row: str(row['word']) in str(row['patchscope_result']), axis=1)\n",
    "        retrieval_rate = df.groupby('layer')['retrieved'].mean()\n",
    "\n",
    "        # Track layers for x-axis limits\n",
    "        all_layers.extend(retrieval_rate.index)\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(\n",
    "            retrieval_rate.index,\n",
    "            retrieval_rate.values,\n",
    "            marker='o',\n",
    "            label=lang,\n",
    "            color=language_colors.get(lang, \"black\")\n",
    "        )\n",
    "\n",
    "    # Set x-axis limits dynamically based on layers\n",
    "    ax.set_xlim([min(all_layers)-1, max(all_layers)+1])\n",
    "\n",
    "    ax.set_title(f\"{model} Retrieval Rate\")\n",
    "    ax.set_xlabel(\"Layer\")\n",
    "    ax.set_ylabel(\"Word Retrieval Rate\")\n",
    "    ax.legend(title=\"Language\")\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from scipy.stats import entropy\n",
    "\n",
    "THRESHOLD = 10\n",
    "TOP_K = 5\n",
    "\n",
    "# Function to calculate repetition and degeneracy metrics\n",
    "def calculate_repetition_degeneracy(df):\n",
    "    repetition = df['word'].duplicated().mean()  # Fraction of repeated words\n",
    "    degeneracy = df['patchscope_result'].apply(lambda x: len(set(str(x).split()))).mean()  # Average unique tokens in results\n",
    "    return repetition, degeneracy\n",
    "\n",
    "# Create a table for each dataset\n",
    "results = []\n",
    "layer_results = []  # For layer-wise analysis\n",
    "for path in dataset_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    model_short = filename.split(\"_\")[2]\n",
    "    lang = filename.split(\"_\")[3].split(\".\")[0]\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Calculate repetition and degeneracy metrics\n",
    "    repetition, degeneracy = calculate_repetition_degeneracy(df)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    value_counts = df['patchscope_result'].value_counts()\n",
    "    total = len(df)\n",
    "    unique = value_counts.count()\n",
    "    duplicate_ratio = 1 - unique / total\n",
    "    count_over_threshold = (value_counts > THRESHOLD).sum()\n",
    "    top_k_total = value_counts.head(TOP_K).sum()\n",
    "    top_k_ratio = top_k_total / total\n",
    "    probabilities = value_counts / value_counts.sum()\n",
    "    ent = entropy(probabilities, base=2)\n",
    "\n",
    "    # Append overall results\n",
    "    results.append({\n",
    "        'Model': model_short,\n",
    "        'Language': lang,\n",
    "        'Repetition': repetition,\n",
    "        'Degeneracy': degeneracy,\n",
    "        'Total Outputs': total,\n",
    "        'Unique Outputs': unique,\n",
    "        'Duplicate Ratio': f\"{duplicate_ratio:.2%}\",\n",
    "        f\"# Outputs >{THRESHOLD}\": count_over_threshold,\n",
    "        f\"Top {TOP_K} %\": f\"{top_k_ratio:.2%}\",\n",
    "        'Entropy (bits)': round(ent, 2)\n",
    "    })\n",
    "\n",
    "    # Layer-wise analysis\n",
    "    for layer, layer_df in df.groupby('layer'):\n",
    "        layer_value_counts = layer_df['patchscope_result'].value_counts()\n",
    "        layer_total = len(layer_df)\n",
    "        layer_unique = layer_value_counts.count()\n",
    "        layer_duplicate_ratio = 1 - layer_unique / layer_total\n",
    "        layer_count_over_threshold = (layer_value_counts > THRESHOLD).sum()\n",
    "        layer_top_k_total = layer_value_counts.head(TOP_K).sum()\n",
    "        layer_top_k_ratio = layer_top_k_total / layer_total\n",
    "        layer_probabilities = layer_value_counts / layer_value_counts.sum()\n",
    "        layer_ent = entropy(layer_probabilities, base=2)\n",
    "        most_frequent_output = layer_value_counts.idxmax()  # Get the most frequent output\n",
    "        top_3_outputs = layer_value_counts.head(3).to_dict()  # Get the top 3 most frequent outputs as a dictionary\n",
    "\n",
    "        layer_results.append({\n",
    "            'Model': model_short,\n",
    "            'Language': lang,\n",
    "            'Layer': layer+1,\n",
    "            'Total Outputs': layer_total,\n",
    "            'Unique Outputs': layer_unique,\n",
    "            'Duplicate Ratio': f\"{layer_duplicate_ratio:.2%}\",\n",
    "            f\"# Outputs >{THRESHOLD}\": layer_count_over_threshold,\n",
    "            f\"Top {TOP_K} %\": f\"{layer_top_k_ratio:.2%}\",\n",
    "            'Entropy (bits)': round(layer_ent, 2),\n",
    "            'Most Frequent Output': most_frequent_output,\n",
    "            'Top 3 Outputs': top_3_outputs\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrames and display\n",
    "results_df = pd.DataFrame(results)\n",
    "layer_results_df = pd.DataFrame(layer_results)\n",
    "print(\"Overall Results:\")\n",
    "print(results_df)\n",
    "print(\"\\nLayer-wise Results:\")\n",
    "print(layer_results_df)\n",
    "\n",
    "# Optionally save the tables to CSV files\n",
    "results_df.to_csv(\"repetition_degeneracy_table.csv\", index=False)\n",
    "layer_results_df.to_csv(\"layer_wise_analysis_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for visualization\n",
    "layerwise_entropy_data = layer_results_df[['Model', 'Language', 'Layer', 'Entropy (bits)']]\n",
    "\n",
    "# Get unique models for subplots\n",
    "unique_models = layerwise_entropy_data['Model'].unique()\n",
    "num_models = len(unique_models)\n",
    "\n",
    "# Create subplots for each model\n",
    "fig, axes = plt.subplots(num_models, 1, figsize=(12, 5 * num_models), sharex=True)\n",
    "\n",
    "for ax, model in zip(axes, unique_models):\n",
    "    model_data = layerwise_entropy_data[layerwise_entropy_data['Model'] == model]\n",
    "    for language, group in model_data.groupby('Language'):\n",
    "        ax.plot(\n",
    "            group['Layer'],\n",
    "            group['Entropy (bits)'],\n",
    "            marker='o',\n",
    "            label=f\"{language}\",\n",
    "            color=language_colors.get(language, \"black\"),\n",
    "            linestyle=languague_styles.get(language, \"-\")\n",
    "        )\n",
    "    ax.set_title(f\"{model} - Layer-wise Entropy\", fontsize=14)\n",
    "    ax.set_xlabel(\"Layer\", fontsize=12)\n",
    "    ax.set_ylabel(\"Entropy (bits)\", fontsize=12)\n",
    "    ax.legend(title=\"Language\", fontsize=10, title_fontsize=12)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import os\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Function to generate heatmaps with 3 subplots per model (one for each language)\n",
    "def generate_model_language_heatmaps(dataset_paths):\n",
    "    # Group dataset paths by model\n",
    "    model_datasets = {}\n",
    "    for path in dataset_paths:\n",
    "        filename = os.path.basename(path)\n",
    "        model_short = filename.split(\"_\")[2]\n",
    "        if model_short not in model_datasets:\n",
    "            model_datasets[model_short] = []\n",
    "        model_datasets[model_short].append(path)\n",
    "\n",
    "    # Iterate over each model\n",
    "    for model, paths in model_datasets.items():\n",
    "        # Create a subplot with 3 rows (one for each language)\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=1,\n",
    "            subplot_titles=[\"English\", \"German\", \"Korean\"],\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.07  # Reduce space between subplots\n",
    "        )\n",
    "\n",
    "        # Group paths by language\n",
    "        language_datasets = {\"English\": [], \"German\": [], \"Korean\": []}\n",
    "        for path in paths:\n",
    "            filename = os.path.basename(path)\n",
    "            lang = filename.split(\"_\")[3].split(\".\")[0]\n",
    "            if lang in language_datasets:\n",
    "                language_datasets[lang].append(path)\n",
    "\n",
    "        # Generate heatmaps for each language\n",
    "        for i, (lang, lang_paths) in enumerate(language_datasets.items(), start=1):\n",
    "            # Combine data for the language\n",
    "            combined_data = pd.DataFrame()\n",
    "            for path in lang_paths:\n",
    "                df = pd.read_csv(path)\n",
    "                combined_data = pd.concat([combined_data, df])\n",
    "\n",
    "            # Get the top 5 most frequent outputs globally\n",
    "            global_top5 = combined_data['patchscope_result'].value_counts().head(5).index\n",
    "\n",
    "            # Create a dictionary to store counts for each layer\n",
    "            top5_by_layer = {}\n",
    "            for layer, layer_df in combined_data.groupby('layer'):\n",
    "                filtered = layer_df[layer_df['patchscope_result'].isin(global_top5)]\n",
    "                counts = filtered['patchscope_result'].value_counts()\n",
    "                top5_by_layer[layer] = counts\n",
    "\n",
    "            # Convert the dictionary to a DataFrame and fill missing values with 0\n",
    "            heatmap_data = pd.DataFrame(top5_by_layer).fillna(0)\n",
    "\n",
    "            # Add heatmap to the subplot\n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=heatmap_data.values,\n",
    "                    x=heatmap_data.columns,  # Layers\n",
    "                    y=heatmap_data.index,    # Tokens\n",
    "                    colorscale=\"YlOrBr\",\n",
    "                    # colorbar=dict(title=\"Frequency\"),\n",
    "                    coloraxis=\"coloraxis\",\n",
    "                    text=heatmap_data.values,  # Optional: Remove this line if not needed\n",
    "                    texttemplate=\"%{z}\"  # Display only the frequency value\n",
    "                ),\n",
    "                row=i, col=1\n",
    "            )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f\"Top 5 Most Frequent Output by Layer for {model}\",\n",
    "            xaxis_title=None,\n",
    "            yaxis_title=None,\n",
    "            height=500,  # Adjust height for 3 subplots\n",
    "            width=1200,\n",
    "            coloraxis=dict(\n",
    "                colorscale=\"YlOrBr\",                 # <--- put colorscale here\n",
    "                colorbar=dict(title=\"Frequency\")     # <--- single shared colorbar\n",
    "            )\n",
    "        )\n",
    "        fig.update_xaxes(title_text=\"Layer\", row=3, col=1)\n",
    "        fig.update_yaxes(title_text=\"Generated Output\", row=2, col=1)\n",
    "\n",
    "        # Show the figure\n",
    "        fig.show()\n",
    "\n",
    "# Call the function\n",
    "generate_model_language_heatmaps(dataset_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, Any, Optional, Tuple, Union, List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Gemma3ForConditionalGeneration, Qwen2ForCausalLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Base directory for the project\n",
    "BASE_DIR = \"/home/hyujang/multilingual-inner-lexicon\"\n",
    "\n",
    "def load_config(config_path: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"Load main configuration file.\"\"\"\n",
    "    if config_path is None:\n",
    "        config_path = os.path.join(BASE_DIR, \"RQ1/config.json\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_user_config(user_config_path: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"Load user configuration file with tokens.\"\"\"\n",
    "    if user_config_path is None:\n",
    "        user_config_path = os.path.join(BASE_DIR, \"user_config.json\")\n",
    "    \n",
    "    try:\n",
    "        with open(user_config_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: user_config.json not found\")\n",
    "        return {\"huggingface_token\": {}}\n",
    "\n",
    "def get_token_value(tokenizer_name: str, config: Dict[str, Any] = None, user_config: Dict[str, Any] = None) -> Optional[str]:\n",
    "    \"\"\"Get token value for a specific tokenizer.\"\"\"\n",
    "    if config is None:\n",
    "        config = load_config()\n",
    "    if user_config is None:\n",
    "        user_config = load_user_config()\n",
    "    \n",
    "    token_key = config.get(\"tokenizers\", {}).get(tokenizer_name)\n",
    "    if token_key:\n",
    "        return user_config.get(\"huggingface_token\", {}).get(token_key)\n",
    "    return None\n",
    "\n",
    "def setup_tokenizer(tokenizer_name: str, use_fast: bool = True) -> AutoTokenizer:\n",
    "    \"\"\"Setup tokenizer with proper token handling and special configurations.\"\"\"\n",
    "    token_value = get_token_value(tokenizer_name)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name, \n",
    "        use_fast=use_fast, \n",
    "        token=token_value\n",
    "    )\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "model_full_name_map = {\n",
    "    \"Llama-2-7b-chat-hf\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"Babel-9B-Chat\": \"Tower-Babel/Babel-9B-Chat\",\n",
    "    \"gemma-3-12b-it\": \"google/gemma-3-12b-it\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "# from utils import setup_tokenizer  # Changed to absolute import to fix ImportError\n",
    "\n",
    "# Function to check if the last token of the target word was successfully extracted\n",
    "def check_last_token_extraction(row, tokenizer):\n",
    "    # Tokenize the target word\n",
    "    target_tokens = tokenizer.tokenize(row['word'])\n",
    "    if not target_tokens:\n",
    "        return False  # If no tokens are generated, return False\n",
    "    \n",
    "    target_last_token = target_tokens[-1]  # Get the last token of the target word\n",
    "    output_tokens = tokenizer.tokenize(row['patchscope_result'])  # Tokenize the output\n",
    "    return target_last_token in output_tokens  # Check if the last token is in the output\n",
    "\n",
    "# Analyze last token extraction for each dataset\n",
    "last_token_results = []\n",
    "for path in dataset_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    model_short = filename.split(\"_\")[2]\n",
    "    lang = filename.split(\"_\")[3].split(\".\")[0]\n",
    "\n",
    "    # Setup tokenizer\n",
    "    tokenizer = setup_tokenizer(model_full_name_map[model_short])\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Add a column to check last token extraction\n",
    "    df['last_token_extracted'] = df.apply(check_last_token_extraction, axis=1, tokenizer=tokenizer)\n",
    "\n",
    "    # Calculate the success rate of last token extraction per layer\n",
    "    last_token_rate = df.groupby('layer')['last_token_extracted'].mean()\n",
    "\n",
    "    # Store results for visualization\n",
    "    last_token_results.append({\n",
    "        'Model': model_short,\n",
    "        'Language': lang,\n",
    "        'Layer': last_token_rate.index.tolist(),\n",
    "        'Last Token Extraction Rate': last_token_rate.values.tolist()\n",
    "    })\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in last_token_results:\n",
    "    layers = result['Layer']\n",
    "    rates = result['Last Token Extraction Rate']\n",
    "    label = f\"{result['Model']} - {result['Language']}\"\n",
    "    plt.plot(layers, rates, marker='o', label=label)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Last Token Extraction Rate\")\n",
    "plt.title(\"Last Token Extraction Success Rate Across Layers\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a combined DataFrame for easier plotting\n",
    "all_results = []\n",
    "for path in dataset_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    model_short = filename.split(\"_\")[2]\n",
    "    lang = filename.split(\"_\")[3].split(\".\")[0]\n",
    "\n",
    "    # Setup tokenizer\n",
    "    tokenizer = setup_tokenizer(model_full_name_map[model_short])\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Add a column to check last token extraction\n",
    "    df['last_token_extracted'] = df.apply(check_last_token_extraction, axis=1, tokenizer=tokenizer)\n",
    "\n",
    "    # Calculate success rate per layer\n",
    "    last_token_rate = df.groupby('layer')['last_token_extracted'].mean().reset_index()\n",
    "    last_token_rate['Model'] = model_short\n",
    "    last_token_rate['Language'] = lang\n",
    "\n",
    "    all_results.append(last_token_rate)\n",
    "\n",
    "# Combine all into a single DataFrame\n",
    "combined_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Get unique models for subplots\n",
    "unique_models = combined_df['Model'].unique()\n",
    "num_models = len(unique_models)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_models, 1, figsize=(12, 5 * num_models), sharex=True)\n",
    "\n",
    "if num_models == 1:\n",
    "    axes = [axes]  # Ensure axes is iterable for single subplot\n",
    "\n",
    "for ax, model in zip(axes, unique_models):\n",
    "    model_data = combined_df[combined_df['Model'] == model]\n",
    "    for language, group in model_data.groupby('Language'):\n",
    "        ax.plot(\n",
    "            group['layer'],\n",
    "            group['last_token_extracted'],\n",
    "            marker='o',\n",
    "            label=language,\n",
    "            color=language_colors.get(language, \"black\"),\n",
    "            linestyle=language_styles.get(language, \"-\")\n",
    "        )\n",
    "    ax.set_title(f\"{model} - Last Token Extraction Rate\", fontsize=14)\n",
    "    ax.set_ylabel(\"Success Rate\", fontsize=12)\n",
    "    ax.legend(title=\"Language\", fontsize=10, title_fontsize=12)\n",
    "    ax.grid(True)\n",
    "\n",
    "axes[-1].set_xlabel(\"Layer\", fontsize=12)  # Only bottom subplot has x-label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to check if a word is retrieved correctly in at least one layer\n",
    "def check_word_retrieval(df):\n",
    "    # Group by word and check if retrieved is True in any layer\n",
    "    df['retrieved'] = df.apply(lambda row: str(row['word']) in str(row['patchscope_result']), axis=1)\n",
    "    df['retrieved_in_any_layer'] = df.groupby('word')['retrieved'].transform('max')\n",
    "    return df\n",
    "\n",
    "# Analyze retrieval for each dataset\n",
    "retrieval_results = []\n",
    "for path in dataset_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    model_short = filename.split(\"_\")[2]\n",
    "    lang = filename.split(\"_\")[3].split(\".\")[0]\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Check if the word is retrieved in any layer\n",
    "    df = check_word_retrieval(df)\n",
    "\n",
    "    # Calculate the proportion of words retrieved in at least one layer\n",
    "    retrieval_rate = df['retrieved_in_any_layer'].mean()\n",
    "\n",
    "    # Store results\n",
    "    retrieval_results.append({\n",
    "        'Model': model_short,\n",
    "        'Language': lang,\n",
    "        'Retrieval Rate (Any Layer)': retrieval_rate\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "retrieval_results_df = pd.DataFrame(retrieval_results)\n",
    "print(retrieval_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
