{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # add parent folder to path\n",
    "from utils import extract_token_hidden_states, setup_tokenizer, setup_model, get_device\n",
    "\n",
    "def prepare_dataset(lang1, lang2, tokenizer):\n",
    "    dataset = load_dataset(\"tatoeba\", lang1=lang1, lang2=lang2)\n",
    "    dataset = dataset['train'].to_pandas()\n",
    "    dataset[lang1] = dataset['translation'].apply(lambda x: x[lang1])\n",
    "    dataset[lang2] = dataset['translation'].apply(lambda x: x[lang2])\n",
    "    dataset = dataset[[lang1, lang2]]\n",
    "    dataset = dataset[dataset[lang1].str.len() >= 3]\n",
    "    dataset = dataset[dataset[lang2].str.len() >= 3]\n",
    "    dataset = dataset.sample(n=1000, random_state=2025).reset_index(drop=True)\n",
    "    dataset[f'{lang1}_tokens'] = dataset[lang1].apply(tokenizer.tokenize)\n",
    "    dataset[f'{lang2}_tokens'] = dataset[lang2].apply(tokenizer.tokenize)\n",
    "    return dataset\n",
    "\n",
    "def compute_crosslingual_cosine(hidden_en, hidden_ko, top_k=3):\n",
    "    results = {}\n",
    "\n",
    "    for layer in hidden_en:\n",
    "        en_vecs = hidden_en[layer]  # shape: (N, D)\n",
    "        ko_vecs = hidden_ko[layer]  # shape: (N, D)\n",
    "\n",
    "        # Normalize to unit vectors for cosine similarity\n",
    "        en_norm = F.normalize(en_vecs, p=2, dim=1)  # (N, D)\n",
    "        ko_norm = F.normalize(ko_vecs, p=2, dim=1)  # (N, D)\n",
    "\n",
    "        # Compute cosine similarity: (N x D) @ (D x N) = (N x N)\n",
    "        sim_matrix = en_norm @ ko_norm.T  # (N, N)\n",
    "\n",
    "        # For each English vector, get top-k most similar Korean vectors\n",
    "        topk_values, topk_indices = torch.topk(sim_matrix, k=top_k, dim=1)  # (N, top_k)\n",
    "\n",
    "        # Check if correct alignment exists in top-k (optional accuracy check)\n",
    "        correct = torch.arange(sim_matrix.size(0)).to(topk_indices.device)\n",
    "        hits = (topk_indices == correct.unsqueeze(1)).any(dim=1).float()  # 1 if correct in top-k\n",
    "\n",
    "        results[layer] = {\n",
    "            \"similarity_matrix\": sim_matrix,\n",
    "            \"topk_indices\": topk_indices,\n",
    "            \"topk_values\": topk_values,\n",
    "            \"topk_accuracy\": hits.mean().item(),  # overall top-k accuracy\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define language pairs\n",
    "language_pairs = [(\"en\", \"ko\"), (\"de\", \"en\"), (\"de\", \"ko\")]\n",
    "models = [\"Tower-Babel/Babel-9B-Chat\", \"google/gemma-3-12b-it\", \"meta-llama/Llama-2-7b-chat-hf\"]\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "for model_name in models:\n",
    "    tokenizer = setup_tokenizer(model_name)\n",
    "    model = setup_model(model_name, device=device)\n",
    "    results_dict[model_name] = {}\n",
    "\n",
    "    for lang1, lang2 in language_pairs:\n",
    "        dataset = prepare_dataset(lang1, lang2, tokenizer)\n",
    "\n",
    "        hidden_1 = extract_token_hidden_states(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            inputs=dataset[f'{lang1}_tokens'].tolist(),\n",
    "            tokenizer_name=model_name,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        hidden_2 = extract_token_hidden_states(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            inputs=dataset[f'{lang2}_tokens'].tolist(),\n",
    "            tokenizer_name=model_name,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        top_k = 3\n",
    "        results = compute_crosslingual_cosine(hidden_1, hidden_2, top_k=top_k)\n",
    "\n",
    "        layers = sorted(results.keys())\n",
    "        accuracies = [results[l]['topk_accuracy'] for l in layers]\n",
    "\n",
    "        results_dict[model_name][f\"{lang1}-{lang2}\"] = (layers, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair_colors = {\n",
    "#     \"en-ko\": \"tab:blue\",\n",
    "#     \"de-en\": \"tab:orange\",\n",
    "#     \"de-ko\": \"tab:green\",\n",
    "# }\n",
    "fig, axes = plt.subplots(1, len(models), figsize=(8, 10), sharex=False, sharey=True)\n",
    "# fig, axes = plt.subplots(len(models), len(language_pairs), figsize=(25, 10), sharex=True, sharey=True)\n",
    "\n",
    "# Ensure axes is iterable even if len(models)==1\n",
    "if len(models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for col_idx, model_name in enumerate(models):\n",
    "    ax = axes[col_idx]\n",
    "    model_short = model_name.split(\"/\")[-1]\n",
    "\n",
    "    pairs_for_model = results_dict.get(model_name, {})\n",
    "    if not pairs_for_model:\n",
    "        ax.set_title(f\"{model_short}\\n(no results)\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    for pair, (layers, accs) in pairs_for_model.items():\n",
    "        ax.plot(\n",
    "            layers,\n",
    "            accs,\n",
    "            marker='o',\n",
    "            label=pair,\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "    # Fit axes per column\n",
    "    ax.relim()\n",
    "    ax.autoscale(enable=True, axis='both', tight=True)\n",
    "    ax.margins(x=0.02, y=0.05)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # integer layer ticks\n",
    "\n",
    "    ax.set_title(model_short, fontsize=20)\n",
    "    if col_idx == 0:\n",
    "        ax.set_ylabel(\"Top-K Accuracy\")\n",
    "    ax.set_xlabel(\"Layer\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(title=\"Lang pair\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "                ax.plot(\n",
    "                    layers,\n",
    "                    values,\n",
    "                    marker='o',\n",
    "                    label=f\"{src} → {tgt}\",\n",
    "                    alpha=0.7\n",
    "                )\n",
    "                ax.set_title(f\"{model_name.split('/')[-1]}: {pair_group[0][0]}↔{pair_group[0][1]}\", fontsize=20)\n",
    "                ax.grid(True)\n",
    "\n",
    "        # Add labels\n",
    "        if i == len(models) - 1:\n",
    "            ax.set_xlabel(\"Layer\", fontsize=18)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(metric, fontsize=18)\n",
    "\n",
    "        # Add legend\n",
    "        ax.grid(True)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.legend(title=\"Language Pair\", fontsize=14)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout for title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the RQ1 directory to the path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from classification import WordNonwordClassifier\n",
    "\n",
    "# model_name = \"Tower-Babel/Babel-9B-Chat\"\n",
    "model_name = \"google/gemma-3-12b-it\"\n",
    "# model_name = \"google/gemma-3-12b-pt\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "word_nonword_cls = WordNonwordClassifier(\"English\", model_name) # language is not used in the model name, but it is required by the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"en\"\n",
    "lang2 = \"ko\"\n",
    "\n",
    "dataset_en_ko = prepare_dataset(lang1, lang2, word_nonword_cls.tokenizer)\n",
    "\n",
    "hidden_1 = word_nonword_cls.extract_token_i_hidden_states(dataset_en_ko[f'{lang1}_tokens'].tolist())\n",
    "hidden_2 = word_nonword_cls.extract_token_i_hidden_states(dataset_en_ko[f'{lang2}_tokens'].tolist())\n",
    "torch.save(hidden_1, f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang1}_1.pt\")\n",
    "torch.save(hidden_2, f\"/home/hyujang/multilingual-inner-lexicon/data/RQ1/TatoebaHiddens/hidden_{model_name.split(\"/\")[-1]}_{lang2}_1.pt\")\n",
    "\n",
    "top_k = 3\n",
    "results = compute_crosslingual_cosine(hidden_1, hidden_2, top_k=top_k)\n",
    "\n",
    "layers = sorted(results.keys())\n",
    "accuracies = [results[l]['topk_accuracy'] for l in layers]\n",
    "plt.plot(layers, accuracies)\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(f\"Top-{top_k} Accuracy\")\n",
    "plt.title(f\"Cross-Lingual Alignment over Layers ({lang1}-{lang2})\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"de\"\n",
    "lang2 = \"en\"\n",
    "\n",
    "dataset_de_en = prepare_dataset(lang1, lang2, word_nonword_cls.tokenizer)\n",
    "\n",
    "hidden_1 = word_nonword_cls.extract_token_i_hidden_states(dataset_de_en[f'{lang1}_tokens'].tolist())\n",
    "hidden_2 = word_nonword_cls.extract_token_i_hidden_states(dataset_de_en[f'{lang2}_tokens'].tolist())\n",
    "\n",
    "top_k = 3\n",
    "results = compute_crosslingual_cosine(hidden_1, hidden_2, top_k=top_k)\n",
    "\n",
    "layers = sorted(results.keys())\n",
    "accuracies = [results[l]['topk_accuracy'] for l in layers]\n",
    "plt.plot(layers, accuracies)\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(f\"Top-{top_k} Accuracy\")\n",
    "plt.title(f\"Cross-Lingual Alignment over Layers ({lang1}-{lang2})\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"de\"\n",
    "lang2 = \"ko\"\n",
    "\n",
    "dataset_de_ko = prepare_dataset(lang1, lang2, word_nonword_cls.tokenizer)\n",
    "\n",
    "hidden_1 = word_nonword_cls.extract_token_i_hidden_states(dataset_de_ko[f'{lang1}_tokens'].tolist())\n",
    "hidden_2 = word_nonword_cls.extract_token_i_hidden_states(dataset_de_ko[f'{lang2}_tokens'].tolist())\n",
    "\n",
    "top_k = 3\n",
    "results = compute_crosslingual_cosine(hidden_1, hidden_2, top_k=top_k)\n",
    "\n",
    "layers = sorted(results.keys())\n",
    "accuracies = [results[l]['topk_accuracy'] for l in layers]\n",
    "plt.plot(layers, accuracies)\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(f\"Top-{top_k} Accuracy\")\n",
    "plt.title(f\"Cross-Lingual Alignment over Layers ({lang1}-{lang2})\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
