# multilingual-inner-lexicon

This repository contains the datasets, experiments, and analysis scripts accompanying the master thesis:

**“Multilingual Inner Lexicon in Transformer-based LLMs”**  
by *Hyunjung Jang*, University of Mannheim (2025).

The thesis investigates how transformer-based large language models (LLMs) construct and organize internal multilingual lexicons, focusing on **English, German, and Korean**. It examines:

- **Detokenization mechanisms** → how subword tokens are recombined into full words  
- **Cross-lingual alignment** → how lexical and sentence-level representations converge across languages  
- **Model comparisons** → Gemma-3, Babel, and Llama-2  
